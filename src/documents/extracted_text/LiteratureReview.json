[
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 1,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n1\nTowards Natural Language Interfaces for Data\nVisualization: A Survey\nLeixian Shen, Enya Shen, Yuyu Luo, Xiaocong Yang, Xuming Hu,\nXiongshuai Zhang, Zhiwei Tai, and Jianmin Wang\nAbstract—Utilizing Visualization-oriented Natural Language Interfaces (V-NLI) as a complementary input modality to direct manipulation\nfor visual analytics can provide an engaging user experience. It enables users to focus on their tasks rather than having to worry about\nhow to operate visualization tools on the interface. In the past two decades, leveraging advanced natural language processing\ntechnologies, numerous V-NLI systems have been developed in academic research and commercial software, especially in recent years.\nIn this article, we conduct a comprehensive review of the existing V-NLIs. In order to classify each paper, we develop categorical\ndimensions based on a classic information visualization pipeline with the extension of a V-NLI layer. The following seven stages are used:\nquery interpretation, data transformation, visual mapping, view transformation, human interaction, dialogue management, and\npresentation. Finally, we also shed light on several promising directions for future work in the V-NLI community.\nIndex Terms—Data Visualization, Natural Language Interfaces, Survey.\n!\n1\nINTRODUCTION\nT\nHE use of interactive visualization is becoming increasingly\npopular in data analytics [17]. As a common part of analytics\nsuites, Windows, Icons, Menus, and Pointer (WIMP) interfaces\nhave been widely employed to facilitate interactive visual analysis\nin current practices. However, this interaction paradigm presents a\nsteep learning curve in visualization tools since it requires users to\ntranslate their analysis intents into tool-speciﬁc operations [126],\nas shown in the upper part of Figure 1.\nOver the years, the rapid development of Natural Language\nProcessing (NLP) technology has provided great opportunities to\nexplore a natural-language-based interaction paradigm for data\nvisualization [18], [271]. With the help of advanced NLP toolkits\n[1], [3], [21], [81], [157], a surge of Visualization-oriented Natural\nLanguage Interfaces (V-NLI) emerged recently as a complementary\ninput modality to traditional WIMP interaction. V-NLIs accept the\nuser’s natural language queries (e.g., “visualize the distribution\nof budget as a histogram”) as input and output appropriate visu-\nalizations (e.g., correspondingly, a histogram with the production\nbudget, a data attribute of cars dataset). The emergence of V-NLI\ncan greatly enhance the usability of visualization tools in terms of:\n(a) Convenience and novice-friendliness. Natural language is a skill\nthat is mastered by the public. By using natural language to interact\nwith computers, V-NLI closes the tool-speciﬁc manipulations to\nusers, as shown in Figure 1, facilitating the analysis ﬂow for\nnovices. (b) Intuitiveness and effectiveness. It is a consensus that\nvisual analysis is most effective when users can focus on their data\nrather than manipulations on the interface of analysis tools [83].\nWith the help of V-NLI, users can express their analytic tasks in\ntheir terms. (c) Humanistic care. A sizeable amount of information\nwe access nowadays is supported by visual means. V-NLI can be\nan innovative means for non-visual access, which promotes the\ninclusion of blind and low vision people.\n•\nAll authors are from Tsinghua University, Beijing, China. E-mail:\n{slx20,luoyy18,yangxc18,hxm19,zxs21,tzw20}@mails.tsinghua.edu.cn,\n{shenenya, jimwang}@tsinghua.edu.cn.\nManuscript received XX XX, 2021; revised XX XX, 2022.\nAnalysis\nIntents\nVisualization\ntools\nTranslate \nanalysis intents into\ntool-specific operations \nVisualizations\nAnalysis\nIntents\nVisualization\ntools\nVisualizations\nTypical interactive visualization paradigm\nInteractive visualization with V-NLIs\nFig. 1. The traditional interaction paradigm requires users to translate\ntheir analysis intents into tool-speciﬁc operations [126]. With the help of\nV-NLI, users can express their analysis intents in their terms.\nHowever, designing and implementing V-NLIs is challenging\ndue to the ambiguous and underspeciﬁed nature of human language,\nthe complexity of maintaining context in a conversational ﬂow,\nand the lack of discoverability (communicating to users what\nthe system can do). In the past two decades, to address the\nchallenges, numerous systems have been developed in academic\nresearch and commercial software, especially in recent years. The\ntimeline of V-NLI is shown in Figure 2. Back in 2001, Cox et\nal. [40] presented an initial prototype of NLI for visualization,\nwhich can only accept well-structured queries. Articulate [239]\nintroduced a two-step process to create visualizations from NL\nqueries almost a decade later. It ﬁrst extracts the user’s analytic\ntask and data attributes and then automatically determines the\nappropriate visualizations based on the information. Although the\ninfancy-stage investigations were a promising start, as natural\nlanguage was not yet a prevalent interaction modality, the V-NLI\nsystems were restricted to simple prototypes. However, since Apple\nintegrated Siri [219] into the iPhone, NLIs began to attract more\nattention. Around 2013, the advent of word embeddings [163]\npromoted the advances of neural networks for NLP, rekindling\ncommercial interest in V-NLI. IBM ﬁrstly published their NL-\nbased cognitive service, Watson Analytics [4], in 2014. Microsoft\nPower BI’s Q&A [5] and Tableau’s Ask data [2] were announced\nin 2018 and 2019, respectively, offering various features like\nautocompletion and context management. DataTone [63] ﬁrst\narXiv:2109.03506v2  [cs.HC]  4 Feb 2022\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 1\nTowards Natural Language Interfaces for Data\nVisualization: A Survey\nLeixian Shen, Enya Shen, Yuyu Luo, Xiaocong Yang, Xuming Hu,\nXiongshuai Zhang, Zhiwei Tai, and Jianmin Wang\nAbstract—UtilizingVisualization-orientedNaturalLanguageInterfaces(V-NLI)asacomplementaryinputmodalitytodirectmanipulation\nforvisualanalyticscanprovideanengaginguserexperience.Itenablesuserstofocusontheirtasksratherthanhavingtoworryabout\nhowtooperatevisualizationtoolsontheinterface.Inthepasttwodecades,leveragingadvancednaturallanguageprocessing\ntechnologies,numerousV-NLIsystemshavebeendevelopedinacademicresearchandcommercialsoftware,especiallyinrecentyears.\nInthisarticle,weconductacomprehensivereviewoftheexistingV-NLIs.Inordertoclassifyeachpaper,wedevelopcategorical\ndimensionsbasedonaclassicinformationvisualizationpipelinewiththeextensionofaV-NLIlayer.Thefollowingsevenstagesareused:\nqueryinterpretation,datatransformation,visualmapping,viewtransformation,humaninteraction,dialoguemanagement,and\npresentation.Finally,wealsoshedlightonseveralpromisingdirectionsforfutureworkintheV-NLIcommunity.\nIndexTerms—DataVisualization,NaturalLanguageInterfaces,Survey.\n(cid:70)\n1 INTRODUCTION\nTypical interactive visualization paradigm\nT HEuseofinteractivevisualizationisbecomingincreasingly Translate\nAnalysis Visualization\npopularindataanalytics[17].Asacommonpartofanalytics Intents analysis intentsinto tools\ntool-specific operations\nsuites, Windows, Icons, Menus, and Pointer (WIMP) interfaces\nVisualizations\nhavebeenwidelyemployedtofacilitateinteractivevisualanalysis\nincurrentpractices.However,thisinteractionparadigmpresentsa Interactive visualization with V-NLIs\nsteeplearningcurveinvisualizationtoolssinceitrequiresusersto\ntranslatetheiranalysisintentsintotool-specificoperations[126], Analysis Visualization\nIntents tools\nasshownintheupperpartofFigure1.\nOver the years, the rapid development of Natural Language Visualizations\nProcessing(NLP)technologyhasprovidedgreatopportunitiesto Fig.1.Thetraditionalinteractionparadigmrequiresuserstotranslate\nexplore a natural-language-based interaction paradigm for data theiranalysisintentsintotool-specificoperations[126].Withthehelpof\nV-NLI,userscanexpresstheiranalysisintentsintheirterms.\nvisualization[18],[271].WiththehelpofadvancedNLPtoolkits\n[1],[3],[21],[81],[157],asurgeofVisualization-orientedNatural\nHowever,designingandimplementingV-NLIsischallenging\nLanguageInterfaces(V-NLI)emergedrecentlyasacomplementary\nduetotheambiguousandunderspecifiednatureofhumanlanguage,\ninputmodalitytotraditionalWIMPinteraction.V-NLIsacceptthe\nthe complexity of maintaining context in a conversational flow,\nuser’s natural language queries (e.g., “visualize the distribution\nand the lack of discoverability (communicating to users what\nof budget as a histogram”) as input and output appropriate visu-\nthe system can do). In the past two decades, to address the\nalizations(e.g.,correspondingly,ahistogramwiththeproduction\nchallenges,numeroussystemshavebeendevelopedinacademic\nbudget,adataattributeofcarsdataset).TheemergenceofV-NLI\nresearchandcommercialsoftware,especiallyinrecentyears.The\ncangreatlyenhancetheusabilityofvisualizationtoolsintermsof:\ntimeline of V-NLI is shown in Figure 2. Back in 2001, Cox et\n(a)Convenienceandnovice-friendliness.Naturallanguageisaskill\nal. [40] presented an initial prototype of NLI for visualization,\nthatismasteredbythepublic.Byusingnaturallanguagetointeract\nwhich can only accept well-structured queries. Articulate [239]\nwith computers, V-NLI closes the tool-specific manipulations to\nintroduced a two-step process to create visualizations from NL\nusers, as shown in Figure 1, facilitating the analysis flow for\nqueries almost a decade later. It first extracts the user’s analytic\nnovices.(b)Intuitivenessandeffectiveness.Itisaconsensusthat\ntask and data attributes and then automatically determines the\nvisualanalysisismosteffectivewhenuserscanfocusontheirdata\nappropriatevisualizationsbasedontheinformation.Althoughthe\nrather than manipulations on the interface of analysis tools [83].\ninfancy-stage investigations were a promising start, as natural\nWiththehelpofV-NLI,userscanexpresstheiranalytictasksin\nlanguagewasnotyetaprevalentinteractionmodality,theV-NLI\ntheirterms.(c)Humanisticcare.Asizeableamountofinformation\nsystemswererestrictedtosimpleprototypes.However,sinceApple\nweaccessnowadaysissupportedbyvisualmeans.V-NLIcanbe\nintegratedSiri[219]intotheiPhone,NLIsbegantoattractmore\nan innovative means for non-visual access, which promotes the\nattention. Around 2013, the advent of word embeddings [163]\ninclusionofblindandlowvisionpeople.\npromoted the advances of neural networks for NLP, rekindling\ncommercial interest in V-NLI. IBM firstly published their NL-\nbasedcognitiveservice,WatsonAnalytics[4],in2014.Microsoft\n• All authors are from Tsinghua University, Beijing, China. E-mail:\n{slx20,luoyy18,yangxc18,hxm19,zxs21,tzw20}@mails.tsinghua.edu.cn, PowerBI’sQ&A[5]andTableau’sAskdata[2]wereannounced\n{shenenya,jimwang}@tsinghua.edu.cn. in 2018 and 2019, respectively, offering various features like\nManuscriptreceivedXXXX,2021;revisedXXXX,2022. autocompletion and context management. DataTone [63] first\n2202\nbeF\n4\n]CH.sc[\n2v60530.9012:viXra",
    "tables_extraction_text_csv": "0,1\n\"1\nINTRODUCTION\",\n,Typical interactive visualization paradigm\n,Translate\n,\"Analysis\nVisualization\"\nT HE use of interactive visualization is becoming increasingly,analysis intents into\n,\"Intents\ntools\"\n,\n,tool-specific operations\n\"suites, Windows,\nIcons, Menus, and Pointer\n(WIMP)\ninterfaces\",\n,Visualizations\nhave been widely employed to facilitate interactive visual analysis,\n\"in current practices. However,\nthis interaction paradigm presents a\",Interactive visualization with V-NLIs\nsteep learning curve in visualization tools since it requires users to,\n,\"Analysis\nVisualization\"\n\"translate their analysis intents into tool-speciﬁc operations [126],\",\"Intents\ntools\"\nas shown in the upper part of Figure 1.,\n,Visualizations\n\"Over\nthe years,\nthe rapid development of Natural Language\",\nProcessing (NLP) technology has provided great opportunities to,\"Fig. 1. The traditional\ninteraction paradigm requires users to translate\"\n,their analysis intents into tool-speciﬁc operations [126]. With the help of\nexplore a natural-language-based interaction paradigm for data,\n,\"V-NLI, users can express their analysis intents in their terms.\"\n\"visualization [18], [271]. With the help of advanced NLP toolkits\",\n\"[1], [3], [21], [81], [157], a surge of Visualization-oriented Natural\",\n,\"However, designing and implementing V-NLIs is challenging\"\nLanguage Interfaces (V-NLI) emerged recently as a complementary,\n,\"due to the ambiguous and underspeciﬁed nature of human language,\"\n\"input modality to traditional WIMP interaction. V-NLIs accept\nthe\",\n,\"the complexity of maintaining context\nin a conversational ﬂow,\"\n\"user’s natural\nlanguage queries (e.g., “visualize the distribution\",\n,\"and the\nlack of discoverability (communicating to users what\"\nof budget as a histogram”) as input and output appropriate visu-,\n,\"the\nsystem can do).\nIn the past\ntwo decades,\nto address\nthe\"\n\"alizations (e.g., correspondingly, a histogram with the production\",\n,\"challenges, numerous systems have been developed in academic\"\n\"budget, a data attribute of cars dataset). The emergence of V-NLI\",\n,\"research and commercial software, especially in recent years. The\"\ncan greatly enhance the usability of visualization tools in terms of:,\n,\"timeline of V-NLI\nis shown in Figure 2. Back in 2001, Cox et\"\n(a) Convenience and novice-friendliness. Natural language is a skill,\n,\"al.\n[40] presented an initial prototype of NLI\nfor visualization,\"\nthat is mastered by the public. By using natural language to interact,\n,which can only accept well-structured queries. Articulate [239]\n\"with computers, V-NLI closes the tool-speciﬁc manipulations to\",\n,introduced a two-step process to create visualizations from NL\n\"users,\nas\nshown in Figure 1,\nfacilitating the\nanalysis ﬂow for\",\n,queries almost a decade later. It ﬁrst extracts the user’s analytic\nnovices. (b) Intuitiveness and effectiveness. It is a consensus that,\n,\"task and data attributes and then automatically determines\nthe\"\nvisual analysis is most effective when users can focus on their data,\n,appropriate visualizations based on the information. Although the\nrather than manipulations on the interface of analysis tools [83].,\n,\"infancy-stage\ninvestigations were\na promising start,\nas natural\"\n\"With the help of V-NLI, users can express their analytic tasks in\",\n,\"language was not yet a prevalent interaction modality, the V-NLI\"\ntheir terms. (c) Humanistic care. A sizeable amount of information,\n,\"systems were restricted to simple prototypes. However, since Apple\"\nwe access nowadays is supported by visual means. V-NLI can be,\n,\"integrated Siri [219] into the iPhone, NLIs began to attract more\"\n\"an innovative means for non-visual access, which promotes the\",\n,\"attention. Around 2013,\nthe advent of word embeddings\n[163]\"\ninclusion of blind and low vision people.,\n,\"promoted the advances of neural networks for NLP,\nrekindling\"\n,\"commercial\ninterest\nin V-NLI.\nIBM ﬁrstly published their NL-\"\n,\"based cognitive service, Watson Analytics [4],\nin 2014. Microsoft\"\n\"All\nauthors\nare\nfrom Tsinghua University,\nBeijing, China.\nE-mail:\n•\",\n\"{slx20, luoyy18, yangxc18, hxm19, zxs21,tzw20}@mails.tsinghua.edu.cn,\",Power BI’s Q&A [5] and Tableau’s Ask data [2] were announced\n\"{shenenya, jimwang}@tsinghua.edu.cn.\",\n,\"in 2018 and 2019,\nrespectively, offering various\nfeatures\nlike\"\n\"Manuscript received XX XX, 2021; revised XX XX, 2022.\",\"autocompletion and context management. DataTone\n[63] ﬁrst\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 2,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n2\nCox et al. \nArticulate\nApple Siri\nIBM \nWatson Analytics\nMicrosoft \nPower BI Q&A\nTableau \nAsk Data\nDataTone\nEviza\nOrko\nncNet, ADVISor,\nData@Hand\nQuda, NLV\nNL4DV\nSneak pique\nnvBench\nWord embeddings\nELMO, BERT\nInfancy Stage\nDevelopment Stage\nOutbreak Stage\n.   .   .   .   .   .   .   .\nFlowSense\n2001 (1)\n2010 (1)\n2011 (1)\n2021 (11)\n2013 (1)\n2014 (1)\n2015 (1)\n2016 (3)\n2018 (7)\n2019 (9)\n2020 (13)\n2012 (2)\n2017 (4)\n2002 (1)\n2005 (1)\n: Academic research \n  : Commercial software      \n        : NLP milestone \n: Dataset\nAttention\nSeq-to-Seq\nFig. 2. Timeline of V-NLI. We brieﬂy divide the timeline into the infancy, development, and outbreak stages. The number of yearly published papers is\nattached. The timeline consists of four parts: academic research, commercial software, NLP milestone, and dataset.\nTABLE 1\nRelevant venues\nField\nVenues\nVisualization\n(VIS)\nIEEE VIS (InfoVis, VAST, SciVis), EuroVis,\nPaciﬁcVis, TVCG, CGF, CGA\nHuman-Computer Interaction\n(HCI)\nCHI, UIST, IUI\nNatural Language Processing\n(NLP)\nACL, EMNLP, NAACL, COLING\nData Mining and Management\n(DMM)\nKDD, SIGMOD, ICDE, VLDB\nintroduced ambiguity widgets, which are user interface widgets\n(e.g., dropdown and slider) to help users modify system-generated\nresponses. As opposed to one-off commands, Eviza [205] enabled\nusers to have interactive conversations with their data. After\ntechnological accumulation, the past ﬁve years have seen the\noutbreak of V-NLI (see the number of yearly published papers in\nFigure 2). With the development of hardware devices, synergistic\nmultimodal visualization interfaces gained notable interest. Orko\n[231] was the ﬁrst system to combine touch and speech input on\ntablet devices, and Data@Hand [115] focused on smartphones.\nSneak pique [206] explored how autocompletion can improve\nsystem discoverability while helping users formulate analytical\nquestions. The pretrained language models obtained new state-of-\nthe-art results on various NLP tasks from 2018, which provided\ngreat opportunities to improve the intelligence of V-NLI [49], [179].\nADVISor [143] and ncNet [150] were follow-up deep learning-\nbased solutions. Quda [61] and NLV [228] contributed datasets of\nNL queries for visual data analytics, and nvBench produced the\nﬁrst V-NLI benchmark [151]. Beyond data exploration, FlowSense\n[273] augmented a data ﬂow-based visualization system with V-\nNLI. The NL4DV [174] toolkit can be easily integrated into existing\nvisualization systems to provide V-NLI service.\nLiterature on V-NLI research is proliferating, covering aspects\nsuch as Visualization (VIS), Human-Computer Interaction (HCI),\nNatural Language Processing (NLP), and Data Mining and Man-\nagement (DMM). As a result, there is an increasing need to better\norganize the research landscape, categorize current work, identify\nknowledge gaps, and assist people who are new to this growing area\nto understand the challenges and subtleties in the community. For\nthis purpose, there have been several prior efforts to summarize the\nadvances in this area. For example, Srinivasan and Stasko (Short\npaper in EuroVis 2017 [230]) conducted a simple examination of\nﬁve existing V-NLI systems by comparing and contrasting them\nbased on the tasks they allow users to perform. They (Journal\npaper in CGA 2020 [232]) further highlighted critical challenges\nfor evaluating V-NLIs and discussed beneﬁts and considerations of\nthree task framing strategies. Although the two surveys can provide\nvaluable guidance for follow-up research, with the outbreak of\nV-NLI in recent years, considerable new works are to be covered\nand details to be discussed. To the best of our knowledge, this\npaper is the ﬁrst step towards comprehensively reviewing V-NLI\napproaches in a systematic manner.\nThis paper is structured as follows: First, we explain the scope\nand methodology of the survey in Section 2. What follows is the\nclassiﬁcation overview of existing works in Section 3. Then, the\ncomprehensive survey is presented in Sections 4 - 10, corresponding\nto seven stages extracted in the information visualization pipeline\n[28]. Finally, we shed light on several promising directions for\nfuture work in Section 11.\n2\nSURVEY LANDSCAPE\n2.1\nScope of the Survey\nIn order to narrow the scope of the survey within a controllable\nrange, we focused on visualization-oriented natural language\ninterfaces, which accept natural language queries as input and\noutput appropriate visualizations automatically. Users can input\nnatural language in various ways, including typing with a keyboard\n(e.g., Datatone [63] in Figure 7), speech input via a microphone\n(e.g., InChorus [226] in Figure 9), selecting text from articles (e.g.,\nMetoyer et al. [162] in Figure 13), and inputting existing textual\ndescriptions (e.g., Vis-Annotator [122] in Figure 12).\nIn addition, several ﬁelds are closely related to V-NLI. As\nshown in Table 2, the Related Stage column lists overlap stages\nwith V-NLI in Figure 3 that apply similar technologies. In order\nto make the survey more comprehensive, when introducing V-\nNLI in the following sections, we will involve additional crucial\ndiscussions on the aforementioned related ﬁelds, with explanations\nof their importance and relationship with V-NLI. For example,\nVisualization Recommendation (VisRec) [187], [289] acts as the\nback-end engine of V-NLI to recommend visualizations. Natural\nLanguage Interface for Database (NLI4DB) [6] and Conversational\nInterface for Visualization (CIV) [20] share similar principles with\nV-NLI. Natural Language Generation for Visualization (NLG4Vis)\n[144], [177] and Visual Question Answering (VQA) [110], [158]\ncomplement visual data analysis with natural language as output.\nAnnotation [190], [213] and Narrative Storytelling [241] present\nfascinating visualizations by combining textual and visual elements.\n2.2\nSurvey Methodology\nTo comprehensively survey V-NLI, we performed an exhaustive\nreview of relevant journals and conferences throughout the past\ntwenty years (2000-2021), which broadly covers VIS, HCI, NLP,\nand DMM. The relevant venues are shown in Table 1. We began\nour survey by searching keywords (“natural language” AND\n“visualization”) in Google Scholar, resulting in 1436 papers (VIS:\n455, HCI: 489, NLP: 289, and DMM: 203). We also searched for\nrepresentative related works that appeared earlier or in the cited\nreferences of related papers. During the review process, we ﬁrst\nexamined the titles of papers from these publications to identify\ncandidate papers. Next, abstracts of the candidate papers were\nbrowsed to determine whether they were related to V-NLI. The\nfull text was reviewed to make a ﬁnal decision if we could not\nobtain precise information from the title and abstract. Finally, we\ncollected 57 papers about V-NLI that accepts natural language\nas input and outputs visualizations. Details of the systems are\nlisted in Table 3, including the applied NLP technologies, chart\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 2\nInfancy Stage Development Stage Outbreak Stage\nIBM Microsoft Tableau NL4DV ncNet, ADVISor,\nCox et al. Articulate Watson Analytics DataTone Orko Power BI Q&A Ask Data Sneak pique Data@Hand\n20.0 1 (.1 ) .2 0 0.2 ( 1.) .2 0 0.5 ( 1.) 2010 (1) 2011 (1) 2012 (2) 2013 (1) 2014 (1) 2015 (1) 2016 (3) 2017 (4) 2018 (7) 2019 (9) 2020 (13) 2021 (11)\nAppleSiri Word embeddingsSeq-to-Seq Attention Eviza ELMO, BERT FlowSense Quda, NLV nvBench\n: Academic research : Commercial software : NLP milestone : Dataset\nFig.2.TimelineofV-NLI.Webrieflydividethetimelineintotheinfancy,development,andoutbreakstages.Thenumberofyearlypublishedpapersis\nattached.Thetimelineconsistsoffourparts:academicresearch,commercialsoftware,NLPmilestone,anddataset.\nTABLE1 andmethodologyofthesurveyinSection2.Whatfollowsisthe\nRelevantvenues classificationoverviewofexistingworksinSection3.Then,the\nField Venues comprehensivesurveyispresentedinSections4-10,corresponding\nVisualization IEEEVIS(InfoVis,VAST,SciVis),EuroVis, tosevenstagesextractedintheinformationvisualizationpipeline\n(VIS) PacificVis,TVCG,CGF,CGA\n[28]. Finally, we shed light on several promising directions for\nHuman-ComputerInteraction\n(HCI) CHI,UIST,IUI futureworkinSection11.\nNaturalLanguageProcessing\nACL,EMNLP,NAACL,COLING\n(NLP) 2 SURVEY LANDSCAPE\nDataMiningandManagement\n(DMM) KDD,SIGMOD,ICDE,VLDB 2.1 ScopeoftheSurvey\nIn order to narrow the scope of the survey within a controllable\nintroduced ambiguity widgets, which are user interface widgets\nrange, we focused on visualization-oriented natural language\n(e.g.,dropdownandslider)tohelpusersmodifysystem-generated\ninterfaces, which accept natural language queries as input and\nresponses.Asopposedtoone-offcommands,Eviza[205]enabled\noutput appropriate visualizations automatically. Users can input\nusers to have interactive conversations with their data. After\nnaturallanguageinvariousways,includingtypingwithakeyboard\ntechnological accumulation, the past five years have seen the\n(e.g.,Datatone[63]inFigure7),speechinputviaamicrophone\noutbreakofV-NLI(seethenumberofyearlypublishedpapersin\n(e.g.,InChorus[226]inFigure9),selectingtextfromarticles(e.g.,\nFigure2).Withthedevelopmentofhardwaredevices,synergistic\nMetoyeretal.[162]inFigure13),andinputtingexistingtextual\nmultimodalvisualizationinterfacesgainednotableinterest.Orko\ndescriptions(e.g.,Vis-Annotator[122]inFigure12).\n[231]wasthefirstsystemtocombinetouchandspeechinputon\nIn addition, several fields are closely related to V-NLI. As\ntablet devices, and Data@Hand [115] focused on smartphones.\nshown in Table 2, the Related Stage column lists overlap stages\nSneak pique [206] explored how autocompletion can improve\nwithV-NLIinFigure3thatapplysimilartechnologies.Inorder\nsystem discoverability while helping users formulate analytical\nto make the survey more comprehensive, when introducing V-\nquestions.Thepretrainedlanguagemodelsobtainednewstate-of-\nNLIinthefollowingsections,wewillinvolveadditionalcrucial\nthe-art results on various NLP tasks from 2018, which provided\ndiscussionsontheaforementionedrelatedfields,withexplanations\ngreatopportunitiestoimprovetheintelligenceofV-NLI[49],[179].\nof their importance and relationship with V-NLI. For example,\nADVISor [143] and ncNet [150] were follow-up deep learning-\nVisualizationRecommendation(VisRec)[187],[289]actsasthe\nbasedsolutions.Quda[61]andNLV[228]contributeddatasetsof\nback-endengineofV-NLItorecommendvisualizations.Natural\nNL queries for visual data analytics, and nvBench produced the\nLanguageInterfaceforDatabase(NLI4DB)[6]andConversational\nfirstV-NLIbenchmark[151].Beyonddataexploration,FlowSense\nInterfaceforVisualization(CIV)[20]sharesimilarprincipleswith\n[273] augmented a data flow-based visualization system with V-\nV-NLI.NaturalLanguageGenerationforVisualization(NLG4Vis)\nNLI.TheNL4DV[174]toolkitcanbeeasilyintegratedintoexisting\n[144],[177]andVisualQuestionAnswering(VQA)[110],[158]\nvisualizationsystemstoprovideV-NLIservice.\ncomplementvisualdataanalysiswithnaturallanguageasoutput.\nLiteratureonV-NLIresearchisproliferating,coveringaspects Annotation[190],[213]andNarrativeStorytelling[241]present\nsuchasVisualization(VIS),Human-ComputerInteraction(HCI), fascinatingvisualizationsbycombiningtextualandvisualelements.\nNaturalLanguageProcessing(NLP),andDataMiningandMan-\nagement(DMM).Asaresult,thereisanincreasingneedtobetter 2.2 SurveyMethodology\norganizetheresearchlandscape,categorizecurrentwork,identify To comprehensively survey V-NLI, we performed an exhaustive\nknowledgegaps,andassistpeoplewhoarenewtothisgrowingarea review of relevant journals and conferences throughout the past\ntounderstandthechallengesandsubtletiesinthecommunity.For twentyyears(2000-2021),whichbroadlycoversVIS,HCI,NLP,\nthispurpose,therehavebeenseveralprioreffortstosummarizethe andDMM.TherelevantvenuesareshowninTable1.Webegan\nadvancesinthisarea.Forexample,SrinivasanandStasko(Short our survey by searching keywords (“natural language” AND\npaperinEuroVis2017[230])conductedasimpleexaminationof “visualization”)inGoogleScholar,resultingin1436papers(VIS:\nfiveexistingV-NLIsystemsbycomparingandcontrastingthem 455,HCI:489,NLP:289,andDMM:203).Wealsosearchedfor\nbased on the tasks they allow users to perform. They (Journal representative related works that appeared earlier or in the cited\npaperinCGA2020[232])furtherhighlightedcriticalchallenges references ofrelated papers.During the review process, wefirst\nforevaluatingV-NLIsanddiscussedbenefitsandconsiderationsof examinedthetitlesofpapersfromthesepublicationstoidentify\nthreetaskframingstrategies.Althoughthetwosurveyscanprovide candidate papers. Next, abstracts of the candidate papers were\nvaluable guidance for follow-up research, with the outbreak of browsed to determine whether they were related to V-NLI. The\nV-NLIinrecentyears,considerablenewworksaretobecovered full text was reviewed to make a final decision if we could not\nand details to be discussed. To the best of our knowledge, this obtainpreciseinformationfromthetitleandabstract.Finally,we\npaperisthefirststeptowardscomprehensivelyreviewingV-NLI collected 57 papers about V-NLI that accepts natural language\napproachesinasystematicmanner. as input and outputs visualizations. Details of the systems are\nThispaperisstructuredasfollows:First,weexplainthescope listed in Table 3, including the applied NLP technologies, chart",
    "tables_extraction_text_csv": "0,1\nField,Venues\n\"Visualization\n(VIS)\",\"IEEE VIS (InfoVis, VAST, SciVis), EuroVis,\nPaciﬁcVis, TVCG, CGF, CGA\"\n\"Human-Computer Interaction\n(HCI)\",\"CHI, UIST, IUI\"\n\"Natural Language Processing\n(NLP)\",\"ACL, EMNLP, NAACL, COLING\"\n\"Data Mining and Management\n(DMM)\",\"KDD, SIGMOD, ICDE, VLDB\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 3,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n3\nTABLE 2\nRepresentative papers of related works to V-NLI. Related Stage column lists the overlap with V-NLI stages in Figure 3 that apply similar technologies.\nTopic\nRelated Stage\nSurvey\nRepresentative Paper\nVisualization Recommendation (VisRec)\nB/C/D/E/G\n[187], [289]\n[32], [37], [42], [46], [51], [70], [84], [86], [106], [109], [114], [130], [138],\n[142], [149], [152], [154], [168], [170], [185], [186], [203], [234], [246], [247],\n[253], [259], [260], [261], [263], [276], [277], [288]\nNatural Language Interface for DataBase (NLI4DB)\nA/B/E/F/G\n[6], [290]\n[13], [14], [15], [19], [22], [55], [72], [73], [74], [80], [87], [92], [95], [118],\n[135], [136], [137], [141], [173], [175], [188], [193], [202], [210], [220], [221],\n[252], [268], [270], [274], [285], [287]\nConversational Interface for Visualization (CIV)\nA/B/C/D/E/F/G\n[20], [58], [77], [129], [134], [171], [172], [201], [216]\nNatural Language Generation for Visualization (NLG4Vis)\nB/E/G\n[39], [42], [44], [45], [59], [111], [144], [166], [167], [177], [184], [223]\nVisual Question Answering (VQA)\nA/B/E/F/G\n[158]\n[27], [30], [97], [98], [110], [143], [155], [222], [258], [269]\nAnnotation and Narrative Storytelling\nA/B/C/D/E/F/G\n[200], [241]\n[11], [26], [32], [33], [34], [35], [36], [41], [62], [64], [65], [82], [89], [101],\n[112], [114], [119], [122], [123], [146], [162], [165], [183], [190], [213], [240],\n[254], [256], [257], [265], [275]\nTABLE 3\nSummary of representative works in V-NLI. The ﬁrst ﬁve columns list basic information, and the left columns are characteristics of V-NLI with\ncategorical dimensions described in Section 3. Details of each column will be discussed in Sections 4 - 10.\n4\n5\n6\n7\n8\n9\n10\nName\nPublication\nNLP Toolkit or Technology\nVisualization\nType\nRecommendation\nAlgorithm\nSemantic and Syntax Analysis\nTask Inference\nData Attributes Inference\nDefault for Underspeciﬁed Utterances\nTransformation for Data Insights\nSpatial Substrate Mapping\nGraphical Elements Mapping\nGraphical Properties Mapping\nView Transformation\nAmbiguity Widget\nAutocompletion\nMultimodal\nWIMP\nDialogue Management\nVisual Presentation\nAnnotation\nNarrative Storytelling\nNL Description Generation\nVisual Question Answering\nCox et al. [40]\nIJST’01\nSisl\nB/Ta\nInfoStill\n× ✓✓× ✓✓✓✓× × × × × ✓✓× × × ×\nKato et al. [105]\nCOLING’02 Logical form\nB/L/P/A\nTemplate\n✓✓✓× ✓✓✓✓× × × × × × ✓× × × ×\nRIA [284]\nInfoVis’05\n-\nI\nOptimization\n✓✓✓× ✓× × × × × × ✓× ✓✓× ✓× ×\nArticulate [239]\nLNCS’10\nStanford Parser\nS/R/B/L/P/Bo\nDecision tree\n✓✓✓× ✓✓✓✓× × × × × × ✓× × × ×\nContextiﬁer [89]\nCHI’13\nNLTK\nL\n-\n✓× ✓× ✓× × × × × × × × × ✓✓✓× ×\nNewsViews [64]\nCHI’14\nOpenCalais\nM\n-\n✓✓✓× ✓× × × × × × × × × ✓✓✓× ×\nDatatone [63]\nUIST’15\nNLTK/Stanford Parser\nS/B/L\nTemplate\n✓✓✓× ✓✓✓✓× ✓× × ✓× ✓× × × ×\nArticulate2 [120], [121]\nSIGDIAL’16 ClearNLP/Stanford Parser/OpenNLP B/L/H\nDecision tree\n✓✓✓× ✓✓✓✓✓× × ✓× ✓✓× × × ×\nEviza [205]\nUIST’16\nANTLR\nM/L/B/S\nTemplate\n✓✓✓× ✓✓✓✓× ✓✓× ✓✓✓× × × ×\nTimeLineCurator [62]\nTVCG’16\nTERNIP\nTl\nTemplate\n✓✓✓× × × × × × × × × × × ✓✓✓✓×\nAnalyza [50]\nIUI’17\nStanford Parser\nB/L\nTemplate\n✓✓✓× ✓✓✓✓× ✓✓× ✓✓✓× × × ×\nTSI [26]\nTVCG’17\nTemplate\nL/A/H\n-\n× ✓✓× ✓× × × × × × × × × ✓✓✓× ×\nAva [134]\nCIDR’17\nControlled Natural Language\nS/L/B\nTemplate\n✓✓✓× ✓✓✓✓× × × × × ✓✓× × × ✓\nDeepEye [148]\nSIGMOD’18 OpenNLP\nP/L/B/S\nTemplate\n✓× ✓× ✓✓✓✓× × × × ✓× ✓× × × ×\nEvizeon [83]\nTVCG’18\nCoreNLP\nM/L/B/S\nTemplate\n✓✓✓× ✓✓✓✓× ✓✓× ✓✓✓× × × ×\nIris [58]\nCHI’18\nDomain-speciﬁc language\nS/T\nTemplate\n✓✓✓× ✓✓✓✓× × × × × ✓✓× × × ✓\nMetoyer et al. [162]\nIUI’18\nCoreNLP\nI\nTemplate\n✓× ✓× ✓× × × × × × × ✓× ✓× ✓× ✓\nOrko [231]\nTVCG’18\nCoreNLP/NLTK\nN\n-\n✓✓✓× ✓✓× ✓✓✓× ✓✓✓✓× × × ×\nShapeSearch [217], [218]\nVLDB’18\nStanford Parser\nL\nShapeQuery\n✓× ✓× ✓✓× ✓× ✓× × ✓× ✓× × × ×\nValletto [103]\nCHI’18\nSpacy\nB/S/M\nTemplate\n✓✓✓× ✓✓✓✓✓× × ✓✓✓✓× × × ×\nArkLang [208]\nIUI’19\nCocke-Kasami-Younger\nB/G/L/M/P/S/Tr\nTemplate\n✓✓✓✓✓✓✓✓× × × × ✓× ✓× × × ×\nAsk Data [2], [242]\nVAST’19\nProprietary\nB/G/L/M/P/S/T\nTemplate\n✓✓✓✓✓✓✓✓× ✓✓× ✓✓✓× × × ×\nData2Vis [51]\nCGA’19\nSeq2Seq\nB/A/L/S/T/St\nSeq2Seq\n× ✓× × ✓✓✓✓× × × × ✓× ✓× × × ×\nHearst et al. [78]\nVIS’19\nWordNet\n-\n-\n✓× ✓✓✓× × × × × × × × × × × × × ×\nVoder [225]\nTVCG’19\nStanford Parser\nSt/Bo/B/S/D\nTemplate\n✓✓✓× ✓✓✓✓× × × × ✓× ✓× × ✓×\nAUDiaL [169]\nLNCS’20\nCoreNLP\nB/L\nKnowledge Base\n✓✓✓× ✓✓✓✓× ✓× × ✓× ✓× × × ×\nBacci et al. [10]\nLNCS’20\nWit.ai\nP/L/B/S\nTemplate\n✓✓✓× ✓✓✓✓× × ✓× ✓× ✓× × × ×\nDataBreeze [227]\nTVCG’20\nCoreNLP/NLTK\nS/Uc\nTemplate\n✓✓✓× ✓✓✓✓✓✓× ✓✓× ✓× × × ×\nFlowSense [273]\nTVCG’20\nCoreNLP/SEMPRE\nL/S/B/M/N/Ta\nTemplate\n✓✓✓× ✓✓✓✓× × ✓× ✓× ✓× × × ×\nInChorus [226]\nCHI’20\nCoreNLP/NLTK\nL/S/B\nTemplate\n✓✓✓× ✓✓✓✓✓× × ✓✓× ✓× × × ×\nNL4DV Quda [61]\narXiv’20\nCoreNLP\nSt/B/L/A/P/S/Bo/H Template\n✓✓✓× ✓✓✓✓× ✓× ✓✓× ✓× × × ×\nSneak Pique [206]\nUIST’20\nANTLR\n-\n-\n✓✓✓× ✓× × × × × ✓× × × × × × × ×\nStory Analyzer [165]\nJCIS’20\nCoreNLP\nWc/C/T/Fg/B/M/I\nTemplate\n✓× ✓× × ✓✓✓× × × × × × ✓× ✓× ×\nText-to-Viz [41]\nTVCG’20\nStanford Parser\nI\nTemplate\n✓✓✓× ✓× × ✓× × × × × × ✓× ✓✓×\nVis-Annotator [122]\nCHI’20\nSpacy/Mask-RCNN\nB/L/P\n-\n✓× ✓× ✓× × × × × × × × × ✓✓× × ×\nSentiﬁers [207]\nVIS’20\nANTLR\n-\n-\n✓✓✓✓✓× × × × × × × ✓✓× × × × ×\nNL4DV [174]\nTVCG’21\nCoreNLP\nSt/B/L/A/P/S/Bo/H Template\n✓✓✓× ✓✓✓✓× ✓× ✓✓× ✓× × × ×\nData@Hand [115]\nCHI’21\nCompromise/Chrono\nB/L/S/Ra/Bo\nTemplate\n✓✓✓× ✓✓✓✓✓✓× ✓✓× ✓× × × ×\nRetrieve-Then-Adapt [183] TVCG’21\nTemplate\nI\nTemplate\n✓× × × × × × × × × × × ✓× ✓✓✓✓×\nADVISor [143]\nPaciﬁcVis’21 End-to-end network\nB/L/S\nTemplate\n✓✓✓× ✓✓✓✓× × × × ✓× ✓✓× × ✓\nSeq2Vis [151]\nSIGMOD’21 Sep-to-Sep model\nB/L/S/P\nSeq2Seq\n× × ✓× ✓✓✓✓× × × × × × ✓× × × ×\nncNet [150]\nVIS’21\nTransformer-based model\nB/L/S/P\nTemplate\n× × ✓× ✓✓✓✓× × × × × × ✓× × × ×\nGeoSneakPique [204]\nVIS’21\nANTLR\n-\n-\n✓✓✓× ✓× × × × × ✓× × × × × × × ×\nSnowy [229]\nUIST’21\nCoreNLP\n-\n-\n✓✓✓× ✓× × × × × ✓× × ✓× × × × ×\nDT2VIS [93]\nCGA’21\nCoreNLP\nB/L/S/P/Bo/R\nTemplate\n✓✓✓× ✓✓✓✓× × ✓× ✓✓✓× × × ✓\n* Abbreviations for Visualization Type: Bar(B), Table(Ta), Infographic(I), Scatter(S), Radar(R), Line(L), Pie(P), Boxplot(Bo), Icon(Ic), Map(M), Heatmap(H), Timeline(Tl),\nArea(A), Network(N), Tree(Tr), Strip(St), Donut(D), Gantt(G), Word clouds(Wc), Force graph(Fg), Range(Ra), Unit column charts(Uc), and Graphics(Gr).\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 3\nTABLE2\nRepresentativepapersofrelatedworkstoV-NLI.RelatedStagecolumnliststheoverlapwithV-NLIstagesinFigure3thatapplysimilartechnologies.\nTopic RelatedStage Survey RepresentativePaper\n[32],[37],[42],[46],[51],[70],[84],[86],[106],[109],[114],[130],[138],\nVisualizationRecommendation(VisRec) B/C/D/E/G [187],[289] [142],[149],[152],[154],[168],[170],[185],[186],[203],[234],[246],[247],\n[253],[259],[260],[261],[263],[276],[277],[288]\n[13],[14],[15],[19],[22],[55],[72],[73],[74],[80],[87],[92],[95],[118],\nNaturalLanguageInterfaceforDataBase(NLI4DB) A/B/E/F/G [6],[290] [135],[136],[137],[141],[173],[175],[188],[193],[202],[210],[220],[221],\n[252],[268],[270],[274],[285],[287]\nConversationalInterfaceforVisualization(CIV) A/B/C/D/E/F/G [20],[58],[77],[129],[134],[171],[172],[201],[216]\nNaturalLanguageGenerationforVisualization(NLG4Vis) B/E/G [39],[42],[44],[45],[59],[111],[144],[166],[167],[177],[184],[223]\nVisualQuestionAnswering(VQA) A/B/E/F/G [158] [27],[30],[97],[98],[110],[143],[155],[222],[258],[269]\n[11],[26],[32],[33],[34],[35],[36],[41],[62],[64],[65],[82],[89],[101],\nAnnotationandNarrativeStorytelling A/B/C/D/E/F/G [200],[241] [112],[114],[119],[122],[123],[146],[162],[165],[183],[190],[213],[240],\n[254],[256],[257],[265],[275]\nTABLE3\nSummaryofrepresentativeworksinV-NLI.Thefirstfivecolumnslistbasicinformation,andtheleftcolumnsarecharacteristicsofV-NLIwith\ncategoricaldimensionsdescribedinSection3.DetailsofeachcolumnwillbediscussedinSections4-10.\n4 5 6 7 8 9 10\nVisualization Recommendation\nName Publication NLPToolkitorTechnology\nType Algorithm\nsisylanAxatnySdnacitnameS\necnerefnIksaT\necnerefnIsetubirttAataD\nsecnarettUdefiicepsrednUroftluafeD\nsthgisnIataDrofnoitamrofsnarT\ngnippaMetartsbuSlaitapS\ngnippaMstnemelElacihparG gnippaMseitreporPlacihparG\nnoitamrofsnarTweiV\ntegdiWytiugibmA\nnoitelpmocotuA\nladomitluM\nPMIW\ntnemeganaMeugolaiD\nnoitatneserPlausiV\nnoitatonnA\ngnilletyrotSevitarraN\nnoitareneGnoitpircseDLN gnirewsnAnoitseuQlausiV\nCoxetal.[40] IJST’01 Sisl B/Ta InfoStill × (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × × × × (cid:88) (cid:88) × × × ×\nKatoetal.[105] COLING’02 Logicalform B/L/P/A Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × × × × × (cid:88) × × × ×\nRIA[284] InfoVis’05 - I Optimization (cid:88) (cid:88)(cid:88) × (cid:88) × × × × × × (cid:88) × (cid:88) (cid:88) × (cid:88) × ×\nArticulate[239] LNCS’10 StanfordParser S/R/B/L/P/Bo Decisiontree (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × × × × × (cid:88) × × × ×\nContextifier[89] CHI’13 NLTK L - (cid:88) × (cid:88) × (cid:88) × × × × × × × × × (cid:88)(cid:88) (cid:88) × ×\nNewsViews[64] CHI’14 OpenCalais M - (cid:88) (cid:88)(cid:88) × (cid:88) × × × × × × × × × (cid:88)(cid:88) (cid:88) × ×\nDatatone[63] UIST’15 NLTK/StanfordParser S/B/L Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) × × (cid:88) × (cid:88) × × × ×\nArticulate2[120],[121] SIGDIAL’16 ClearNLP/StanfordParser/OpenNLPB/L/H Decisiontree (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) × × (cid:88) × (cid:88) (cid:88) × × × ×\nEviza[205] UIST’16 ANTLR M/L/B/S Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) × × × ×\nTimeLineCurator[62] TVCG’16 TERNIP Tl Template (cid:88) (cid:88)(cid:88) × × × × × × × × × × × (cid:88)(cid:88) (cid:88) (cid:88) ×\nAnalyza[50] IUI’17 StanfordParser B/L Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) × × × ×\nTSI[26] TVCG’17 Template L/A/H - × (cid:88)(cid:88) × (cid:88) × × × × × × × × × (cid:88)(cid:88) (cid:88) × ×\nAva[134] CIDR’17 ControlledNaturalLanguage S/L/B Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × × × × (cid:88) (cid:88) × × × (cid:88)\nDeepEye[148] SIGMOD’18 OpenNLP P/L/B/S Template (cid:88) × (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × × × (cid:88) × (cid:88) × × × ×\nEvizeon[83] TVCG’18 CoreNLP M/L/B/S Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) × × × ×\nIris[58] CHI’18 Domain-specificlanguage S/T Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × × × × (cid:88) (cid:88) × × × (cid:88)\nMetoyeretal.[162] IUI’18 CoreNLP I Template (cid:88) × (cid:88) × (cid:88) × × × × × × × (cid:88) × (cid:88) × (cid:88) × (cid:88)\nOrko[231] TVCG’18 CoreNLP/NLTK N - (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) × (cid:88)(cid:88) (cid:88) (cid:88) × × × ×\nShapeSearch[217],[218] VLDB’18 StanfordParser L ShapeQuery (cid:88) × (cid:88) × (cid:88) (cid:88) × (cid:88) × (cid:88) × × (cid:88) × (cid:88) × × × ×\nValletto[103] CHI’18 Spacy B/S/M Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) × × (cid:88)(cid:88) (cid:88) (cid:88) × × × ×\nArkLang[208] IUI’19 Cocke-Kasami-Younger B/G/L/M/P/S/Tr Template (cid:88) (cid:88)(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) × × × × (cid:88) × (cid:88) × × × ×\nAskData[2],[242] VAST’19 Proprietary B/G/L/M/P/S/T Template (cid:88) (cid:88)(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) × × × ×\nData2Vis[51] CGA’19 Seq2Seq B/A/L/S/T/St Seq2Seq × (cid:88) × × (cid:88) (cid:88) (cid:88) (cid:88) × × × × (cid:88) × (cid:88) × × × ×\nHearstetal.[78] VIS’19 WordNet - - (cid:88) × (cid:88) (cid:88) (cid:88) × × × × × × × × × × × × × ×\nVoder[225] TVCG’19 StanfordParser St/Bo/B/S/D Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × × × (cid:88) × (cid:88) × × (cid:88) ×\nAUDiaL[169] LNCS’20 CoreNLP B/L KnowledgeBase (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) × × (cid:88) × (cid:88) × × × ×\nBaccietal.[10] LNCS’20 Wit.ai P/L/B/S Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × (cid:88) × (cid:88) × (cid:88) × × × ×\nDataBreeze[227] TVCG’20 CoreNLP/NLTK S/Uc Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88)(cid:88) × (cid:88) × × × ×\nFlowSense[273] TVCG’20 CoreNLP/SEMPRE L/S/B/M/N/Ta Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × (cid:88) × (cid:88) × (cid:88) × × × ×\nInChorus[226] CHI’20 CoreNLP/NLTK L/S/B Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) × × (cid:88)(cid:88) × (cid:88) × × × ×\nNL4DV Quda[61] arXiv’20 CoreNLP St/B/L/A/P/S/Bo/HTemplate (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) × (cid:88)(cid:88) × (cid:88) × × × ×\nSneakPique[206] UIST’20 ANTLR - - (cid:88) (cid:88)(cid:88) × (cid:88) × × × × × (cid:88) × × × × × × × ×\nStoryAnalyzer[165] JCIS’20 CoreNLP Wc/C/T/Fg/B/M/I Template (cid:88) × (cid:88) × × (cid:88) (cid:88) (cid:88) × × × × × × (cid:88) × (cid:88) × ×\nText-to-Viz[41] TVCG’20 StanfordParser I Template (cid:88) (cid:88)(cid:88) × (cid:88) × × (cid:88) × × × × × × (cid:88) × (cid:88) (cid:88) ×\nVis-Annotator[122] CHI’20 Spacy/Mask-RCNN B/L/P - (cid:88) × (cid:88) × (cid:88) × × × × × × × × × (cid:88)(cid:88) × × ×\nSentifiers[207] VIS’20 ANTLR - - (cid:88) (cid:88)(cid:88) (cid:88) (cid:88) × × × × × × × (cid:88) (cid:88) × × × × ×\nNL4DV[174] TVCG’21 CoreNLP St/B/L/A/P/S/Bo/HTemplate (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) × (cid:88)(cid:88) × (cid:88) × × × ×\nData@Hand[115] CHI’21 Compromise/Chrono B/L/S/Ra/Bo Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88)(cid:88) × (cid:88) × × × ×\nRetrieve-Then-Adapt[183]TVCG’21 Template I Template (cid:88) × × × × × × × × × × × (cid:88) × (cid:88)(cid:88) (cid:88) (cid:88) ×\nADVISor[143] PacificVis’21End-to-endnetwork B/L/S Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × × × (cid:88) × (cid:88)(cid:88) × × (cid:88)\nSeq2Vis[151] SIGMOD’21 Sep-to-Sepmodel B/L/S/P Seq2Seq × × (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × × × × × (cid:88) × × × ×\nncNet[150] VIS’21 Transformer-basedmodel B/L/S/P Template × × (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × × × × × (cid:88) × × × ×\nGeoSneakPique[204] VIS’21 ANTLR - - (cid:88) (cid:88)(cid:88) × (cid:88) × × × × × (cid:88) × × × × × × × ×\nSnowy[229] UIST’21 CoreNLP - - (cid:88) (cid:88)(cid:88) × (cid:88) × × × × × (cid:88) × × (cid:88) × × × × ×\nDT2VIS[93] CGA’21 CoreNLP B/L/S/P/Bo/R Template (cid:88) (cid:88)(cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × × (cid:88) × (cid:88) (cid:88) (cid:88) × × × (cid:88)\n*Abbreviations for Visualization Type: Bar(B), Table(Ta), Infographic(I), Scatter(S), Radar(R), Line(L), Pie(P), Boxplot(Bo), Icon(Ic), Map(M), Heatmap(H), Timeline(Tl),\nArea(A),Network(N),Tree(Tr),Strip(St),Donut(D),Gantt(G),Wordclouds(Wc),Forcegraph(Fg),Range(Ra),Unitcolumncharts(Uc),andGraphics(Gr).",
    "tables_extraction_text_csv": "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14\n,,TABLE 2,,,,,,,,,,,,\nRepresentative papers of related works to V-NLI. Related Stage column lists the overlap with V-NLI stages in Figure 3 that apply similar technologies.,,,,,,,,,,,,,,\nTopic,Related Stage,Survey,,,,,,Representative Paper,,,,,,\n,,,\"[32],\",\"[37],\",\"[42],\",\"[46],\n[51],\",\"[70],\",\"[84],\",\"[86],\",\"[106],\",\"[109],\",\"[114],\",\"[130],\",\"[138],\"\nVisualization Recommendation (VisRec),B/C/D/E/G,\"[187], [289]\",\"[142],\",\"[149],\",\"[152],\",\"[154],\",\"[168],\",\"[170],\",\"[185],\",\"[186],\",\"[203],\",\"[234],\",\"[246],\",\"[247],\"\n,,,,\"[253], [259], [260], [261], [263], [276], [277], [288]\",,,,,,,,,,\n,,,\"[13],\",\"[14],\",\"[15],\",\"[19],\n[22],\",\"[55],\",\"[72],\",\"[73],\",\"[74],\",\"[80],\n[87],\",\"[92],\",\"[95],\",\"[118],\"\nNatural Language Interface for DataBase (NLI4DB),A/B/E/F/G,\"[6], [290]\",\"[135],\",\"[136],\",\"[137],\",\"[141],\",\"[173],\",\"[175],\",\"[188],\",\"[193],\",\"[202],\",\"[210],\",\"[220],\",\"[221],\"\n,,,,\"[252], [268], [270], [274], [285], [287]\",,,,,,,,,,\nConversational Interface for Visualization (CIV),A/B/C/D/E/F/G,,,\"[20], [58], [77], [129], [134], [171], [172], [201], [216]\",,,,,,,,,,\nNatural Language Generation for Visualization (NLG4Vis),B/E/G,,,\"[39], [42], [44], [45], [59], [111], [144], [166], [167], [177], [184], [223]\",,,,,,,,,,\nVisual Question Answering (VQA),A/B/E/F/G,[158],,\"[27], [30], [97], [98], [110], [143], [155], [222], [258], [269]\",,,,,,,,,,\n,,,\"[11],\",\"[26],\",\"[32],\",\"[33],\n[34],\",\"[35],\",\"[36],\",\"[41],\",\"[62],\",\"[64],\n[65],\",\"[82],\",\"[89],\",\"[101],\"\nAnnotation and Narrative Storytelling,A/B/C/D/E/F/G,\"[200], [241]\",\"[112],\",\"[114],\",\"[119],\",\"[122],\",\"[123],\",\"[146],\",\"[162],\",\"[165],\",\"[183],\",\"[190],\",\"[213],\",\"[240],\"\n,,,,\"[254], [256], [257], [265], [275]\",,,,,,,,,,\n\n\n--- NEW TABLE ---\n\n0,1,2,3,4,5\n,,Visualization,Recommendation,\"SemanticandSyntaxAnalysis\nTaskInference\nDataAttributesInference\nSpatialSubstrateMapping\nGraphicalElementsMapping\nGraphicalPropertiesMapping\nViewTransformation\nAmbiguityWidget\nAutocompletion\nMultimodal\nWIMP\nDialogueManagement\nVisualPresentation\nAnnotation\nNarrativeStorytelling\",\"NLDescriptionGeneration\nVisualQuestionAnswering\"\nName,\"Publication\nNLP Toolkit or Technology\",,,,\n,,Type,Algorithm,,\nCox et al. [40],\"IJST’01\nSisl\",B/Ta,InfoStill,\"× (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n×\n× (cid:88) (cid:88) ×\n×\",\"×\n×\"\nKato et al. [105],\"COLING’02\nLogical form\",B/L/P/A,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n×\n×\n× (cid:88) ×\n×\",\"×\n×\"\nRIA [284],\"InfoVis’05\n-\",I,Optimization,\"(cid:88) (cid:88) (cid:88) × (cid:88) ×\n×\n×\n×\n×\n× (cid:88) × (cid:88) (cid:88) × (cid:88) ×\",×\nArticulate [239],\"LNCS’10\nStanford Parser\",S/R/B/L/P/Bo,Decision tree,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n×\n×\n× (cid:88) ×\n×\",\"×\n×\"\nContextiﬁer [89],\"CHI’13\nNLTK\",L,-,\"(cid:88) × (cid:88) × (cid:88) ×\n×\n×\n×\n×\n×\n×\n×\",\"× (cid:88) (cid:88) (cid:88) ×\n×\"\nNewsViews [64],\"CHI’14\nOpenCalais\",M,-,\"(cid:88) (cid:88) (cid:88) × (cid:88) ×\n×\n×\n×\n×\n×\n×\n×\",\"× (cid:88) (cid:88) (cid:88) ×\n×\"\nDatatone [63],\"UIST’15\nNLTK/Stanford Parser\",S/B/L,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) ×\n× (cid:88) × (cid:88) ×\n×\",\"×\n×\"\n\"Articulate2 [120], [121]\",SIGDIAL’16 ClearNLP/Stanford Parser/OpenNLP B/L/H,,Decision tree,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) ×\n× (cid:88) × (cid:88) (cid:88) ×\n×\",\"×\n×\"\nEviza [205],\"UIST’16\nANTLR\",M/L/B/S,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) ×\n×\",\"×\n×\"\nTimeLineCurator [62],\"TVCG’16\nTERNIP\",Tl,Template,\"(cid:88) (cid:88) (cid:88) ×\n×\n×\n×\n×\n×\n×\n×\n×\n×\",× (cid:88) (cid:88) (cid:88) (cid:88) ×\nAnalyza [50],\"IUI’17\nStanford Parser\",B/L,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) ×\n×\",\"×\n×\"\nTSI [26],\"TVCG’17\nTemplate\",L/A/H,-,\"× (cid:88) (cid:88) × (cid:88) ×\n×\n×\n×\n×\n×\n×\n×\",\"× (cid:88) (cid:88) (cid:88) ×\n×\"\nAva [134],\"CIDR’17\nControlled Natural Language\",S/L/B,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n×\n× (cid:88) (cid:88) ×\n×\",× (cid:88)\nDeepEye [148],SIGMOD’18 OpenNLP,P/L/B/S,Template,\"(cid:88) × (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n× (cid:88) × (cid:88) ×\n×\",\"×\n×\"\nEvizeon [83],\"TVCG’18\nCoreNLP\",M/L/B/S,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) ×\n×\",\"×\n×\"\nIris [58],\"CHI’18\nDomain-speciﬁc language\",S/T,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n×\n× (cid:88) (cid:88) ×\n×\",× (cid:88)\nMetoyer et al. [162],\"IUI’18\nCoreNLP\",I,Template,\"(cid:88) × (cid:88) × (cid:88) ×\n×\n×\n×\n×\n×\",× (cid:88) × (cid:88) × (cid:88) × (cid:88)\nOrko [231],\"TVCG’18\nCoreNLP/NLTK\",N,-,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\",\"×\n×\"\n\"ShapeSearch [217], [218]\",\"VLDB’18\nStanford Parser\",L,ShapeQuery,\"(cid:88) × (cid:88) × (cid:88) (cid:88) × (cid:88) × (cid:88) ×\n× (cid:88) × (cid:88) ×\n×\",\"×\n×\"\nValletto [103],\"CHI’18\nSpacy\",B/S/M,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) ×\n× (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\",\"×\n×\"\nArkLang [208],\"IUI’19\nCocke-Kasami-Younger\",B/G/L/M/P/S/Tr,Template,\"(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n× (cid:88) × (cid:88) ×\n×\",\"×\n×\"\n\"Ask Data [2], [242]\",\"VAST’19\nProprietary\",B/G/L/M/P/S/T,Template,\"(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) ×\n×\",\"×\n×\"\nData2Vis [51],\"CGA’19\nSeq2Seq\",B/A/L/S/T/St,Seq2Seq,\"× (cid:88) ×\n× (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n× (cid:88) × (cid:88) ×\n×\",\"×\n×\"\nHearst et al. [78],\"VIS’19\nWordNet\",-,-,\"(cid:88) × (cid:88) (cid:88) (cid:88) ×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\n×\",\"×\n×\"\nVoder [225],\"TVCG’19\nStanford Parser\",St/Bo/B/S/D,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n× (cid:88) × (cid:88) ×\",× (cid:88) ×\nAUDiaL [169],\"LNCS’20\nCoreNLP\",B/L,Knowledge Base,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) ×\n× (cid:88) × (cid:88) ×\n×\",\"×\n×\"\nBacci et al. [10],\"LNCS’20\nWit.ai\",P/L/B/S,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n× (cid:88) × (cid:88) × (cid:88) ×\n×\",\"×\n×\"\nDataBreeze [227],\"TVCG’20\nCoreNLP/NLTK\",S/Uc,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) × (cid:88) ×\n×\",\"×\n×\"\nFlowSense [273],\"TVCG’20\nCoreNLP/SEMPRE\",L/S/B/M/N/Ta,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n× (cid:88) × (cid:88) × (cid:88) ×\n×\",\"×\n×\"\nInChorus [226],\"CHI’20\nCoreNLP/NLTK\",L/S/B,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) ×\n× (cid:88) (cid:88) × (cid:88) ×\n×\",\"×\n×\"\nNL4DV Quda [61],\"arXiv’20\nCoreNLP\",,St/B/L/A/P/S/Bo/H Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) × (cid:88) (cid:88) × (cid:88) ×\n×\",\"×\n×\"\nSneak Pique [206],\"UIST’20\nANTLR\",-,-,\"(cid:88) (cid:88) (cid:88) × (cid:88) ×\n×\n×\n×\n× (cid:88) ×\n×\n×\n×\n×\n×\",\"×\n×\"\nStory Analyzer [165],\"JCIS’20\nCoreNLP\",Wc/C/T/Fg/B/M/I,Template,\"(cid:88) × (cid:88) ×\n× (cid:88) (cid:88) (cid:88) ×\n×\n×\n×\n×\",\"× (cid:88) × (cid:88) ×\n×\"\nText-to-Viz [41],\"TVCG’20\nStanford Parser\",I,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) ×\n× (cid:88) ×\n×\n×\n×\n×\",× (cid:88) × (cid:88) (cid:88) ×\nVis-Annotator [122],\"CHI’20\nSpacy/Mask-RCNN\",B/L/P,-,\"(cid:88) × (cid:88) × (cid:88) ×\n×\n×\n×\n×\n×\n×\n×\n× (cid:88) (cid:88) ×\",\"×\n×\"\nSentiﬁers [207],\"VIS’20\nANTLR\",-,-,\"(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n×\n×\n×\n× (cid:88) (cid:88) ×\n×\n×\",\"×\n×\"\nNL4DV [174],\"TVCG’21\nCoreNLP\",,St/B/L/A/P/S/Bo/H Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) × (cid:88) (cid:88) × (cid:88) ×\n×\",\"×\n×\"\nData@Hand [115],\"CHI’21\nCompromise/Chrono\",B/L/S/Ra/Bo,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) × (cid:88) ×\n×\",\"×\n×\"\nRetrieve-Then-Adapt [183] TVCG’21,Template,I,Template,\"(cid:88) ×\n×\n×\n×\n×\n×\n×\n×\n×\n×\",× (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\nADVISor [143],PaciﬁcVis’21 End-to-end network,B/L/S,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n× (cid:88) × (cid:88) (cid:88) ×\",× (cid:88)\nSeq2Vis [151],SIGMOD’21 Sep-to-Sep model,B/L/S/P,Seq2Seq,\"×\n× (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n×\n×\n× (cid:88) ×\n×\",\"×\n×\"\nncNet [150],\"VIS’21\nTransformer-based model\",B/L/S/P,Template,\"×\n× (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n×\n×\n×\n×\n× (cid:88) ×\n×\",\"×\n×\"\nGeoSneakPique [204],\"VIS’21\nANTLR\",-,-,\"(cid:88) (cid:88) (cid:88) × (cid:88) ×\n×\n×\n×\n× (cid:88) ×\n×\n×\n×\n×\n×\",\"×\n×\"\nSnowy [229],\"UIST’21\nCoreNLP\",-,-,\"(cid:88) (cid:88) (cid:88) × (cid:88) ×\n×\n×\n×\n× (cid:88) ×\n× (cid:88) ×\n×\n×\",\"×\n×\"\nDT2VIS [93],\"CGA’21\nCoreNLP\",B/L/S/P/Bo/R,Template,\"(cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) ×\n× (cid:88) × (cid:88) (cid:88) (cid:88) ×\n×\",× (cid:88)\n* Abbreviations,\"for Visualization Type: Bar(B), Table(Ta),\",\"Infographic(I), Scatter(S), Radar(R), Line(L), Pie(P), Boxplot(Bo),\",,,\"Icon(Ic), Map(M), Heatmap(H), Timeline(Tl),\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 4,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n4\nData\nTransformation\nVisual\nMapping\nView\nTransformation\nRaw data\nTransformed\nData\nVisual\nStructures\nViews\nHuman Interaction\nNL\nquery\nA\nData Space\nVisualization Space\nF\nD\nC\nB\nE\nG\nVisualization-oriented Natural Language Interface \nFig. 3. Extension of classic information visualization pipeline proposed by Card et al. [28] with V-NLI. It depicts how V-NLI works to construct\nvisualizations, which consists of the following seven stages: (A) Query Interpretation, (B) Data Transformation, (C) Visual Mapping, (D) View\nTransformation, (E) Human Interaction, (F) Dialogue Management, and (G) Presentation.\ntypes supported, visualization recommendation algorithm adopted,\nand various characteristics in V-NLI, which will be discussed in\nSections 4 - 10. On this basis, we proceeded to a comprehensive\nanalysis of the 57 papers to systematically understand the main\nresearch trends. We also collected 283 papers of related works\ndescribed in Section 2.1. Table 2 lists representative papers of each\ntopic for reference, which will be selectively discussed with V-NLI\ncharacteristics in the subsequent sections. More information can be\nfound at https://v-nlis.github.io/V-NLIs-survey/.\n3\nCLASSIFICATION OVERVIEW\nThe information visualization pipeline presented by Card et al.\n[28] (see Figure 3) describes how the raw data transits into\nvisualizations and interacts with the user. We extend this pipeline\nwith an additional V-NLI layer (Green colored in Figure 3). On\nthis basis, we move forward to develop categorical dimensions\nby focusing on how V-NLI facilitates visualization generation.\nInspired by McNabb et al. [160], to facilitate the categorization\nprocess, the following stages in the pipeline are used:\n• Query Interpretation (A): Since we add the V-NLI layer in the\npipeline, query interpretation is a foundational stage. Semantic\nand syntax analysis is generally performed ﬁrst to discover\nhierarchical structures of the NL queries so that the system can\nparse relevant information in terms of data attributes and analytic\ntasks. Due to the vague nature of natural language, dealing with\nthe underspeciﬁed utterances is another essential task in this\nstage. Details can be found in Section 4.\n• Data Transformation (B): In the original pipeline, this stage\nmainly plays a role in transforming raw data into data tables along\nwith various operations (e.g., aggregation and pivot). Since the\nmajority of raw data to analyze is already in a tabular format, we\nrename it transformed data. Data transformation is responsible for\ngenerating alternative data subsets or derivations for visualization.\nAll operations on the data plane belong to this stage. Details can\nbe found in Section 5.\n• Visual Mapping (C): This stage focuses on mapping the\ninformation extracted from NL queries to visual structures. The\nthree elements of visual mapping for information visualization\nare spatial substrate, graphical elements, and graphical properties\n[28]. Spatial substrate is the space to create visualizations, and\nit is essential to consider the layout conﬁguration. Graphical\nelements are the visual elements appearing in the spatial substrate\n(e.g., points, lines, surfaces, and volumes). Graphical properties\ncan be implemented on the graphical elements to make them\nmore noticeable (e.g., size, orientation, color, textures, and\nshapes). Details can be found in Section 6.\n• View Transformation (D): View transformation (rendering)\ntransforms visual structures into views by specifying graphical\nproperties that turn these structures into pixels. Common forms\ninclude navigation, animation, and visual distortion (e.g., ﬁsheye\nlens). However, in the context of our survey, this stage is rarely\ninvolved in V-NLI. Details can be found in Section 7.\n• Human Interaction (E): Human interactions with the visualiza-\ntion interface feed back into the pipeline. A user can connect\nwith a visualization manually by modifying or transforming a\nview state, or by reviewing the use, effectiveness, and knowledge\non the visualization. Dimara et al. [52] deﬁned interaction for\nvisualization as: “The interplay between a person and a data\ninterface involving a data-related intent, at least one action from\nthe person and an interface reaction that is perceived as such.”\nDescribing interaction requires all the mandatory components:\ninterplay, user, and interface. Details can be found in Section 8.\n• Dialogue Management (F): Interpreting an utterance contextu-\nally is essential for visualization system intelligence, which is\nparticularly evident in V-NLI. This stage involves each step in\nthe pipeline and concentrates on facilitating a conversation with\nthe system based on the current visualization state and previous\nutterances. Details can be found in Section 9.\n• Presentation (G): We name this stage as Presentation. The\nclassic pipeline focuses on how to generate visualizations but\nignores presenting them to the user. With natural language\nintegrated into the pipeline, the vast majority of V-NLI systems\naccept natural language as input and directly display generated\nvisualizations. Furthermore, complementing visualizations with\nnatural language can provide additional surprises to the user.\nDetails can be found in Section 10.\nThe pipeline is meant to model work activities. A single\nutterance can cover multiple stages (e.g., data transformation, visual\nmapping, and human interaction), and users can iterate over any of\nthese stages. An utterance can also be interpreted contextually. The\nfollowing seven sections will discuss the seven stages accordingly.\n4\nQUERY INTERPRETATION\nQuery interpretation is the foundation of all subsequent stages. This\nsection will discuss how to perform semantic and syntax analysis\nof the input natural language queries, infer analytic tasks of the\nuser and data attributes to be analyzed, and make defaults for\nunderspeciﬁed utterances.\n4.1\nSemantic and Syntax Analysis\nSemantic and syntax analysis can be powerful to discover hier-\narchical structures and understand meanings in human language.\nThe semantic parser can conduct a series of NLP sub-tasks on\nthe query string to extract valuable details that can be used to\ndetect relevant phrases. The processing steps include tokenization,\nidentifying parts-of-speech (POS) tags, recognizing name entities,\nremoving stop words, performing stemming, creating a dependency\ntree, generating N-grams, etc. For example, Flowsense [273] is a\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 4\nVisualization-oriented Natural Language Interface F\nB C D\nNL\nData Visual View G query\nTransformation Transformed Mapping Visual Transformation\nRaw data Data Structures Views A\nE Human Interaction\nData Space Visualization Space\nFig.3.ExtensionofclassicinformationvisualizationpipelineproposedbyCardetal.[28]withV-NLI.ItdepictshowV-NLIworkstoconstruct\nvisualizations, which consists of the following seven stages: (A) Query Interpretation, (B) Data Transformation, (C) Visual Mapping, (D) View\nTransformation,(E)HumanInteraction,(F)DialogueManagement,and(G)Presentation.\ntypessupported,visualizationrecommendationalgorithmadopted, includenavigation,animation,andvisualdistortion(e.g.,fisheye\nandvariouscharacteristicsinV-NLI,whichwillbediscussedin lens).However,inthecontextofoursurvey,thisstageisrarely\nSections4-10.Onthisbasis,weproceededtoacomprehensive involvedinV-NLI.DetailscanbefoundinSection7.\nanalysis of the 57 papers to systematically understand the main • HumanInteraction(E):Humaninteractionswiththevisualiza-\nresearch trends. We also collected 283 papers of related works tion interface feed back into the pipeline. A user can connect\ndescribedinSection2.1.Table2listsrepresentativepapersofeach with a visualization manually by modifying or transforming a\ntopicforreference,whichwillbeselectivelydiscussedwithV-NLI viewstate,orbyreviewingtheuse,effectiveness,andknowledge\ncharacteristicsinthesubsequentsections.Moreinformationcanbe onthevisualization.Dimaraetal.[52]definedinteractionfor\nfoundathttps://v-nlis.github.io/V-NLIs-survey/. visualization as: “The interplay between a person and a data\ninterfaceinvolvingadata-relatedintent,atleastoneactionfrom\n3 CLASSIFICATION OVERVIEW thepersonandaninterfacereactionthatisperceivedassuch.”\nThe information visualization pipeline presented by Card et al. Describinginteractionrequiresallthemandatorycomponents:\n[28] (see Figure 3) describes how the raw data transits into interplay,user,andinterface.DetailscanbefoundinSection8.\nvisualizationsandinteractswiththeuser.Weextendthispipeline • DialogueManagement(F):Interpretinganutterancecontextu-\nwith an additional V-NLI layer (Green colored in Figure 3). On allyisessentialforvisualizationsystemintelligence,whichis\nthis basis, we move forward to develop categorical dimensions particularlyevidentinV-NLI.Thisstageinvolveseachstepin\nby focusing on how V-NLI facilitates visualization generation. thepipelineandconcentratesonfacilitatingaconversationwith\nInspired by McNabb et al. [160], to facilitate the categorization thesystembasedonthecurrentvisualizationstateandprevious\nprocess,thefollowingstagesinthepipelineareused: utterances.DetailscanbefoundinSection9.\n• QueryInterpretation(A):SinceweaddtheV-NLIlayerinthe • Presentation (G): We name this stage as Presentation. The\nclassic pipeline focuses on how to generate visualizations but\npipeline,queryinterpretationisafoundationalstage.Semantic\nignores presenting them to the user. With natural language\nand syntax analysis is generally performed first to discover\nintegratedintothepipeline,thevastmajorityofV-NLIsystems\nhierarchicalstructuresoftheNLqueriessothatthesystemcan\nacceptnaturallanguageasinputanddirectlydisplaygenerated\nparserelevantinformationintermsofdataattributesandanalytic\nvisualizations.Furthermore,complementingvisualizationswith\ntasks.Duetothevaguenatureofnaturallanguage,dealingwith\nnatural language can provide additional surprises to the user.\nthe underspecified utterances is another essential task in this\nDetailscanbefoundinSection10.\nstage.DetailscanbefoundinSection4.\n• Data Transformation (B): In the original pipeline, this stage The pipeline is meant to model work activities. A single\nmainlyplaysaroleintransformingrawdataintodatatablesalong utterancecancovermultiplestages(e.g.,datatransformation,visual\nwithvariousoperations(e.g.,aggregationandpivot).Sincethe mapping,andhumaninteraction),anduserscaniterateoveranyof\nmajorityofrawdatatoanalyzeisalreadyinatabularformat,we thesestages.Anutterancecanalsobeinterpretedcontextually.The\nrenameittransformeddata.Datatransformationisresponsiblefor followingsevensectionswilldiscussthesevenstagesaccordingly.\ngeneratingalternativedatasubsetsorderivationsforvisualization.\n4 QUERY INTERPRETATION\nAlloperationsonthedataplanebelongtothisstage.Detailscan\nbefoundinSection5. Queryinterpretationisthefoundationofallsubsequentstages.This\n• Visual Mapping (C): This stage focuses on mapping the sectionwilldiscusshowtoperformsemanticandsyntaxanalysis\ninformationextractedfromNLqueriestovisualstructures.The of the input natural language queries, infer analytic tasks of the\nthreeelementsofvisualmappingforinformationvisualization user and data attributes to be analyzed, and make defaults for\narespatialsubstrate,graphicalelements,andgraphicalproperties underspecifiedutterances.\n[28].Spatialsubstrateisthespacetocreatevisualizations,and\n4.1 SemanticandSyntaxAnalysis\nit is essential to consider the layout configuration. Graphical\nelementsarethevisualelementsappearinginthespatialsubstrate Semantic and syntax analysis can be powerful to discover hier-\n(e.g.,points,lines,surfaces,andvolumes).Graphicalproperties archicalstructuresandunderstandmeaningsinhumanlanguage.\ncan be implemented on the graphical elements to make them The semantic parser can conduct a series of NLP sub-tasks on\nmore noticeable (e.g., size, orientation, color, textures, and the query string to extract valuable details that can be used to\nshapes).DetailscanbefoundinSection6. detectrelevantphrases.Theprocessingstepsincludetokenization,\n• View Transformation (D): View transformation (rendering) identifyingparts-of-speech(POS)tags,recognizingnameentities,\ntransformsvisualstructuresintoviewsbyspecifyinggraphical removingstopwords,performingstemming,creatingadependency\npropertiesthatturnthesestructuresintopixels.Commonforms tree,generatingN-grams,etc.Forexample,Flowsense[273]isa",
    "tables_extraction_text_csv": "0,1\nData Space,Visualization Space\nFig. 3. Extension of classic information visualization pipeline proposed by Card et al.,\"[28] with V-NLI.\nIt depicts how V-NLI works to construct\"\n\"visualizations, which consists of\nthe following seven stages:\",\"(A) Query Interpretation,\n(B) Data Transformation,\n(C) Visual Mapping,\n(D) View\"\n\"Transformation, (E) Human Interaction, (F) Dialogue Management, and (G) Presentation.\",\n\"types supported, visualization recommendation algorithm adopted,\",\"include navigation, animation, and visual distortion (e.g., ﬁsheye\"\n\"and various characteristics in V-NLI, which will be discussed in\",\"lens). However, in the context of our survey, this stage is rarely\"\n\"Sections 4 - 10. On this basis, we proceeded to a comprehensive\",involved in V-NLI. Details can be found in Section 7.\n\"analysis of\nthe 57 papers to systematically understand the main\",• Human Interaction (E): Human interactions with the visualiza-\n\"research trends. We also collected 283 papers of\nrelated works\",tion interface feed back into the pipeline. A user can connect\ndescribed in Section 2.1. Table 2 lists representative papers of each,with a visualization manually by modifying or transforming a\n\"topic for reference, which will be selectively discussed with V-NLI\",\"view state, or by reviewing the use, effectiveness, and knowledge\"\ncharacteristics in the subsequent sections. More information can be,on the visualization. Dimara et al. [52] deﬁned interaction for\nfound at https://v-nlis.github.io/V-NLIs-survey/.,visualization as: “The interplay between a person and a data\n,\"interface involving a data-related intent, at\nleast one action from\"\n\"3\nCLASSIFICATION OVERVIEW\",the person and an interface reaction that is perceived as such.”\n,\"Describing interaction requires all\nthe mandatory components:\"\nThe information visualization pipeline presented by Card et al.,\n,\"interplay, user, and interface. Details can be found in Section 8.\"\n\"[28]\n(see Figure\n3)\ndescribes\nhow the\nraw data\ntransits\ninto\",\n,• Dialogue Management (F): Interpreting an utterance contextu-\nvisualizations and interacts with the user. We extend this pipeline,\n,\"ally is essential for visualization system intelligence, which is\"\nwith an additional V-NLI layer (Green colored in Figure 3). On,\n,\"particularly evident\nin V-NLI. This stage involves each step in\"\n\"this basis, we move forward to develop categorical dimensions\",\n,the pipeline and concentrates on facilitating a conversation with\n\"by focusing on how V-NLI\nfacilitates visualization generation.\",\n,the system based on the current visualization state and previous\n\"Inspired by McNabb et al. [160],\nto facilitate the categorization\",\n,utterances. Details can be found in Section 9.\n\"process,\nthe following stages in the pipeline are used:\",\n,\"• Presentation (G): We name this\nstage as Presentation. The\"\n• Query Interpretation (A): Since we add the V-NLI layer in the,\n,classic pipeline focuses on how to generate visualizations but\n\"pipeline, query interpretation is a foundational stage. Semantic\",\n,\"ignores presenting them to the user. With natural\nlanguage\"\n\"and syntax analysis\nis generally performed ﬁrst\nto discover\",\n,\"integrated into the pipeline,\nthe vast majority of V-NLI systems\"\nhierarchical structures of the NL queries so that the system can,\n,accept natural language as input and directly display generated\nparse relevant information in terms of data attributes and analytic,\n,\"visualizations. Furthermore, complementing visualizations with\"\n\"tasks. Due to the vague nature of natural\nlanguage, dealing with\",\n,\"natural\nlanguage can provide additional surprises to the user.\"\n\"the underspeciﬁed utterances is another essential\ntask in this\",\n,Details can be found in Section 10.\nstage. Details can be found in Section 4.,\n,\"The pipeline\nis meant\nto model work activities. A single\"\n\"• Data Transformation (B):\nIn the original pipeline,\nthis stage\",\n,\"utterance can cover multiple stages (e.g., data transformation, visual\"\nmainly plays a role in transforming raw data into data tables along,\n,\"mapping, and human interaction), and users can iterate over any of\"\n\"with various operations (e.g., aggregation and pivot). Since the\",\n,these stages. An utterance can also be interpreted contextually. The\n\"majority of raw data to analyze is already in a tabular format, we\",\n,following seven sections will discuss the seven stages accordingly.\nrename it transformed data. Data transformation is responsible for,\ngenerating alternative data subsets or derivations for visualization.,\n,\"4\nQUERY INTERPRETATION\"\nAll operations on the data plane belong to this stage. Details can,\nbe found in Section 5.,Query interpretation is the foundation of all subsequent stages. This\n\"• Visual Mapping\n(C): This\nstage\nfocuses\non mapping\nthe\",section will discuss how to perform semantic and syntax analysis\ninformation extracted from NL queries to visual structures. The,\"of the input natural\nlanguage queries,\ninfer analytic tasks of the\"\nthree elements of visual mapping for information visualization,\"user and data attributes\nto be analyzed, and make defaults\nfor\"\n\"are spatial substrate, graphical elements, and graphical properties\",underspeciﬁed utterances.\n\"[28]. Spatial substrate is the space to create visualizations, and\",\n,\"4.1\nSemantic and Syntax Analysis\"\n\"it\nis essential\nto consider\nthe layout conﬁguration. Graphical\",\nelements are the visual elements appearing in the spatial substrate,\"Semantic and syntax analysis can be powerful\nto discover hier-\"\n\"(e.g., points,\nlines, surfaces, and volumes). Graphical properties\",archical structures and understand meanings in human language.\ncan be implemented on the graphical elements to make them,The semantic parser can conduct a series of NLP sub-tasks on\n\"more noticeable\n(e.g.,\nsize, orientation,\ncolor,\ntextures,\nand\",\"the query string to extract valuable details\nthat can be used to\"\nshapes). Details can be found in Section 6.,\"detect relevant phrases. The processing steps include tokenization,\"\n• View Transformation (D): View transformation (rendering),\"identifying parts-of-speech (POS) tags, recognizing name entities,\"\ntransforms visual structures into views by specifying graphical,\"removing stop words, performing stemming, creating a dependency\"\n\"properties that\nturn these structures into pixels. Common forms\",\"tree, generating N-grams, etc. For example, Flowsense [273] is a\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 5,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n5\nTABLE 4\nComparison of commonly used NLP Toolkits\nToolkit\nCoreNLP [157]\nNLTK [21]\nOpenNLP [1]\nSpaCy [81]\nStanza [182]\nFlair [7]\nGoogleNLP [3]\nProgramming language\nJava\nPython\nJava\nPython\nPython\nPython\nPython\nTokenization\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nSentence segmentation\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nPart-of-speech tagging\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nParsing\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nLemmatization\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nNamed entities recognition\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nCoreference resolution\n✓\n✓\n✓\n-\n✓\n-\n-\nEntity linking\n✓\n✓\n-\n✓\n✓\n-\n✓\nChunker\n-\n-\n✓\n✓\n-\n-\n-\nSentiment\n✓\n-\n-\n-\n✓\n-\n✓\nText classiﬁcation\n-\n✓\n-\n✓\n-\n✓\n✓\nTrain custom model\n-\n-\n✓\n✓\n✓\n✓\n✓\nFig. 4. Semantics parsing in FlowSense [273]. The derivation of the query\nis shown as a parse tree.\nnatural language interface designed for a dataﬂow visualization\nsystem [272]. It applies a semantic parser with special utterances\n(table column names, node labels, node types, and dataset names)\ntagging and placeholders. Figure 4 displays a parse tree for the\nderivation of the user’s query. The ﬁve major components of a\nquery pattern and its related parts are highlighted by a unique\ncolor, and the expanded sub-diagram is illustrated at the bottom.\nFlowsense is powered by the Stanford SEMPRE framework [282]\nand CoreNLP toolkit [157]. Advanced NLP toolkits [1], [3], [7],\n[21], [81], [157], [182] allow developers to quickly integrate NLP\nservices into their systems. As semantic and syntax analysis is a\nfundamental task for V-NLI, almost all systems support semantic\nparsing by directly using existing NLP toolkits, such as CoreNLP\n[157], NLTK [21], OpenNLP [1], SpaCy [81], Stanza [182], Flair\n[7], and GoogleNLP [3], as listed in Table 3 (column NLP Toolkit\nor Technology). We also sort out commonly used characteristics of\nexisting NLP toolkits in Table 4 for reference. In addition, recently,\nthere have been some systems that do not rely on these tools but\nleverage Language Models (LMs) to directly “interpret” queries\n[143], [150]. They ﬁrst generate a rich representation of the input\nby translating them into high-dimensional vectors and then adopt\nneural networks to enable smart visualization inference.\nDiscussion: Most V-NLI systems rely on existing NLP toolkits\nfor semantic and syntax analysis. So far, these tools are a good\nchoice for query parsing and can be easily integrated into the\nsystem. However, they are trained on NLP datasets, thus lacking\nadequate consideration of visualization elements (e.g., mark, visual\nchannel, and encoding property). A promising solution may be to\ndevelop a new NLP toolkit speciﬁcally for visualization.\n4.2\nTask Inference\n4.2.1\nTask modeling\nA growing body of literature recognizes that the user’s analytic\ntask is vital for automatic visualization creation [24], [108], [113],\n[191], [194]. As for task modeling, there have been many prior\nefforts to provide deﬁnitions of the analytic tasks. For instance,\nAmar et al. [9] proposed ten low-level analytic tasks that capture\nthe user’s activities while employing visualization tools for data\nexploration. The ten tasks are later extensively applied in numerous\nvisualization systems [93], [113], [142], [168], [194], [212], [225],\n[245], [254], as listed in Table 5. Saket et al. [194] evaluated\nthe effectiveness of ﬁve commonly used chart types across the\nten tasks [9] by a crowdsourced experiment. Furthermore, they\nderived chart type recommendations for different tasks. Kim et\nal. [113] measured subject performance across task types derived\nfrom the ten tasks [9] and added a compare values task. The\nanalytic tasks are further grouped into two categories: value tasks\nthat just retrieve or compare individual values and summary tasks\nthat require identiﬁcation or comparison of aggregate properties.\nNL4DV [174] includes a Trend task in addition. AutoBrief [107]\nenhances visualization systems by introducing domain-level tasks,\nwhile Matthew et al. [24] contributed a multi-level typology of\nvisualization tasks. Deep into scatter charts, Sarikaya et al. [196]\ncollected model tasks from a variety of sources in data visualization\nliterature to formulate the seed for a scatterplot-speciﬁc analytic\ntask list. Recently, Shen et al. [211] summarized 18 classical\nanalytic tasks by a survey covering both academia and industry.\n4.2.2\nIntent inference\nAlthough considerable previous works focus on task modeling, few\nvisualization systems have attempted to infer the user’s analytic\ntask before the emergence of natural language interfaces. Gotz\nand Wen [70] monitored the user’s click interactions for implicit\nsignals of user intent. Steichen et al. [233] and Gingerich et al.\n[68] used the eye-gazing patterns of users while interacting with a\ngiven visualization to predict the user’s analytic task. Battle et al.\n[16] investigated the relationship between latency, task complexity,\nand user performance. However, these behavior-based systems\nare limited to few pre-deﬁned tasks; generalization for automatic\nvisualization systems does not exist yet.\nRather than inferring the analytic task through the user’s behav-\nior, systems supporting NL interaction depend on understanding\nthe NL utterances to analyze the user’s intent since they may\nhint at the user’s analysis goals. Most systems infer the analytic\ntasks by comparing the query tokens to a predeﬁned list of task\nkeywords [63], [83], [174], [205], [239], [273]. For example,\nNL4DV [174] identiﬁes ﬁve low-level analytic tasks: Correlation,\nDistribution, Derived Value, Trend, and Filter. A task keyword list\nis integrated internally (e.g., Correlation task includes ‘correlate,’\n‘relationship,’ etc., Distribution task includes ‘range,’ ‘spread,’ etc.).\nNL4DV also leverages POS tags and query parsing results to model\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 5\nTABLE4\nComparisonofcommonlyusedNLPToolkits\nToolkit CoreNLP[157] NLTK[21] OpenNLP[1] SpaCy[81] Stanza[182] Flair[7] GoogleNLP[3]\nProgramminglanguage Java Python Java Python Python Python Python\nTokenization (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\nSentencesegmentation (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\nPart-of-speechtagging (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\nParsing (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\nLemmatization (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\nNamedentitiesrecognition (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\nCoreferenceresolution (cid:88) (cid:88) (cid:88) - (cid:88) - -\nEntitylinking (cid:88) (cid:88) - (cid:88) (cid:88) - (cid:88)\nChunker - - (cid:88) (cid:88) - - -\nSentiment (cid:88) - - - (cid:88) - (cid:88)\nTextclassification - (cid:88) - (cid:88) - (cid:88) (cid:88)\nTraincustommodel - - (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)\n[191], [194]. As for task modeling, there have been many prior\nefforts to provide definitions of the analytic tasks. For instance,\nAmaretal.[9]proposedtenlow-levelanalytictasksthatcapture\nthe user’s activities while employing visualization tools for data\nexploration.Thetentasksarelaterextensivelyappliedinnumerous\nvisualizationsystems[93],[113],[142],[168],[194],[212],[225],\n[245], [254], as listed in Table 5. Saket et al. [194] evaluated\nthe effectiveness of five commonly used chart types across the\nten tasks [9] by a crowdsourced experiment. Furthermore, they\nderived chart type recommendations for different tasks. Kim et\nFig.4.SemanticsparsinginFlowSense[273].Thederivationofthequery al.[113]measuredsubjectperformanceacrosstasktypesderived\nisshownasaparsetree.\nfrom the ten tasks [9] and added a compare values task. The\nanalytictasksarefurthergroupedintotwocategories:valuetasks\nnatural language interface designed for a dataflow visualization\nthatjustretrieveorcompareindividualvaluesandsummarytasks\nsystem[272].Itappliesasemanticparserwithspecialutterances\nthatrequireidentificationorcomparisonofaggregateproperties.\n(tablecolumnnames,nodelabels,nodetypes,anddatasetnames)\nNL4DV[174]includesaTrendtaskinaddition.AutoBrief[107]\ntagging and placeholders. Figure 4 displays a parse tree for the\nenhancesvisualizationsystemsbyintroducingdomain-leveltasks,\nderivation of the user’s query. The five major components of a\nwhile Matthew et al. [24] contributed a multi-level typology of\nquery pattern and its related parts are highlighted by a unique\nvisualizationtasks.Deepintoscattercharts,Sarikayaetal.[196]\ncolor,andtheexpandedsub-diagramisillustratedatthebottom.\ncollectedmodeltasksfromavarietyofsourcesindatavisualization\nFlowsenseispoweredbytheStanfordSEMPREframework[282]\nliteraturetoformulatetheseedforascatterplot-specificanalytic\nandCoreNLPtoolkit[157].AdvancedNLPtoolkits[1],[3],[7],\ntask list. Recently, Shen et al. [211] summarized 18 classical\n[21],[81],[157],[182]allowdeveloperstoquicklyintegrateNLP\nanalytictasksbyasurveycoveringbothacademiaandindustry.\nservicesintotheirsystems.Assemanticandsyntaxanalysisisa\nfundamentaltaskforV-NLI,almostallsystemssupportsemantic 4.2.2 Intentinference\nparsingbydirectlyusingexistingNLPtoolkits,suchasCoreNLP Althoughconsiderablepreviousworksfocusontaskmodeling,few\n[157],NLTK[21],OpenNLP[1],SpaCy[81],Stanza[182],Flair visualization systems have attempted to infer the user’s analytic\n[7],andGoogleNLP[3],aslistedinTable3(columnNLPToolkit\ntask before the emergence of natural language interfaces. Gotz\norTechnology).Wealsosortoutcommonlyusedcharacteristicsof\nandWen[70]monitoredtheuser’sclickinteractionsforimplicit\nexistingNLPtoolkitsinTable4forreference.Inaddition,recently, signals of user intent. Steichen et al. [233] and Gingerich et al.\ntherehavebeensomesystemsthatdonotrelyonthesetoolsbut [68]usedtheeye-gazingpatternsofuserswhileinteractingwitha\nleverageLanguageModels(LMs)todirectly“interpret”queries givenvisualizationtopredicttheuser’sanalytictask.Battleetal.\n[143],[150].Theyfirstgeneratearichrepresentationoftheinput [16]investigatedtherelationshipbetweenlatency,taskcomplexity,\nbytranslatingthemintohigh-dimensionalvectorsandthenadopt and user performance. However, these behavior-based systems\nneuralnetworkstoenablesmartvisualizationinference. arelimitedtofewpre-definedtasks;generalizationforautomatic\nDiscussion:MostV-NLIsystemsrelyonexistingNLPtoolkits visualizationsystemsdoesnotexistyet.\nfor semantic and syntax analysis. So far, these tools are a good\nRatherthaninferringtheanalytictaskthroughtheuser’sbehav-\nchoice for query parsing and can be easily integrated into the\nior,systemssupportingNLinteractiondependonunderstanding\nsystem.However,theyaretrainedonNLPdatasets,thuslacking\nthe NL utterances to analyze the user’s intent since they may\nadequateconsiderationofvisualizationelements(e.g.,mark,visual\nhintattheuser’sanalysisgoals.Mostsystemsinfertheanalytic\nchannel,andencodingproperty).Apromisingsolutionmaybeto\ntasks by comparing the query tokens to a predefined list of task\ndevelopanewNLPtoolkitspecificallyforvisualization.\nkeywords [63], [83], [174], [205], [239], [273]. For example,\nNL4DV[174]identifiesfivelow-levelanalytictasks:Correlation,\n4.2 TaskInference\nDistribution,DerivedValue,Trend,andFilter.Ataskkeywordlist\n4.2.1 Taskmodeling isintegratedinternally(e.g.,Correlationtaskincludes‘correlate,’\nA growing body of literature recognizes that the user’s analytic ‘relationship,’etc.,Distributiontaskincludes‘range,’‘spread,’etc.).\ntaskisvitalforautomaticvisualizationcreation[24],[108],[113], NL4DValsoleveragesPOStagsandqueryparsingresultstomodel",
    "tables_extraction_text_csv": "0,1,2,3,4,5,6,7\n,,,TABLE 4,,,,\n,,,Comparison of commonly used NLP Toolkits,,,,\nToolkit,CoreNLP [157],NLTK [21],OpenNLP [1],SpaCy [81],Stanza [182],Flair [7],GoogleNLP [3]\nProgramming language,Java,Python,Java,Python,Python,Python,Python\nTokenization,(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88)\nSentence segmentation,(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88)\nPart-of-speech tagging,(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88)\nParsing,(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88)\nLemmatization,(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88)\nNamed entities recognition,(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88),(cid:88)\nCoreference resolution,(cid:88),(cid:88),(cid:88),-,(cid:88),-,-\nEntity linking,(cid:88),(cid:88),-,(cid:88),(cid:88),-,(cid:88)\nChunker,-,-,(cid:88),(cid:88),-,-,-\nSentiment,(cid:88),-,-,-,(cid:88),-,(cid:88)\nText classiﬁcation,-,(cid:88),-,(cid:88),-,(cid:88),(cid:88)\nTrain custom model,-,-,(cid:88),(cid:88),(cid:88),(cid:88),(cid:88)\n\n\n--- NEW TABLE ---\n\n0,1,2\n\"Chunker\n-\n-\",(cid:88),\"(cid:88)\n-\n-\n-\"\n\"(cid:88)\nSentiment\n-\",-,\"(cid:88)\n(cid:88)\n-\n-\"\n\"(cid:88)\nText classiﬁcation\n-\",-,\"(cid:88)\n(cid:88)\n(cid:88)\n-\"\n\"Train custom model\n-\n-\",(cid:88),\"(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\"\n,,\"[191],\n[194]. As for\ntask modeling,\nthere have been many prior\"\n,,\"efforts to provide deﬁnitions of\nthe analytic tasks. For\ninstance,\"\n,,Amar et al. [9] proposed ten low-level analytic tasks that capture\n,,the user’s activities while employing visualization tools for data\n,,exploration. The ten tasks are later extensively applied in numerous\n,,\"visualization systems [93], [113], [142], [168], [194], [212], [225],\"\n,,\"[245],\n[254], as\nlisted in Table 5. Saket et al.\n[194] evaluated\"\n,,\"the effectiveness of ﬁve commonly used chart\ntypes across the\"\n,,\"ten tasks [9] by a crowdsourced experiment. Furthermore,\nthey\"\n,,\"derived chart\ntype recommendations for different\ntasks. Kim et\"\nFig. 4. Semantics parsing in FlowSense [273]. The derivation of the query,,\n,,al. [113] measured subject performance across task types derived\nis shown as a parse tree.,,\n,,\"from the ten tasks\n[9] and added a compare values\ntask. The\"\n,,analytic tasks are further grouped into two categories: value tasks\n\"natural\nlanguage interface designed for a dataﬂow visualization\",,\n,,\"that\njust retrieve or compare individual values and summary tasks\"\nsystem [272]. It applies a semantic parser with special utterances,,\n,,that require identiﬁcation or comparison of aggregate properties.\n\"(table column names, node labels, node types, and dataset names)\",,\n,,NL4DV [174] includes a Trend task in addition. AutoBrief [107]\n\"tagging and placeholders. Figure 4 displays a parse tree for\nthe\",,\n,,\"enhances visualization systems by introducing domain-level\ntasks,\"\n\"derivation of\nthe user’s query. The ﬁve major components of a\",,\n,,\"while Matthew et al.\n[24] contributed a multi-level\ntypology of\"\n\"query pattern and its\nrelated parts are highlighted by a unique\",,\n,,\"visualization tasks. Deep into scatter charts, Sarikaya et al. [196]\"\n\"color, and the expanded sub-diagram is illustrated at\nthe bottom.\",,\n,,collected model tasks from a variety of sources in data visualization\nFlowsense is powered by the Stanford SEMPRE framework [282],,\n,,literature to formulate the seed for a scatterplot-speciﬁc analytic\n\"and CoreNLP toolkit [157]. Advanced NLP toolkits [1], [3], [7],\",,\n,,\"task list. Recently, Shen et al.\n[211]\nsummarized 18 classical\"\n\"[21], [81], [157], [182] allow developers to quickly integrate NLP\",,\n,,analytic tasks by a survey covering both academia and industry.\nservices into their systems. As semantic and syntax analysis is a,,\n\"fundamental\ntask for V-NLI, almost all systems support semantic\",,\"4.2.2\nIntent\ninference\"\n\"parsing by directly using existing NLP toolkits, such as CoreNLP\",,\n,,\"Although considerable previous works focus on task modeling, few\"\n\"[157], NLTK [21], OpenNLP [1], SpaCy [81], Stanza [182], Flair\",,\n,,visualization systems have attempted to infer the user’s analytic\n\"[7], and GoogleNLP [3], as listed in Table 3 (column NLP Toolkit\",,\n,,\"task before the emergence of natural\nlanguage interfaces. Gotz\"\nor Technology). We also sort out commonly used characteristics of,,\n,,and Wen [70] monitored the user’s click interactions for implicit\n\"existing NLP toolkits in Table 4 for reference. In addition, recently,\",,\n,,\"signals of user\nintent. Steichen et al.\n[233] and Gingerich et al.\"\nthere have been some systems that do not rely on these tools but,,\n,,[68] used the eye-gazing patterns of users while interacting with a\nleverage Language Models (LMs) to directly “interpret” queries,,\n,,given visualization to predict the user’s analytic task. Battle et al.\n\"[143], [150]. They ﬁrst generate a rich representation of the input\",,\n,,\"[16] investigated the relationship between latency,\ntask complexity,\"\nby translating them into high-dimensional vectors and then adopt,,\n,,\"and user performance. However,\nthese behavior-based systems\"\nneural networks to enable smart visualization inference.,,\n,,are limited to few pre-deﬁned tasks; generalization for automatic\nDiscussion: Most V-NLI systems rely on existing NLP toolkits,,\n,,visualization systems does not exist yet.\n\"for semantic and syntax analysis. So far,\nthese tools are a good\",,\n,,Rather than inferring the analytic task through the user’s behav-\nchoice for query parsing and can be easily integrated into the,,\n,,\"ior, systems supporting NL interaction depend on understanding\"\n\"system. However,\nthey are trained on NLP datasets,\nthus lacking\",,\n,,\"the NL utterances\nto analyze the user’s\nintent\nsince they may\"\n\"adequate consideration of visualization elements (e.g., mark, visual\",,\n,,\"hint at\nthe user’s analysis goals. Most systems infer the analytic\"\n\"channel, and encoding property). A promising solution may be to\",,\n,,tasks by comparing the query tokens to a predeﬁned list of task\ndevelop a new NLP toolkit speciﬁcally for visualization.,,\n,,\"keywords\n[63],\n[83],\n[174],\n[205],\n[239],\n[273]. For\nexample,\"\n,,\"NL4DV [174] identiﬁes ﬁve low-level analytic tasks: Correlation,\"\n\"4.2\nTask Inference\",,\n,,\"Distribution, Derived Value, Trend, and Filter. A task keyword list\"\n\"4.2.1\nTask modeling\",,\"is integrated internally (e.g., Correlation task includes ‘correlate,’\"\n\"A growing body of\nliterature recognizes that\nthe user’s analytic\",,\"‘relationship,’ etc., Distribution task includes ‘range,’ ‘spread,’ etc.).\"\n\"task is vital for automatic visualization creation [24], [108], [113],\",,NL4DV also leverages POS tags and query parsing results to model\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 6,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n6\nTABLE 5\nTen low-level analytic tasks proposed by Amar et al. [9]. They have been widely applied for automatic visualization creation.\nTask\nDescription\nRepresentative papers\nCharacterize Distribution Given a set of data cases and a quantitative attribute of interest, characterize\nthe distribution of that attribute’s values.\n[9], [61], [143], [174], [194], [196], [225], [254], [273]\nCluster\nGiven a dataset, ﬁnd clusters of similar attribute values.\n[9], [61], [194], [212], [225], [273]\nCompute Derived Value\nGiven a dataset, compute an aggregate numeric representation of the data.\n[9], [61], [86], [143], [194], [225], [254], [273]\nCorrelate\nGiven a dataset and two attributes, determine useful relationships between the\nvalues of those attributes.\n[9], [61], [86], [143], [174], [194], [196], [212], [225],\n[254], [273]\nDetermine Range\nGiven a dataset and a data attribute, ﬁnd the span of values within the set.\n[9], [61], [194], [225], [273]\nFilter\nFind data cases satisfying the given concrete conditions on attribute values.\n[9], [61], [174], [194], [225], [254], [273]\nFind Anomalies\nIdentify any anomalies within a given set of data cases with respect to a given\nrelationship or expectation, e.g. statistical outliers.\n[9], [61], [85], [194], [196], [212], [225], [254], [273]\nFind Extremum\nFind data cases possessing an extreme value of an attribute over its range.\n[9], [61], [85], [113], [143], [194], [225], [254], [273]\nRetrieve Value\nGiven a set of speciﬁc cases, ﬁnd attributes of those cases.\n[9], [61], [85], [113], [174], [194], [225], [254], [273]\nSort\nGiven a dataset, rank them according to some ordinal metric.\n[9], [61], [194], [225], [254], [273]\nthe relationship between query phrases and populate task details.\nFor conversational analysis systems, Nicky [116] interprets task\ninformation in conversations based on a domain-speciﬁc ontology.\nEvizeon [83] and Orko [231] support follow-up queries through\nconversational centering [71], which is a model commonly used\nin linguistic conversational structure but only for the ﬁlter task.\nADE [38] supports the analytic process via task recommendation\ninvoked by inferences about user interactions in mixed-initiative\nenvironments. The recommendation is also accompanied by a\nnatural language explanation. Instead of simply matching keywords,\nFu et al. [61] maintained a dataset of NL queries for visual data\nanalytics and trained a multi-label task classiﬁcation model based\non BERT [49], an advanced pre-trained NLP model.\nDiscussion: Although most V-NLI systems support task in-\nference, as shown in Table 3 (column Task Inference), the tasks\nintegrated in the system are limited (e.g., a subset of ten low-\nlevel tasks [9]). More tasks with hierarchical modeling can be\nconsidered to better cover the user’s intent, as well as tasks for\nanalyzing speciﬁc data types (e.g., tree [235], map [54], and graph\n[127]). Besides, rule-based approaches account for the majority;\nvarious advanced learning models provide a great opportunity to\ninfer analytic tasks in an unbiased and rigorous manner.\n4.3\nData Attributes Inference\nA dataset contains numerous data attributes. However, the user\nmay only be interested in several certain data attributes in a query.\nThe systems should be able to extract data attributes that are\nmentioned both explicitly (e.g., directly refer to attribute names)\nand implicitly (e.g., refer to an attribute’s values or alias). Illustrated\nby an example, NL4DV [174] maintains an alias map and allows\ndevelopers to conﬁgure aliases (e.g., GDP for Gross domestic\nproduct and investment for production budget). It iterates through\nthe generated N-grams discussed in Section 4.1, checking for both\nsyntactic and semantic similarity between N-grams and a lexicon\ncomposed of data attributes, aliases, and values. For syntactic\nsimilarity, NL4DV adopts cosine similarity function; for semantic\nsimilarity, it computes the Wu-Palmer similarity score based on\nWordNet [164]. If the similarity score reaches the threshold, the\ncorresponding data attributes will be extracted and further presented\non the visualization. Most V-NLI systems have taken a similar\napproach, and they just differ in the integrated rules. Besides,\nAnalyza [50] utilizes additional heuristics to derive information\nfrom data attributes and expands the lexicon with a proprietary\nknowledge graph. Recently, Liu et al. [143] proposed a deep\nlearning-based pipeline, ADVISor. It uses BERT [49] to generate\nembeddings of both NL queries and table headers, which are\nfurther used by a deep neural networks model to decide data\nattributes, ﬁlter conditions, and aggregation type. Similarly, Luo\net al. [150] presented an end-to-end solution using a transformer-\nbased [248] sequence-to-sequence (seq2seq) model [12]. In the\nﬂow of visual analytical conversations, data attributes can also be\nextracted through co-reference resolution (Section 9.2).\nDiscussion: During the survey and evaluation of state-of-the-\nart systems, we found that most V-NLIs are primarily geared to\nsupport speciﬁcation-oriented queries like examples in their manual\n(e.g., “Show me acceleration and horsepower across origins”),\nfrom which it is easy to extract data attributes. However, when it\ncomes to underspeciﬁed or vague queries (e.g., include synonym,\nabbreviation, and terminology in different ﬁelds), these systems\noften fail to generate appropriate visualizations. So improving the\nrobustness of V-NLIs should be one of the focuses in future work.\n4.4\nDefault for Underspeciﬁed Utterances\nNumerous systems support the ﬁlter task [63], [83], [174], [205],\n[273], aiming to select data attributes and ranges. An input query\nwill be underspeciﬁed if it lacks enough information for the\nsystem to infer data attributes and perform ﬁltering. Presenting\ndefaults in the interface by inferring the underspeciﬁed utterances\ncan be an effective option to address this issue. However, the\narea has received relatively little attention, as shown in Table\n3 (column Default for Underspeciﬁed Utterances). Within the\nresearch, vague modiﬁers like “tall” and “cheap” are a prevalent\nkind of underspeciﬁcation in human language. Hearst et al. [78]\nmade the ﬁrst step toward design guidelines for dealing with vague\nmodiﬁers based on existing cognitive linguistics research and a\ncrowdsourcing study. Tableau’s Ask Data [2] internally leverages\nlightweight and intermediate language, Arklang [208], to infer\nunderspeciﬁed details. It emphasizes how the linguistic context of\nprevious utterances affects the interpretation of a new utterance.\nSentiﬁers [207] can determine which data attributes and ﬁlter\nranges to associate the vague predicates using word co-occurrence\nand sentiment polarities. As shown in Figure 5, when analyzing\nthe earthquakes dataset, the user inputs “where is it unsafe” and\nSentiﬁers automatically associates “unsafe” with the magnitude\nattribute. A top N ﬁlter of magnitude six and higher is applied, and\nsimilar negative sentiment polarities are marked red on the map.\nDiscussion: Although the aforementioned approaches are\nuseful, some more complex interpretations are still not supported in\ncurrent V-NLIs, such as combinations of vague modiﬁers. So when\nencountering ambiguity, apart from formulating a sensible default,\nhuman interaction (e.g., ambiguity widgets) is another effective\nmethod (see Section 8.1). In addition, users’ explicit judgments\nfor the same utterance can differ signiﬁcantly. A more adaptative\nsolution may be to personalize the inferencing logic by identifying\nindividuals’ unique analytic tasks.\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 6\nTABLE5\nTenlow-levelanalytictasksproposedbyAmaretal.[9].Theyhavebeenwidelyappliedforautomaticvisualizationcreation.\nTask Description Representativepapers\nGivenasetofdatacasesandaquantitativeattributeofinterest,characterize\nCharacterizeDistribution [9],[61],[143],[174],[194],[196],[225],[254],[273]\nthedistributionofthatattribute’svalues.\nCluster Givenadataset,findclustersofsimilarattributevalues. [9],[61],[194],[212],[225],[273]\nComputeDerivedValue Givenadataset,computeanaggregatenumericrepresentationofthedata. [9],[61],[86],[143],[194],[225],[254],[273]\nGivenadatasetandtwoattributes,determineusefulrelationshipsbetweenthe [9],[61],[86],[143],[174],[194],[196],[212],[225],\nCorrelate\nvaluesofthoseattributes. [254],[273]\nDetermineRange Givenadatasetandadataattribute,findthespanofvalueswithintheset. [9],[61],[194],[225],[273]\nFilter Finddatacasessatisfyingthegivenconcreteconditionsonattributevalues. [9],[61],[174],[194],[225],[254],[273]\nIdentifyanyanomalieswithinagivensetofdatacaseswithrespecttoagiven\nFindAnomalies [9],[61],[85],[194],[196],[212],[225],[254],[273]\nrelationshiporexpectation,e.g.statisticaloutliers.\nFindExtremum Finddatacasespossessinganextremevalueofanattributeoveritsrange. [9],[61],[85],[113],[143],[194],[225],[254],[273]\nRetrieveValue Givenasetofspecificcases,findattributesofthosecases. [9],[61],[85],[113],[174],[194],[225],[254],[273]\nSort Givenadataset,rankthemaccordingtosomeordinalmetric. [9],[61],[194],[225],[254],[273]\ntherelationshipbetweenqueryphrasesandpopulatetaskdetails. attributes,filterconditions,andaggregationtype.Similarly,Luo\nFor conversational analysis systems, Nicky [116] interprets task etal.[150]presentedanend-to-endsolutionusingatransformer-\ninformationinconversationsbasedonadomain-specificontology. based [248] sequence-to-sequence (seq2seq) model [12]. In the\nEvizeon [83] and Orko [231] support follow-up queries through flowofvisualanalyticalconversations,dataattributescanalsobe\nconversationalcentering[71],whichisamodelcommonlyused extractedthroughco-referenceresolution(Section9.2).\nin linguistic conversational structure but only for the filter task. Discussion:Duringthesurveyandevaluationofstate-of-the-\nADE[38]supportstheanalyticprocessviataskrecommendation art systems, we found that most V-NLIs are primarily geared to\ninvokedbyinferencesaboutuserinteractionsinmixed-initiative supportspecification-orientedquerieslikeexamplesintheirmanual\nenvironments. The recommendation is also accompanied by a (e.g., “Show me acceleration and horsepower across origins”),\nnaturallanguageexplanation.Insteadofsimplymatchingkeywords, fromwhichitiseasytoextractdataattributes.However,whenit\nFuetal.[61]maintainedadatasetofNLqueriesforvisualdata comestounderspecifiedorvaguequeries(e.g.,includesynonym,\nanalyticsandtrainedamulti-labeltaskclassificationmodelbased abbreviation, and terminology in different fields), these systems\nonBERT[49],anadvancedpre-trainedNLPmodel. oftenfailtogenerateappropriatevisualizations.Soimprovingthe\nDiscussion: Although most V-NLI systems support task in- robustnessofV-NLIsshouldbeoneofthefocusesinfuturework.\nference, as shown in Table 3 (column Task Inference), the tasks\n4.4 DefaultforUnderspecifiedUtterances\nintegrated in the system are limited (e.g., a subset of ten low-\nlevel tasks [9]). More tasks with hierarchical modeling can be Numeroussystemssupportthefiltertask[63],[83],[174],[205],\nconsidered to better cover the user’s intent, as well as tasks for [273],aimingtoselectdataattributesandranges.Aninputquery\nanalyzingspecificdatatypes(e.g.,tree[235],map[54],andgraph will be underspecified if it lacks enough information for the\n[127]).Besides,rule-basedapproachesaccountforthemajority; system to infer data attributes and perform filtering. Presenting\nvariousadvancedlearningmodelsprovideagreatopportunityto defaultsintheinterfacebyinferringtheunderspecifiedutterances\ninferanalytictasksinanunbiasedandrigorousmanner. can be an effective option to address this issue. However, the\narea has received relatively little attention, as shown in Table\n4.3 DataAttributesInference 3 (column Default for Underspecified Utterances). Within the\nresearch,vaguemodifierslike“tall”and“cheap”areaprevalent\nA dataset contains numerous data attributes. However, the user\nkindofunderspecificationinhumanlanguage.Hearstetal.[78]\nmayonlybeinterestedinseveralcertaindataattributesinaquery.\nmadethefirststeptowarddesignguidelinesfordealingwithvague\nThe systems should be able to extract data attributes that are\nmodifiers based on existing cognitive linguistics research and a\nmentionedbothexplicitly(e.g.,directlyrefertoattributenames)\ncrowdsourcingstudy.Tableau’sAskData[2]internallyleverages\nandimplicitly(e.g.,refertoanattribute’svaluesoralias).Illustrated\nlightweight and intermediate language, Arklang [208], to infer\nbyanexample,NL4DV[174]maintainsanaliasmapandallows\nunderspecifieddetails.Itemphasizeshowthelinguisticcontextof\ndevelopers to configure aliases (e.g., GDP for Gross domestic\nprevious utterances affects the interpretation of a new utterance.\nproductandinvestmentforproductionbudget).Ititeratesthrough\nSentifiers [207] can determine which data attributes and filter\nthegeneratedN-gramsdiscussedinSection4.1,checkingforboth\nrangestoassociatethevaguepredicatesusingwordco-occurrence\nsyntacticandsemanticsimilaritybetweenN-gramsandalexicon\nand sentiment polarities. As shown in Figure 5, when analyzing\ncomposed of data attributes, aliases, and values. For syntactic\ntheearthquakesdataset,theuserinputs“whereisitunsafe”and\nsimilarity,NL4DVadoptscosinesimilarityfunction;forsemantic\nSentifiers automatically associates “unsafe” with the magnitude\nsimilarity, it computes the Wu-Palmer similarity score based on\nattribute.AtopNfilterofmagnitudesixandhigherisapplied,and\nWordNet[164].Ifthesimilarityscorereachesthethreshold,the\nsimilarnegativesentimentpolaritiesaremarkedredonthemap.\ncorrespondingdataattributeswillbeextractedandfurtherpresented\nDiscussion: Although the aforementioned approaches are\non the visualization. Most V-NLI systems have taken a similar\nuseful,somemorecomplexinterpretationsarestillnotsupportedin\napproach, and they just differ in the integrated rules. Besides,\ncurrentV-NLIs,suchascombinationsofvaguemodifiers.Sowhen\nAnalyza [50] utilizes additional heuristics to derive information\nencounteringambiguity,apartfromformulatingasensibledefault,\nfrom data attributes and expands the lexicon with a proprietary\nhuman interaction (e.g., ambiguity widgets) is another effective\nknowledge graph. Recently, Liu et al. [143] proposed a deep\nmethod (see Section 8.1). In addition, users’ explicit judgments\nlearning-basedpipeline,ADVISor.ItusesBERT[49]togenerate\nforthesameutterancecandiffersignificantly.Amoreadaptative\nembeddings of both NL queries and table headers, which are\nsolutionmaybetopersonalizetheinferencinglogicbyidentifying\nfurther used by a deep neural networks model to decide data\nindividuals’uniqueanalytictasks.",
    "tables_extraction_text_csv": "0,1,2,3\n\"IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\",,,6\n,TABLE 5,,\n,Ten low-level analytic tasks proposed by Amar et al.,[9]. They have been widely applied for automatic visualization creation.,\nTask,Description,Representative papers,\n,\"Given a set of data cases and a quantitative attribute of interest, characterize\",,\nCharacterize Distribution,,\"[9], [61], [143], [174], [194], [196], [225], [254], [273]\",\n,the distribution of that attribute’s values.,,\nCluster,\"Given a dataset, ﬁnd clusters of similar attribute values.\",\"[9], [61], [194], [212], [225], [273]\",\nCompute Derived Value,\"Given a dataset, compute an aggregate numeric representation of the data.\",\"[9], [61], [86], [143], [194], [225], [254], [273]\",\n,\"Given a dataset and two attributes, determine useful relationships between the\",\"[9], [61], [86], [143], [174], [194], [196], [212], [225],\",\nCorrelate,,,\n,values of those attributes.,\"[254], [273]\",\nDetermine Range,\"Given a dataset and a data attribute, ﬁnd the span of values within the set.\",\"[9], [61], [194], [225], [273]\",\nFilter,Find data cases satisfying the given concrete conditions on attribute values.,\"[9], [61], [174], [194], [225], [254], [273]\",\n,Identify any anomalies within a given set of data cases with respect to a given,,\nFind Anomalies,,\"[9], [61], [85], [194], [196], [212], [225], [254], [273]\",\n,\"relationship or expectation, e.g. statistical outliers.\",,\nFind Extremum,Find data cases possessing an extreme value of an attribute over its range.,\"[9], [61], [85], [113], [143], [194], [225], [254], [273]\",\nRetrieve Value,\"Given a set of speciﬁc cases, ﬁnd attributes of those cases.\",\"[9], [61], [85], [113], [174], [194], [225], [254], [273]\",\nSort,\"Given a dataset, rank them according to some ordinal metric.\",\"[9], [61], [194], [225], [254], [273]\",\nthe relationship between query phrases and populate task details.,,\"attributes, ﬁlter conditions, and aggregation type. Similarly, Luo\",\n\n\n--- NEW TABLE ---\n\n0,1\nIdentify any anomalies within a given set of data cases with respect to a given,\nFind Anomalies,\"[9], [61], [85], [194], [196], [212], [225], [254], [273]\"\n\"relationship or expectation, e.g. statistical outliers.\",\n\"Find Extremum\nFind data cases possessing an extreme value of an attribute over its range.\",\"[9], [61], [85], [113], [143], [194], [225], [254], [273]\"\n\"Retrieve Value\nGiven a set of speciﬁc cases, ﬁnd attributes of those cases.\",\"[9], [61], [85], [113], [174], [194], [225], [254], [273]\"\n\"Sort\nGiven a dataset, rank them according to some ordinal metric.\",\"[9], [61], [194], [225], [254], [273]\"\nthe relationship between query phrases and populate task details.,\"attributes, ﬁlter conditions, and aggregation type. Similarly, Luo\"\n\"For conversational analysis systems, Nicky [116] interprets task\",et al. [150] presented an end-to-end solution using a transformer-\ninformation in conversations based on a domain-speciﬁc ontology.,\"based [248] sequence-to-sequence (seq2seq) model\n[12].\nIn the\"\nEvizeon [83] and Orko [231] support follow-up queries through,\"ﬂow of visual analytical conversations, data attributes can also be\"\n\"conversational centering [71], which is a model commonly used\",extracted through co-reference resolution (Section 9.2).\n\"in linguistic conversational structure but only for\nthe ﬁlter task.\",Discussion: During the survey and evaluation of state-of-the-\nADE [38] supports the analytic process via task recommendation,\"art systems, we found that most V-NLIs are primarily geared to\"\ninvoked by inferences about user interactions in mixed-initiative,support speciﬁcation-oriented queries like examples in their manual\nenvironments. The recommendation is also accompanied by a,\"(e.g., “Show me acceleration and horsepower across origins”),\"\n\"natural language explanation. Instead of simply matching keywords,\",\"from which it is easy to extract data attributes. However, when it\"\nFu et al. [61] maintained a dataset of NL queries for visual data,\"comes to underspeciﬁed or vague queries (e.g., include synonym,\"\n\"analytics and trained a multi-label\ntask classiﬁcation model based\",\"abbreviation, and terminology in different ﬁelds),\nthese systems\"\n\"on BERT [49], an advanced pre-trained NLP model.\",\"often fail\nto generate appropriate visualizations. So improving the\"\n\"Discussion: Although most V-NLI\nsystems\nsupport\ntask in-\",robustness of V-NLIs should be one of the focuses in future work.\n\"ference, as shown in Table 3 (column Task Inference),\nthe tasks\",\n,\"4.4\nDefault for Underspeciﬁed Utterances\"\n\"integrated in the system are limited (e.g., a subset of\nten low-\",\n,\"Numerous systems support the ﬁlter task [63], [83], [174], [205],\"\n\"level\ntasks\n[9]). More tasks with hierarchical modeling can be\",\n,\"[273], aiming to select data attributes and ranges. An input query\"\n\"considered to better cover\nthe user’s intent, as well as tasks for\",\n,\"will be underspeciﬁed if\nit\nlacks\nenough information for\nthe\"\n\"analyzing speciﬁc data types (e.g.,\ntree [235], map [54], and graph\",\n,system to infer data attributes and perform ﬁltering. Presenting\n\"[127]). Besides, rule-based approaches account for the majority;\",\n,defaults in the interface by inferring the underspeciﬁed utterances\nvarious advanced learning models provide a great opportunity to,\n,\"can be an effective option to address\nthis\nissue. However,\nthe\"\ninfer analytic tasks in an unbiased and rigorous manner.,\n,\"area has\nreceived relatively little attention, as\nshown in Table\"\n\"4.3\nData Attributes Inference\",\"3 (column Default\nfor Underspeciﬁed Utterances). Within the\"\n,\"research, vague modiﬁers like “tall” and “cheap” are a prevalent\"\n\"A dataset contains numerous data attributes. However,\nthe user\",\n,kind of underspeciﬁcation in human language. Hearst et al. [78]\nmay only be interested in several certain data attributes in a query.,\n,made the ﬁrst step toward design guidelines for dealing with vague\n\"The\nsystems\nshould be\nable\nto extract data\nattributes\nthat\nare\",\n,modiﬁers based on existing cognitive linguistics research and a\n\"mentioned both explicitly (e.g., directly refer to attribute names)\",\n,crowdsourcing study. Tableau’s Ask Data [2] internally leverages\n\"and implicitly (e.g., refer to an attribute’s values or alias). Illustrated\",\n,\"lightweight and intermediate language, Arklang [208],\nto infer\"\n\"by an example, NL4DV [174] maintains an alias map and allows\",\n,underspeciﬁed details. It emphasizes how the linguistic context of\n\"developers\nto conﬁgure aliases\n(e.g., GDP for Gross domestic\",\n,previous utterances affects the interpretation of a new utterance.\nproduct and investment for production budget). It iterates through,\n,\"Sentiﬁers\n[207]\ncan determine which data attributes\nand ﬁlter\"\n\"the generated N-grams discussed in Section 4.1, checking for both\",\n,ranges to associate the vague predicates using word co-occurrence\nsyntactic and semantic similarity between N-grams and a lexicon,\n,\"and sentiment polarities. As shown in Figure 5, when analyzing\"\n\"composed of data attributes, aliases, and values. For\nsyntactic\",\n,\"the earthquakes dataset,\nthe user inputs “where is it unsafe” and\"\n\"similarity, NL4DV adopts cosine similarity function; for semantic\",\n,Sentiﬁers automatically associates “unsafe” with the magnitude\n\"similarity,\nit computes the Wu-Palmer similarity score based on\",\n,\"attribute. A top N ﬁlter of magnitude six and higher is applied, and\"\n\"WordNet [164]. If the similarity score reaches the threshold,\nthe\",\n,similar negative sentiment polarities are marked red on the map.\ncorresponding data attributes will be extracted and further presented,\n,\"Discussion: Although\nthe\naforementioned\napproaches\nare\"\non the visualization. Most V-NLI systems have taken a similar,\n,\"useful, some more complex interpretations are still not supported in\"\n\"approach, and they just differ\nin the integrated rules. Besides,\",\n,\"current V-NLIs, such as combinations of vague modiﬁers. So when\"\nAnalyza [50] utilizes additional heuristics to derive information,\n,\"encountering ambiguity, apart from formulating a sensible default,\"\nfrom data attributes and expands the lexicon with a proprietary,\n,\"human interaction (e.g., ambiguity widgets)\nis another effective\"\n\"knowledge graph. Recently, Liu et al.\n[143] proposed a deep\",\n,\"method (see Section 8.1).\nIn addition, users’ explicit\njudgments\"\n\"learning-based pipeline, ADVISor. It uses BERT [49] to generate\",\n,for the same utterance can differ signiﬁcantly. A more adaptative\n\"embeddings of both NL queries and table headers, which are\",\n,solution may be to personalize the inferencing logic by identifying\n\"further used by a deep neural networks model\nto decide data\",\n,individuals’ unique analytic tasks.\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 7,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n7\nFig. 5. Vague modiﬁer interpretation in Sentiﬁers [63]. The system\nassociates the vague modiﬁer “unsafe” with magnitude attribute.\n5\nDATA TRANSFORMATION\nAfter extracting data attributes, various data transformation oper-\nations can be made to transform raw data into focused data. This\nsection will introduce how V-NLI systems transform raw data for\ndata insights, with additional discussions on a closely related topic,\nNLI for databases.\n5.1\nTransformation for Data Insights\nA consensus is that the purpose of visualization is insights, not\npictures. A “boring” dataset may become “interesting” after data\ntransformations (e.g., aggregation, binning, and grouping). There-\nfore, identifying the data transformation information is another\nimportant characteristic in V-NLI. To describe related works, we\ncategorize tabular data into four types: temporal, quantitative,\nnominal, and ordinal, which are widely adopted in the visualization\ncommunity [168]. For temporal data, systems can support binning\nbased on temporal units [62], [148], [165], [169], [174], [205],\n[273]. For instance, Setlur et al. [205] developed a temporal\nanalytics module based on temporal entities and expressions\ndeﬁned in TimeML [91]. To parse temporal expressions, this\nmodule incorporates the following temporal token types: Temporal\nUnits (e.g., years, months, days, hours and minutes, and their\nabbreviations.), Temporal Prepositions (e.g., ’in,’ ’during,’ ’near,’\nand ’around.’), and Temporal Connectives (e.g., ‘before’ and\n‘after’). For quantitative data, various aggregation functions (e.g.,\ncount, average, and sum) can be performed, binning and grouping\noperations are also commonly used [10], [38], [41], [50], [63],\n[83], [93], [121], [148], [159], [178], [205], [208], [225], [273],\n[284]. For example, DeepEye [148] includes various aggregation\nfunctions in the visualization language. Arklang [208], which is an\nintermediate language to resolve NL utterances to formal queries,\nintegrates a ﬁnite set of aggregation operators. Eviza [205] enables\naggregation using regular spatial bins. Unlike continuous data,\nnominal and ordinal data are usually used as grouping metrics. As\nan end-to-end system, ncNet [150] supports more complex data\ntransformation types (e.g., relational join, GroupBY, and OrderBY)\nby applying Neural Machine Translation (NMT) models. After\nextracting relevant information, the commonly used visualization\nspeciﬁcation languages (e.g., Vega [198], Vega-Lite [197], and\nVizQL [234]) all provide various data transformation primitives\nto realize. Besides, to our knowledge, most V-NLIs are designed\nto deal with tabular data, but there are also several systems that\nfocus on speciﬁc data types (e.g., network [231] and map [204]).\nHowever, they mostly only display the raw data without data\ntransformations.\nDiscussion: The back-end engine can directly perform calcula-\ntions and visualize the results if the data transformation information\ncan be extracted from the user’s NL queries. Otherwise, despite\nadvances in technologies for data management and analysis [17],\n[67], it remains time-consuming to inspect a dataset and construct a\nvisualization that allows meaningful analysis to begin. So a practical\nsystem should be in a position to automatically recommend data\ninsights for users. Fortunately, various systems have been designed\nto generate data facts in the context of visual data exploration, such\nas DataSite [42], Voder [225], Wrangler [99], Quickinsights [53],\nForesight [46], and SpotLight [75]. They can serve as back-end\nengines for insight-driven recommendations.\n5.2\nNLI for Database\nNatural Language Interface for Database (NLI4DB), or Text-to-\nSQL, is a task to automatically translate a user’s query text in\nnatural language into an executable query for databases like SQL\n[6]. NLI4DB is strongly related to data transformation in V-NLI,\nand V-NLI can augment NLI4DB with effective visualizations of\nquery results. Besides, not all the queries about a dataset need a\nvisualization as a response. For instance, one might ask, “What\nwas the GDP of the USA last year?” or “Which country won\nthe most gold medals in the Tokyo Olympics?”. In such cases,\nNLI4DB can directly compute and present values in response to\nthe user’s questions instead of displaying through visualizations\nthat require the user to interpret for answers. Generally, there are\nthree types of methods for NLI4DB: symbolic approach, statistical\napproach, and connectionist (neural network) approach [175]. The\nsymbolic approach utilizes explicit linguistic rules and knowledge\nto process the user’s query, which dominated the area in the early\nyears. DISCERN [221] is a representative example and integrates a\nmodel that extracts information from scripts, lexicon, and memory.\nThe statistical approach exploits statistical learning methods such\nas hidden Markov models [141] and probabilistic context-free\ngrammars [87]. From the view of the connectionist approach,\nthe Text-to-SQL task is a kind of special Machine Translation\n(MT) problem. However, it is hard to be directly handled with\nstandard end-to-end neural machine translation models due to the\nabsence of detailed speciﬁcation in the user’s query [73]. Another\nproblem is the out-of-domain items mentioned in the user’s query\ndue to the user’s unawareness of the ontology set. Aiming at the\ntwo problems, IRNet [73] proposes a tree-shaped intermediate\nrepresentation synthesized from the user’s query. It is later fed into\nan inference model to generate SQL statements with a domain\nknowledge base. Furthermore, to capture the special format of SQL\nstatements, SQLnet [268] adopts a universal sketch as a template\nand predicts values for each slot. Wang et al. [252] employed a\ntwo-stage pipeline to subsequently predict the semantic structure\nand generate SQL statements with structure-enhanced query text.\nRecently, TaPas [80] extends BERT’s architecture and trains from\nweak supervision to answer questions over tables. For more details,\nplease refer to the survey [6], [290] and related papers in Table 2.\nDiscussion: V-NLI can augment NLI4DB with well-designed\nvisualizations, and NLI4DB can complement V-NLI with exact\nvalue answers. First, the two ﬁelds can draw on each other as\nthey share similar principles. Second, we can combine the two\ntechnologies to generate visualizations that “contain” answers.\nThird, there are many benchmarks in the NLI4DB community like\nWikiSQL [287] and Spider [274], which can be utilized to further\ndesign V-NLI benchmarks.\n6\nVISUAL MAPPING\nThis section will discuss how V-NLIs perform visual mapping in:\nspatial substrate, graphical elements, and graphical properties.\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 7\ncan be extracted from the user’s NL queries. Otherwise, despite\nadvancesintechnologiesfordatamanagementandanalysis[17],\n[67],itremainstime-consumingtoinspectadatasetandconstructa\nvisualizationthatallowsmeaningfulanalysistobegin.Soapractical\nsystemshouldbeinapositiontoautomaticallyrecommenddata\ninsightsforusers.Fortunately,varioussystemshavebeendesigned\ntogeneratedatafactsinthecontextofvisualdataexploration,such\nasDataSite[42],Voder[225],Wrangler[99],Quickinsights[53],\nForesight [46], and SpotLight [75]. They can serve as back-end\nenginesforinsight-drivenrecommendations.\n5.2 NLIforDatabase\nNatural Language Interface for Database (NLI4DB), or Text-to-\nFig. 5. Vague modifier interpretation in Sentifiers [63]. The system\nSQL, is a task to automatically translate a user’s query text in\nassociatesthevaguemodifier“unsafe”withmagnitudeattribute.\nnaturallanguageintoanexecutablequeryfordatabaseslikeSQL\n5 DATA TRANSFORMATION\n[6].NLI4DBisstronglyrelatedtodatatransformationinV-NLI,\nAfterextractingdataattributes,variousdatatransformationoper- andV-NLIcanaugmentNLI4DBwitheffectivevisualizationsof\nationscanbemadetotransformrawdataintofocuseddata.This queryresults.Besides,notallthequeriesaboutadatasetneeda\nsectionwillintroducehowV-NLIsystemstransformrawdatafor visualization as a response. For instance, one might ask, “What\ndatainsights,withadditionaldiscussionsonacloselyrelatedtopic, was the GDP of the USA last year?” or “Which country won\nNLIfordatabases. the most gold medals in the Tokyo Olympics?”. In such cases,\n5.1 TransformationforDataInsights NLI4DBcandirectlycomputeandpresentvaluesinresponseto\nthe user’s questions instead of displaying through visualizations\nA consensus is that the purpose of visualization is insights, not\nthatrequiretheusertointerpretforanswers.Generally,thereare\npictures.A“boring”datasetmaybecome“interesting”afterdata\nthreetypesofmethodsforNLI4DB:symbolicapproach,statistical\ntransformations(e.g.,aggregation,binning,andgrouping).There-\napproach,andconnectionist(neuralnetwork)approach[175].The\nfore, identifying the data transformation information is another\nsymbolicapproachutilizesexplicitlinguisticrulesandknowledge\nimportantcharacteristicinV-NLI.Todescriberelatedworks,we\ntoprocesstheuser’squery,whichdominatedtheareaintheearly\ncategorize tabular data into four types: temporal, quantitative,\nyears.DISCERN[221]isarepresentativeexampleandintegratesa\nnominal,andordinal,whicharewidelyadoptedinthevisualization\nmodelthatextractsinformationfromscripts,lexicon,andmemory.\ncommunity[168].Fortemporaldata,systemscansupportbinning\nThestatisticalapproachexploitsstatisticallearningmethodssuch\nbased on temporal units [62], [148], [165], [169], [174], [205],\nas hidden Markov models [141] and probabilistic context-free\n[273]. For instance, Setlur et al. [205] developed a temporal\ngrammars [87]. From the view of the connectionist approach,\nanalytics module based on temporal entities and expressions\nthe Text-to-SQL task is a kind of special Machine Translation\ndefined in TimeML [91]. To parse temporal expressions, this\n(MT) problem. However, it is hard to be directly handled with\nmoduleincorporatesthefollowingtemporaltokentypes:Temporal\nstandardend-to-endneuralmachinetranslationmodelsduetothe\nUnits (e.g., years, months, days, hours and minutes, and their\nabsenceofdetailedspecificationintheuser’squery[73].Another\nabbreviations.),TemporalPrepositions(e.g.,’in,’’during,’’near,’\nproblemistheout-of-domainitemsmentionedintheuser’squery\nand ’around.’), and Temporal Connectives (e.g., ‘before’ and\nduetotheuser’sunawarenessoftheontologyset.Aimingatthe\n‘after’).Forquantitativedata,variousaggregationfunctions(e.g.,\ntwo problems, IRNet [73] proposes a tree-shaped intermediate\ncount,average,andsum)canbeperformed,binningandgrouping\nrepresentationsynthesizedfromtheuser’squery.Itislaterfedinto\noperations are also commonly used [10], [38], [41], [50], [63],\nan inference model to generate SQL statements with a domain\n[83], [93], [121], [148], [159], [178], [205], [208], [225], [273],\nknowledgebase.Furthermore,tocapturethespecialformatofSQL\n[284].Forexample,DeepEye[148]includesvariousaggregation\nstatements,SQLnet[268]adoptsauniversalsketchasatemplate\nfunctionsinthevisualizationlanguage.Arklang[208],whichisan\nand predicts values for each slot. Wang et al. [252] employed a\nintermediatelanguagetoresolveNLutterancestoformalqueries,\ntwo-stagepipelinetosubsequentlypredictthesemanticstructure\nintegratesafinitesetofaggregationoperators.Eviza[205]enables\nandgenerateSQLstatementswithstructure-enhancedquerytext.\naggregation using regular spatial bins. Unlike continuous data,\nRecently,TaPas[80]extendsBERT’sarchitectureandtrainsfrom\nnominalandordinaldataareusuallyusedasgroupingmetrics.As\nweaksupervisiontoanswerquestionsovertables.Formoredetails,\nan end-to-end system, ncNet [150] supports more complex data\npleaserefertothesurvey[6],[290]andrelatedpapersinTable2.\ntransformationtypes(e.g.,relationaljoin,GroupBY,andOrderBY)\nDiscussion:V-NLIcanaugmentNLI4DBwithwell-designed\nby applying Neural Machine Translation (NMT) models. After\nvisualizations, and NLI4DB can complement V-NLI with exact\nextractingrelevantinformation,thecommonlyusedvisualization\nvalue answers. First, the two fields can draw on each other as\nspecification languages (e.g., Vega [198], Vega-Lite [197], and\nthey share similar principles. Second, we can combine the two\nVizQL [234]) all provide various data transformation primitives\ntechnologies to generate visualizations that “contain” answers.\ntorealize.Besides,toourknowledge,mostV-NLIsaredesigned\nThird,therearemanybenchmarksintheNLI4DBcommunitylike\nto deal with tabular data, but there are also several systems that\nWikiSQL[287]andSpider[274],whichcanbeutilizedtofurther\nfocusonspecificdatatypes(e.g.,network[231]andmap[204]).\ndesignV-NLIbenchmarks.\nHowever, they mostly only display the raw data without data\ntransformations. 6 VISUAL MAPPING\nDiscussion:Theback-endenginecandirectlyperformcalcula-\nThissectionwilldiscusshowV-NLIsperformvisualmappingin:\ntionsandvisualizetheresultsifthedatatransformationinformation\nspatialsubstrate,graphicalelements,andgraphicalproperties.",
    "tables_extraction_text_csv": "0\n\"IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n7\"\n\"can be extracted from the user’s NL queries. Otherwise, despite\"\n\"advances in technologies for data management and analysis [17],\"\n\"[67], it remains time-consuming to inspect a dataset and construct a\"\nvisualization that allows meaningful analysis to begin. So a practical\nsystem should be in a position to automatically recommend data\n\"insights for users. Fortunately, various systems have been designed\"\n\"to generate data facts in the context of visual data exploration, such\"\n\"as DataSite [42], Voder [225], Wrangler [99], Quickinsights [53],\"\n\"Foresight [46], and SpotLight [75]. They can serve as back-end\"\nengines for insight-driven recommendations.\n\"5.2\nNLI for Database\"\n\"Natural Language Interface for Database (NLI4DB), or Text-to-\"\n\"Fig. 5. Vague modiﬁer\ninterpretation in Sentiﬁers [63]. The system\"\n\"SQL,\nis a task to automatically translate a user’s query text\nin\"\nassociates the vague modiﬁer “unsafe” with magnitude attribute.\n\"natural\nlanguage into an executable query for databases like SQL\"\n\"5\nDATA TRANSFORMATION\"\n\"[6]. NLI4DB is strongly related to data transformation in V-NLI,\"\n\"After extracting data attributes, various data transformation oper-\nand V-NLI can augment NLI4DB with effective visualizations of\"\n\"ations can be made to transform raw data into focused data. This\nquery results. Besides, not all\nthe queries about a dataset need a\"\n\"section will\nintroduce how V-NLI systems transform raw data for\nvisualization as a response. For\ninstance, one might ask, “What\"\n\"data insights, with additional discussions on a closely related topic,\nwas\nthe GDP of\nthe USA last year?” or “Which country won\"\n\"NLI for databases.\nthe most gold medals in the Tokyo Olympics?”.\nIn such cases,\"\n\"NLI4DB can directly compute and present values in response to\n5.1\nTransformation for Data Insights\"\nthe user’s questions instead of displaying through visualizations\n\"A consensus is that\nthe purpose of visualization is insights, not\"\n\"that require the user to interpret for answers. Generally, there are\"\npictures. A “boring” dataset may become “interesting” after data\n\"three types of methods for NLI4DB: symbolic approach, statistical\"\n\"transformations (e.g., aggregation, binning, and grouping). There-\"\n\"approach, and connectionist (neural network) approach [175]. The\"\n\"fore,\nidentifying the data transformation information is another\"\n\"symbolic approach utilizes explicit\nlinguistic rules and knowledge\"\n\"important characteristic in V-NLI. To describe related works, we\"\n\"to process the user’s query, which dominated the area in the early\"\n\"categorize\ntabular data\ninto four\ntypes:\ntemporal, quantitative,\"\nyears. DISCERN [221] is a representative example and integrates a\n\"nominal, and ordinal, which are widely adopted in the visualization\"\n\"model\nthat extracts information from scripts,\nlexicon, and memory.\"\n\"community [168]. For temporal data, systems can support binning\"\nThe statistical approach exploits statistical learning methods such\n\"based on temporal units [62],\n[148],\n[165],\n[169],\n[174],\n[205],\"\n\"as hidden Markov models\n[141] and probabilistic context-free\"\n\"et al.\n[273]. For\ninstance, Setlur\n[205] developed a\ntemporal\"\n\"grammars\n[87]. From the view of\nthe\nconnectionist\napproach,\"\n\"analytics module\nbased\non\ntemporal\nentities\nand\nexpressions\"\n\"the Text-to-SQL task is a kind of\nspecial Machine Translation\"\n\"deﬁned in TimeML [91]. To parse\ntemporal\nexpressions,\nthis\"\n\"(MT) problem. However,\nit\nis hard to be directly handled with\"\n\"module incorporates the following temporal\ntoken types: Temporal\"\nstandard end-to-end neural machine translation models due to the\n\"Units\n(e.g., years, months, days, hours and minutes, and their\"\nabsence of detailed speciﬁcation in the user’s query [73]. Another\n\"abbreviations.), Temporal Prepositions (e.g., ’in,’ ’during,’ ’near,’\"\nproblem is the out-of-domain items mentioned in the user’s query\n\"and ’around.’),\nand Temporal Connectives\n(e.g.,\n‘before’\nand\"\n\"due to the user’s unawareness of the ontology set. Aiming at\nthe\"\n\"‘after’). For quantitative data, various aggregation functions (e.g.,\"\n\"two problems,\nIRNet\n[73] proposes a tree-shaped intermediate\"\n\"count, average, and sum) can be performed, binning and grouping\"\n\"representation synthesized from the user’s query. It\nis later fed into\"\n\"operations are also commonly used [10],\n[38],\n[41],\n[50],\n[63],\"\n\"an inference model\nto generate SQL statements with a domain\"\n\"[83], [93], [121], [148], [159], [178], [205], [208], [225], [273],\"\n\"knowledge base. Furthermore, to capture the special format of SQL\"\n\"[284]. For example, DeepEye [148] includes various aggregation\"\n\"statements, SQLnet [268] adopts a universal sketch as a template\"\n\"functions in the visualization language. Arklang [208], which is an\"\nand predicts values for each slot. Wang et al. [252] employed a\n\"intermediate language to resolve NL utterances to formal queries,\"\ntwo-stage pipeline to subsequently predict the semantic structure\nintegrates a ﬁnite set of aggregation operators. Eviza [205] enables\nand generate SQL statements with structure-enhanced query text.\n\"aggregation using regular\nspatial bins. Unlike continuous data,\"\n\"Recently, TaPas [80] extends BERT’s architecture and trains from\"\nnominal and ordinal data are usually used as grouping metrics. As\n\"weak supervision to answer questions over tables. For more details,\"\n\"an end-to-end system, ncNet [150] supports more complex data\"\n\"please refer to the survey [6], [290] and related papers in Table 2.\"\n\"transformation types (e.g., relational\njoin, GroupBY, and OrderBY)\"\nDiscussion: V-NLI can augment NLI4DB with well-designed\nby applying Neural Machine Translation (NMT) models. After\n\"visualizations, and NLI4DB can complement V-NLI with exact\"\n\"extracting relevant\ninformation,\nthe commonly used visualization\"\n\"value answers. First,\nthe two ﬁelds can draw on each other as\"\n\"speciﬁcation languages\n(e.g., Vega [198], Vega-Lite [197], and\"\n\"they share similar principles. Second, we can combine the two\"\nVizQL [234]) all provide various data transformation primitives\n\"technologies\nto generate visualizations\nthat “contain” answers.\"\n\"to realize. Besides,\nto our knowledge, most V-NLIs are designed\"\n\"Third,\nthere are many benchmarks in the NLI4DB community like\"\n\"to deal with tabular data, but\nthere are also several systems that\"\n\"WikiSQL [287] and Spider [274], which can be utilized to further\"\n\"focus on speciﬁc data types (e.g., network [231] and map [204]).\"\ndesign V-NLI benchmarks.\n\"However,\nthey mostly only display the\nraw data without data\"\ntransformations.\n\"6\nVISUAL MAPPING\"\nDiscussion: The back-end engine can directly perform calcula-\nThis section will discuss how V-NLIs perform visual mapping in:\ntions and visualize the results if the data transformation information\n\"spatial substrate, graphical elements, and graphical properties.\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 8,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n8\n6.1\nSpatial Substrate\nSpatial substrate is the space to create visualizations. It is important\nto determine the layout conﬁgurations to apply in the spatial\nsubstrate, such as which axes to use. Some V-NLI systems support\nexplicitly specifying layout information like inputting “show GDP\nseries at y axis and year at x axis grouped by Country Code”. If the\nmapping information is not clariﬁed in the query, the search space\nwill be very huge, in which some combinations of data attributes\nand visual encodings may not generate a valid visualization.\nFor instance, the encoding type “Y-axis” is inappropriate for\ncategorical attributes. Fortunately, there are many design rules\neither from traditional wisdom or users to help prune meaningless\nvisualizations. These design rules are typically given by experts.\nVoyager [259], [260], [261], DIVE [86], Show me [154], Polaris\n[234], Proﬁler [100], DeepEye [149], and Draco [168] all contribute\nto the enrichment of design rules with more data types. Besides,\nmany systems [168], [247], [261] also allow users to specify their\ninterested data attributes and assign them on certain axes, which\nis a more direct way to perform visual mapping. In Falx [250],\nusers can specify visualizations with examples of visual mapping,\nand the system automatically infers the visualization speciﬁcation\nand transforms data to match the design. With the development of\nmachine learning in recent years, advanced models can be applied\nfor more effective visual mapping. VizML [84] identiﬁes ﬁve key\ndesign choices while creating visualizations, including mark type,\nX or Y column encoding, and whether X or Y is the single column\nrepresented along that axis or not. For a new dataset, 841 dataset-\nlevel features extracted are fed into neural network models to\npredict the design choices. Luo et al. [150] proposed an end-to-end\napproach that applies the transformer-based seq2seq [12] model to\ndirectly map NL queries and data to chart templates. In addition to\nsingle chart, considering multiple visualizations, Evizeon [83] uses\na grid-based layout algorithm to position new charts, while Voder\n[225] allows the user to choose a slide or dashboard layout.\nDiscussion: When performing visual mappings in the spatial\nsubstrate, the vast majority of systems are limited to 2-dimension\nspace (along x and y axes). However, considering more axes\ntypes, creating 3-dimensional (e.g., add z axes) and even hyper-\ndimensional (e.g., parallel coordinate) representations is possible.\n6.2\nGraphical Elements\nGraphical element (e.g., points, lines, surfaces, and volumes),\nwhich is usually named mark or chart type, is an important\npart of visualizations. Choosing an appropriate mark can convey\ninformation more efﬁciently. Similar to deciding the layout, some\nV-NLI systems allow users to specify marks like inputting “create\na scatterplot of mpg and cylinders”, which is a simple way to map\nmark information of visualizations. However, in most cases, the\nmark information is not accessible. So after parsing the NL queries,\nmost systems integrate predeﬁned rules or leverage Visualization\nRecommendation (VisRec) technologies to deal with the extracted\nelements. Voyager [260], [261] applies visualization design best\npractices drawn from prior research [153], [154], [192], [203]\nand recommends mark types based on the data types of the x\nand y channels. Srinivasan et al. [225] developed the heuristics\nand mappings between data attribute combinations, analytic tasks,\nand chart types in Table 6 through iterative informal feedback\nfrom fellow researchers and students. TaskVis [211] integrates 18\nclassical low-level analysis tasks with their appropriate chart types\nby a survey both in academia and industry. Instead of considering\ncommon visualization types, Wang et al. [253] developed rules\nTABLE 6\nMappings between data attribute combinations (N: Numeric, C:\nCategorical), analytic tasks, and chart types in Voder [225].\nAttributes\nTask(s)\nVisualization\nN\nFind Extremum\nStrip plot\nCharacterize Distribution\nStrip plot\nBox plot\nHistogram\nFind Anomalies\nStrip plot\nBox plot\nC\nFind Extremum\nBar chart\nDomut chart\nCharacterize Distribution\nNxN\nCorelation\nScatterplot\nCharacterize Distribution\nCxN\nCharacterize Distribution(+Derived Value) Bar chart\nDomut chart\nFind Extremum(+Derived Value)\nFind Extremum\nStrip plot\nScatterplot\nCxC\nFind Extremum\nStacked bar chart\nScatterplot+Size\nCharacterize Distribution(+Find Extremum)\nNxNxN\nCharacterize Distribution\nScatterplot+Size\nCxNxN\nCorrelation\nScatterplot\nCorrelation(+Filter)\nScatterplot+Color\nCharacterize Distibution(+Filter)\nScatterplot+Color\nScatterplot+Size\nCxCxN\nFind Extremum\nStrip plot+Color\nScatterplot+Color\nFind Extremum(+Derived Value)\nStrip plot+Color\nScatterplot+Color\nScatterplot+Size\nfor automatic selection of line graphs or scatter plots to visualize\ntrends in time series, while Sarikaya et al. [196] deepened into\nscatter charts. To better beneﬁt from design guidance provided by\nempirical studies, Moritz et al. [168] proposed a formal framework\nthat models visualization design knowledge as a collection of\nanswer set programming constraints.\nDiscussion: V-NLIs mostly maintain an alias map for data\nattribute inference (see Section 4.3) but often ignore mark inference.\nTherefore, though they can explicitly interpret mark information\nfrom the user’s NL queries, they may fail when the information is\nimplicitly expressed (e.g., “point” instead of “scatterplot”). Besides,\nas shown in Table 3 (column Visualization Type), existing works\nfocus more on traditional chart types, more chart types, and even\ninteractive visualizations can be extended in future work.\n6.3\nGraphical Properties\nJacques Bertin ﬁrst identiﬁed seven “retinal” graphical properties\n(visual encoding properties) in visualizations: position, size, value,\ntexture, color, orientation, and shape. Some other types are later\nexpanded in the community, such as “gestalt” properties (e.g.,\nconnectivity and grouping) and animation [153], [243]. For visual\nmapping in V-NLI, color, size, and shape are most commonly\napplied to graphical elements, making them more noticeable. The\nrule-based approach is also dominant here, and human perceptual\neffectiveness metrics are usually considered. For example, the\ncardinality of variables in the color/size/shape channel should not\nbe too high (e.g., 100). Otherwise, the chart would be too messy\nto distinguish. The aforementioned works also developed various\ndesign rules for graphical properties, especially for color, size,\nand shape channels in legend [86], [100], [149], [154], [168],\n[211], [234], [260], [261]. Besides, Text-to-Viz [41] integrates a\ncolor module that aims to generate a set of color palettes for a\nspeciﬁc infographic. Similarly, InfoColorizer [275] recommends\ncolor palettes for infographics in an interactive and dynamic\nmanner. Wu et al. [264] proposed a learning-based method to\nautomate layout parameter conﬁgurations, including orientation,\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 8\n6.1 SpatialSubstrate TABLE6\nMappingsbetweendataattributecombinations(N:Numeric,C:\nSpatialsubstrateisthespacetocreatevisualizations.Itisimportant\nCategorical),analytictasks,andcharttypesinVoder[225].\nto determine the layout configurations to apply in the spatial\nAttributes Task(s) Visualization\nsubstrate,suchaswhichaxestouse.SomeV-NLIsystemssupport\nFindExtremum Stripplot\nexplicitlyspecifyinglayoutinformationlikeinputting“showGDP Stripplot\nseriesatyaxisandyearatxaxisgroupedbyCountryCode”.Ifthe CharacterizeDistribution Boxplot\nN\nHistogram\nmappinginformationisnotclarifiedinthequery,thesearchspace\nStripplot\nwillbeveryhuge,inwhichsomecombinationsofdataattributes FindAnomalies Boxplot\nand visual encodings may not generate a valid visualization. FindExtremum Barchart\nC\nFor instance, the encoding type “Y-axis” is inappropriate for CharacterizeDistribution Domutchart\nCorelation\ncategorical attributes. Fortunately, there are many design rules NxN Scatterplot\nCharacterizeDistribution\neitherfromtraditionalwisdomoruserstohelpprunemeaningless CharacterizeDistribution(+DerivedValue) Barchart\nvisualizations. These design rules are typically given by experts. CxN FindExtremum(+DerivedValue) Domutchart\nVoyager[259],[260],[261],DIVE[86],Showme[154],Polaris FindExtremum Stripplot\nScatterplot\n[234],Profiler[100],DeepEye[149],andDraco[168]allcontribute\nFindExtremum Stackedbarchart\ntotheenrichmentofdesignruleswithmoredatatypes.Besides, CxC CharacterizeDistribution(+FindExtremum) Scatterplot+Size\nmanysystems[168],[247],[261]alsoallowuserstospecifytheir NxNxN CharacterizeDistribution Scatterplot+Size\ninteresteddataattributesandassignthemoncertainaxes,which Correlation Scatterplot\nCorrelation(+Filter) Scatterplot+Color\nis a more direct way to perform visual mapping. In Falx [250], CxNxN\nScatterplot+Color\nuserscanspecifyvisualizationswithexamplesofvisualmapping, CharacterizeDistibution(+Filter) Scatterplot+Size\nandthesystemautomaticallyinfersthevisualizationspecification Stripplot+Color\nFindExtremum\nandtransformsdatatomatchthedesign.Withthedevelopmentof CxCxN Scatterplot+Color\nStripplot+Color\nmachinelearninginrecentyears,advancedmodelscanbeapplied\nFindExtremum(+DerivedValue) Scatterplot+Color\nformoreeffectivevisualmapping.VizML[84]identifiesfivekey Scatterplot+Size\ndesignchoiceswhilecreatingvisualizations,includingmarktype,\nforautomaticselectionoflinegraphsorscatterplotstovisualize\nXorYcolumnencoding,andwhetherXorYisthesinglecolumn\ntrends in time series, while Sarikaya et al. [196] deepened into\nrepresentedalongthataxisornot.Foranewdataset,841dataset-\nscattercharts.Tobetterbenefitfromdesignguidanceprovidedby\nlevel features extracted are fed into neural network models to\nempiricalstudies,Moritzetal.[168]proposedaformalframework\npredictthedesignchoices.Luoetal.[150]proposedanend-to-end\nthat models visualization design knowledge as a collection of\napproachthatappliesthetransformer-basedseq2seq[12]modelto\nanswersetprogrammingconstraints.\ndirectlymapNLqueriesanddatatocharttemplates.Inadditionto\nDiscussion: V-NLIs mostly maintain an alias map for data\nsinglechart,consideringmultiplevisualizations,Evizeon[83]uses\nattributeinference(seeSection4.3)butoftenignoremarkinference.\nagrid-basedlayoutalgorithmtopositionnewcharts,whileVoder\nTherefore,thoughtheycanexplicitlyinterpretmarkinformation\n[225]allowstheusertochooseaslideordashboardlayout.\nfromtheuser’sNLqueries,theymayfailwhentheinformationis\nDiscussion:Whenperformingvisualmappingsinthespatial\nimplicitlyexpressed(e.g.,“point”insteadof“scatterplot”).Besides,\nsubstrate,thevastmajorityofsystemsarelimitedto2-dimension\nasshowninTable3(columnVisualizationType),existingworks\nspace (along x and y axes). However, considering more axes\nfocusmoreontraditionalcharttypes,morecharttypes,andeven\ntypes, creating 3-dimensional (e.g., add z axes) and even hyper-\ninteractivevisualizationscanbeextendedinfuturework.\ndimensional(e.g.,parallelcoordinate)representationsispossible.\n6.2 GraphicalElements 6.3 GraphicalProperties\nGraphical element (e.g., points, lines, surfaces, and volumes), JacquesBertinfirstidentifiedseven“retinal”graphicalproperties\nwhich is usually named mark or chart type, is an important (visualencodingproperties)invisualizations:position,size,value,\npartofvisualizations.Choosinganappropriatemarkcanconvey texture,color,orientation,andshape.Someothertypesarelater\ninformationmoreefficiently.Similartodecidingthelayout,some expanded in the community, such as “gestalt” properties (e.g.,\nV-NLIsystemsallowuserstospecifymarkslikeinputting“create connectivityandgrouping)andanimation[153],[243].Forvisual\nascatterplotofmpgandcylinders”,whichisasimplewaytomap mapping in V-NLI, color, size, and shape are most commonly\nmark information of visualizations. However, in most cases, the appliedtographicalelements,makingthemmorenoticeable.The\nmarkinformationisnotaccessible.SoafterparsingtheNLqueries, rule-basedapproachisalsodominanthere,andhumanperceptual\nmostsystemsintegratepredefinedrulesorleverageVisualization effectiveness metrics are usually considered. For example, the\nRecommendation(VisRec)technologiestodealwiththeextracted cardinalityofvariablesinthecolor/size/shapechannelshouldnot\nelements.Voyager[260],[261]appliesvisualizationdesignbest betoohigh(e.g.,100).Otherwise,thechartwouldbetoomessy\npractices drawn from prior research [153], [154], [192], [203] todistinguish.Theaforementionedworksalsodevelopedvarious\nand recommends mark types based on the data types of the x design rules for graphical properties, especially for color, size,\nand y channels. Srinivasan et al. [225] developed the heuristics and shape channels in legend [86], [100], [149], [154], [168],\nandmappingsbetweendataattributecombinations,analytictasks, [211],[234],[260],[261].Besides,Text-to-Viz[41]integratesa\nand chart types in Table 6 through iterative informal feedback color module that aims to generate a set of color palettes for a\nfromfellowresearchersandstudents.TaskVis[211]integrates18 specificinfographic.Similarly,InfoColorizer[275]recommends\nclassicallow-levelanalysistaskswiththeirappropriatecharttypes color palettes for infographics in an interactive and dynamic\nbyasurveybothinacademiaandindustry.Insteadofconsidering manner. Wu et al. [264] proposed a learning-based method to\ncommon visualization types, Wang et al. [253] developed rules automate layout parameter configurations, including orientation,",
    "tables_extraction_text_csv": "0\n\"IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n8\"\n\"TABLE 6\n6.1\nSpatial Substrate\"\n\"Mappings between data attribute combinations (N: Numeric, C:\"\nSpatial substrate is the space to create visualizations. It is important\n\"Categorical), analytic tasks, and chart\ntypes in Voder [225].\"\n\"to determine\nthe\nlayout\nconﬁgurations\nto apply in the\nspatial\"\n\"Attributes\nTask(s)\nVisualization\"\n\"substrate, such as which axes to use. Some V-NLI systems support\"\n\"Find Extremum\nStrip plot\"\n\"Strip plot\nexplicitly specifying layout\ninformation like inputting “show GDP\"\n\"Characterize Distribution\nBox plot\"\n\"series at y axis and year at x axis grouped by Country Code”. If the\nN\"\nHistogram\n\"mapping information is not clariﬁed in the query,\nthe search space\"\nStrip plot\nFind Anomalies\n\"will be very huge, in which some combinations of data attributes\nBox plot\"\n\"Find Extremum\nBar chart\nand\nvisual\nencodings may\nnot\ngenerate\na\nvalid\nvisualization.\"\nC\n\"Characterize Distribution\nDomut chart\"\n\"For\ninstance,\nthe\nencoding type\n“Y-axis”\nis\ninappropriate\nfor\"\nCorelation\n\"NxN\nScatterplot\ncategorical attributes. Fortunately,\nthere are many design rules\"\nCharacterize Distribution\n\"either from traditional wisdom or users to help prune meaningless\nCharacterize Distribution(+Derived Value)\nBar chart\"\n\"CxN\nFind Extremum(+Derived Value)\nDomut chart\nvisualizations. These design rules are typically given by experts.\"\nStrip plot\n\"Voyager [259], [260], [261], DIVE [86], Show me [154], Polaris\nFind Extremum\"\nScatterplot\n\"[234], Proﬁler [100], DeepEye [149], and Draco [168] all contribute\"\n\"Find Extremum\nStacked bar chart\"\nCxC\n\"to the enrichment of design rules with more data types. Besides,\nCharacterize Distribution(+Find Extremum)\nScatterplot+Size\"\n\"NxNxN\nCharacterize Distribution\nScatterplot+Size\nmany systems [168], [247], [261] also allow users to specify their\"\n\"Correlation\nScatterplot\"\n\"interested data attributes and assign them on certain axes, which\"\n\"Correlation(+Filter)\nScatterplot+Color\"\n\"CxNxN\nis a more direct way to perform visual mapping.\nIn Falx [250],\"\nScatterplot+Color\nCharacterize Distibution(+Filter)\n\"users can specify visualizations with examples of visual mapping,\nScatterplot+Size\"\n\"Strip plot+Color\nand the system automatically infers the visualization speciﬁcation\"\nFind Extremum\nScatterplot+Color\n\"and transforms data to match the design. With the development of\nCxCxN\"\nStrip plot+Color\n\"machine learning in recent years, advanced models can be applied\"\n\"Scatterplot+Color\nFind Extremum(+Derived Value)\"\n\"Scatterplot+Size\nfor more effective visual mapping. VizML [84] identiﬁes ﬁve key\"\n\"design choices while creating visualizations,\nincluding mark type,\"\nfor automatic selection of line graphs or scatter plots to visualize\n\"X or Y column encoding, and whether X or Y is the single column\"\n\"trends in time series, while Sarikaya et al.\n[196] deepened into\"\n\"represented along that axis or not. For a new dataset, 841 dataset-\"\nscatter charts. To better beneﬁt from design guidance provided by\n\"level\nfeatures extracted are fed into neural network models\nto\"\n\"empirical studies, Moritz et al. [168] proposed a formal framework\"\npredict the design choices. Luo et al. [150] proposed an end-to-end\nthat models visualization design knowledge as a collection of\n\"approach that applies the transformer-based seq2seq [12] model\nto\"\nanswer set programming constraints.\n\"directly map NL queries and data to chart\ntemplates. In addition to\"\nDiscussion: V-NLIs mostly maintain an alias map for data\n\"single chart, considering multiple visualizations, Evizeon [83] uses\"\nattribute inference (see Section 4.3) but often ignore mark inference.\n\"a grid-based layout algorithm to position new charts, while Voder\"\n\"Therefore,\nthough they can explicitly interpret mark information\"\n[225] allows the user to choose a slide or dashboard layout.\n\"from the user’s NL queries,\nthey may fail when the information is\"\nDiscussion: When performing visual mappings in the spatial\n\"implicitly expressed (e.g., “point” instead of “scatterplot”). Besides,\"\n\"substrate,\nthe vast majority of systems are limited to 2-dimension\"\n\"as shown in Table 3 (column Visualization Type), existing works\"\n\"space\n(along x and y axes). However,\nconsidering more\naxes\"\n\"focus more on traditional chart types, more chart types, and even\"\n\"types, creating 3-dimensional (e.g., add z axes) and even hyper-\"\ninteractive visualizations can be extended in future work.\n\"dimensional (e.g., parallel coordinate) representations is possible.\"\n\"6.3\nGraphical Properties\n6.2\nGraphical Elements\"\n\"Graphical\nelement\n(e.g., points,\nlines,\nsurfaces,\nand volumes),\nJacques Bertin ﬁrst identiﬁed seven “retinal” graphical properties\"\n\"which\nis\nusually\nnamed mark\nor\nchart\ntype,\nis\nan\nimportant\n(visual encoding properties) in visualizations: position, size, value,\"\n\"part of visualizations. Choosing an appropriate mark can convey\ntexture, color, orientation, and shape. Some other types are later\"\n\"information more efﬁciently. Similar to deciding the layout, some\nexpanded in the community,\nsuch as “gestalt” properties\n(e.g.,\"\n\"V-NLI systems allow users to specify marks like inputting “create\nconnectivity and grouping) and animation [153], [243]. For visual\"\n\"a scatterplot of mpg and cylinders”, which is a simple way to map\nmapping in V-NLI, color,\nsize, and shape are most commonly\"\n\"mark information of visualizations. However,\nin most cases,\nthe\napplied to graphical elements, making them more noticeable. The\"\n\"mark information is not accessible. So after parsing the NL queries,\nrule-based approach is also dominant here, and human perceptual\"\n\"most systems integrate predeﬁned rules or leverage Visualization\neffectiveness metrics are usually considered. For example,\nthe\"\n\"Recommendation (VisRec) technologies to deal with the extracted\ncardinality of variables in the color/size/shape channel should not\"\n\"elements. Voyager [260], [261] applies visualization design best\nbe too high (e.g., 100). Otherwise,\nthe chart would be too messy\"\n\"practices drawn from prior\nresearch [153],\n[154],\n[192],\n[203]\nto distinguish. The aforementioned works also developed various\"\n\"and recommends mark types based on the data types of\nthe x\ndesign rules\nfor graphical properties, especially for color,\nsize,\"\n\"and y channels. Srinivasan et al.\n[225] developed the heuristics\nand shape channels\nin legend [86],\n[100],\n[149],\n[154],\n[168],\"\n\"and mappings between data attribute combinations, analytic tasks,\n[211], [234], [260], [261]. Besides, Text-to-Viz [41] integrates a\"\n\"and chart\ntypes\nin Table 6 through iterative informal\nfeedback\ncolor module that aims to generate a set of color palettes for a\"\n\"from fellow researchers and students. TaskVis [211] integrates 18\nspeciﬁc infographic. Similarly, InfoColorizer [275] recommends\"\n\"classical\nlow-level analysis tasks with their appropriate chart\ntypes\ncolor palettes\nfor\ninfographics\nin an interactive\nand dynamic\"\n\"by a survey both in academia and industry. Instead of considering\nmanner. Wu et al.\n[264] proposed a learning-based method to\"\n\"common visualization types, Wang et al.\n[253] developed rules\nautomate layout parameter conﬁgurations,\nincluding orientation,\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 9,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n9\nFig. 6. User interface of Orko [231] to explore a network of European\nsoccer players.\nbar bandwidth, max label length, etc. Liu et al. [146] explored\ndata-driven mark orientation for trend estimation in scatterplots.\nCAST [65] and Data Animator [240] enable interactive creation of\nchart animations.\nDiscussion: For graphical properties, color, size, and shape\naccount for the majority in current V-NLIs. Although some other\nforms have been investigated deeply, they are rarely used in V-NLI\n(e.g., orientation and texture). Besides, color is a tricky choice to\nuse. On one hand, it is unfriendly to those who are color-blind. On\nthe other hand, it may involve cultural issues in certain scenarios\n(e.g., red is a “festive” color in China, whereas it may mean\n“warning” in many Western societies). So color needs to be used\nwith the discretion of color vision deﬁciencies and cultural meaning.\n7\nVIEW TRANSFORMATION\nAfter visual mapping, the generated visualization speciﬁcations\ncan be rendered through a library (e.g., D3 [23]). View transfor-\nmations are also supported here. The three commonly used view\ntransformation types are location probes which reveal additional\ninformation through locations, viewpoint controls that scale or\ntranslate a view, and distortions that modify the visual structure\n[28]. Surprisingly, this stage is rarely used in the context of our\nsurvey. To our knowledge, there have been few works that focus on\nnatural language interaction for view transformation. It is usually\nrealized through multimodal interaction, as shown in Table 3\n(column View Transformation). For example, Orko [231] facilitates\nboth natural language and direct manipulation input to explore\nnetwork visualization. Figure 6 is the user interface of Orko, and\nit shows an example that explores a network of European soccer\nplayers. When the user says “Show connection between Ronaldo\nand Bale”, the system will zoom in to display the details of nodes.\nIt also allows users to perform view transformation (e.g., zooming\nand panning) through a pen or ﬁnger. Similarly, Valletto [103],\nInChorus [226], Data@Hand [115], DataBreeze [227], Power BI’s\nQ&A [5], and Tableau’s Ask data [2] all support standard view\ntransformations. Besides, the software visualization community\nalso presents several closely related works. Bieliauskas et al. [20]\nproposed an interaction approach with software visualizations\nbased on a conversational interface. It can automatically display\nbest ﬁtting views according to meta information from natural\nlanguage sentences. Seipel et al. [201] explored natural language\ninteraction for software visualization in Augmented Reality (AR)\nwith Microsoft HoloLens device, which can provide various view\ntransformation interactions through gesture, gaze, and speech.\nDiscussion: View transformation has attracted less attention in\nthe community. Only a few V-NLI systems support NL control over\nview transformations, and they are mainly limited to viewpoint\nFig. 7. Ambiguity widgets in Datatone [63]. The user can correct the\nimprecise system decisions caused by ambiguity.\nnavigation. Future improvements can focus on other aspects (e.g.,\nanimation [240], data-GIF [215], and visual distortions [176]).\n8\nHUMAN INTERACTION\nThis section will discuss how V-NLI allows users to provide\nfeedback to visualization with human interactions. The primary\npurpose is to help users better express their intents.\n8.1\nAmbiguity Widgets\nDue to the vague nature of natural language, the system may\nfail to recognize the user intent and extract data attributes [161].\nConsiderable research has been devoted to addressing the ambiguity\nand underspeciﬁcation of NL queries. Mainstream approaches can\nbe divided into two categories. One is to generate appropriate\ndefaults by inferencing underspeciﬁed natural language utterances,\ndiscussed in Section 4.4. The other is to return the decision right\nto users through ambiguity widgets. Ambiguity widget is a kind\nof interactive widget that allows users to input through a mouse.\nDatatone [63] ﬁrst integrates ambiguity widgets and presents a\nmixed-initiative approach to manage ambiguity in the user query.\nAs shown in Figure 7, for the query in (b), DataTone detects data\nambiguities about medal, hockey, and skating. Three corresponding\nambiguity widgets are presented for users to correct the system\nchoices in (c). Additional design decision widgets like color and\naggregation are also available in (e). Eviza [205], Evizeon [83],\nNL4DV [174], and AUDiaL [169] all borrow the idea of DataTone\nand expand the ambiguity widgets to richer forms (see Table\n3 (column Ambiguity Widgets)), such as maps and calendars.\nDIY [173] enables users to interactively assess the response from\nNLI4DB systems for correctness.\nDiscussion: Resolving underspeciﬁed queries through ambi-\nguity widgets mainly consists of two steps: detecting ambiguity\nand presenting widgets to the user. For the ﬁrst step, most systems\nnow still leverage heuristics for algorithmic resolution of ambiguity.\nA general probabilistic framework may handle a broader range\nof ambiguities. For the second, richer interactive widgets can be\nadopted to enhance the user experience.\n8.2\nAutocompletion and Command Suggestion\nUsers may be unaware of what operations the system can perform\nand whether a speciﬁc language structure is preferred in the\nsystem. Although advanced text understanding models give users\nthe freedom to express their intents, system discoverability to\nhelp formulate analytical questions is still an indispensable part\nof V-NLI. Discoverability entails awareness: making users aware\nof what operations the system can perform, and understanding:\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 9\nFig.6.UserinterfaceofOrko[231]toexploreanetworkofEuropean\nsoccerplayers.\nbar bandwidth, max label length, etc. Liu et al. [146] explored\ndata-driven mark orientation for trend estimation in scatterplots. Fig. 7. Ambiguity widgets in Datatone [63]. The user can correct the\nimprecisesystemdecisionscausedbyambiguity.\nCAST[65]andDataAnimator[240]enableinteractivecreationof\nchartanimations. navigation.Futureimprovementscanfocusonotheraspects(e.g.,\nDiscussion: For graphical properties, color, size, and shape animation[240],data-GIF[215],andvisualdistortions[176]).\naccountforthemajorityincurrentV-NLIs.Althoughsomeother\n8 HUMAN INTERACTION\nformshavebeeninvestigateddeeply,theyarerarelyusedinV-NLI\n(e.g.,orientationandtexture).Besides,colorisatrickychoiceto This section will discuss how V-NLI allows users to provide\nuse.Ononehand,itisunfriendlytothosewhoarecolor-blind.On feedback to visualization with human interactions. The primary\ntheotherhand,itmayinvolveculturalissuesincertainscenarios purposeistohelpusersbetterexpresstheirintents.\n(e.g., red is a “festive” color in China, whereas it may mean\n8.1 AmbiguityWidgets\n“warning”inmanyWesternsocieties).Socolorneedstobeused\nDue to the vague nature of natural language, the system may\nwiththediscretionofcolorvisiondeficienciesandculturalmeaning.\nfailtorecognizetheuserintentandextractdataattributes[161].\nConsiderableresearchhasbeendevotedtoaddressingtheambiguity\n7 VIEW TRANSFORMATION andunderspecificationofNLqueries.Mainstreamapproachescan\nbe divided into two categories. One is to generate appropriate\nAfter visual mapping, the generated visualization specifications defaultsbyinferencingunderspecifiednaturallanguageutterances,\ncan be rendered through a library (e.g., D3 [23]). View transfor- discussedinSection4.4.Theotheristoreturnthedecisionright\nmationsarealsosupportedhere.Thethreecommonlyusedview tousersthroughambiguitywidgets.Ambiguitywidgetisakind\ntransformationtypesarelocationprobeswhichrevealadditional ofinteractivewidgetthatallowsuserstoinputthroughamouse.\ninformation through locations, viewpoint controls that scale or Datatone [63] first integrates ambiguity widgets and presents a\ntranslate a view, and distortions that modify the visual structure mixed-initiativeapproachtomanageambiguityintheuserquery.\n[28]. Surprisingly, this stage is rarely used in the context of our AsshowninFigure7,forthequeryin(b),DataTonedetectsdata\nsurvey.Toourknowledge,therehavebeenfewworksthatfocuson ambiguitiesaboutmedal,hockey,andskating.Threecorresponding\nnaturallanguageinteractionforviewtransformation.Itisusually ambiguity widgets are presented for users to correct the system\nrealized through multimodal interaction, as shown in Table 3 choicesin(c).Additionaldesigndecisionwidgetslikecolorand\n(columnViewTransformation).Forexample,Orko[231]facilitates\naggregation are also available in (e). Eviza [205], Evizeon [83],\nboth natural language and direct manipulation input to explore NL4DV[174],andAUDiaL[169]allborrowtheideaofDataTone\nnetworkvisualization.Figure6istheuserinterfaceofOrko,and and expand the ambiguity widgets to richer forms (see Table\nitshowsanexamplethatexploresanetworkofEuropeansoccer 3 (column Ambiguity Widgets)), such as maps and calendars.\nplayers.Whentheusersays“ShowconnectionbetweenRonaldo DIY[173]enablesuserstointeractivelyassesstheresponsefrom\nandBale”,thesystemwillzoomintodisplaythedetailsofnodes. NLI4DBsystemsforcorrectness.\nItalsoallowsuserstoperformviewtransformation(e.g.,zooming Discussion: Resolving underspecified queries through ambi-\nand panning) through a pen or finger. Similarly, Valletto [103], guity widgets mainly consists of two steps: detecting ambiguity\nInChorus[226],Data@Hand[115],DataBreeze[227],PowerBI’s andpresentingwidgetstotheuser.Forthefirststep,mostsystems\nQ&A [5], and Tableau’s Ask data [2] all support standard view nowstillleverageheuristicsforalgorithmicresolutionofambiguity.\ntransformations. Besides, the software visualization community A general probabilistic framework may handle a broader range\nalsopresentsseveralcloselyrelatedworks.Bieliauskasetal.[20] ofambiguities.Forthesecond,richerinteractivewidgetscanbe\nproposed an interaction approach with software visualizations adoptedtoenhancetheuserexperience.\nbasedonaconversationalinterface.Itcanautomaticallydisplay\nbest fitting views according to meta information from natural 8.2 AutocompletionandCommandSuggestion\nlanguagesentences.Seipeletal.[201]explorednaturallanguage Usersmaybeunawareofwhatoperationsthesystemcanperform\ninteractionforsoftwarevisualizationinAugmentedReality(AR) and whether a specific language structure is preferred in the\nwithMicrosoftHoloLensdevice,whichcanprovidevariousview system.Althoughadvancedtextunderstandingmodelsgiveusers\ntransformationinteractionsthroughgesture,gaze,andspeech. the freedom to express their intents, system discoverability to\nDiscussion:Viewtransformationhasattractedlessattentionin help formulate analytical questions is still an indispensable part\nthecommunity.OnlyafewV-NLIsystemssupportNLcontrolover ofV-NLI.Discoverabilityentailsawareness:makingusersaware\nview transformations, and they are mainly limited to viewpoint of what operations the system can perform, and understanding:",
    "tables_extraction_text_csv": "0,1\n\"Fig. 6. User interface of Orko [231]\nto explore a network of European\",\nsoccer players.,\n\"bar bandwidth, max label\nlength, etc. Liu et al.\n[146] explored\",\n,\"Fig. 7. Ambiguity widgets in Datatone [63]. The user can correct\nthe\"\ndata-driven mark orientation for trend estimation in scatterplots.,\n,imprecise system decisions caused by ambiguity.\nCAST [65] and Data Animator [240] enable interactive creation of,\nchart animations.,\"navigation. Future improvements can focus on other aspects (e.g.,\"\n\"Discussion: For graphical properties, color, size, and shape\",\"animation [240], data-GIF [215], and visual distortions [176]).\"\naccount for the majority in current V-NLIs. Although some other,\n,\"8\nHUMAN INTERACTION\"\n\"forms have been investigated deeply,\nthey are rarely used in V-NLI\",\n\"(e.g., orientation and texture). Besides, color is a tricky choice to\",\"This\nsection will discuss how V-NLI\nallows users\nto provide\"\n\"use. On one hand,\nit\nis unfriendly to those who are color-blind. On\",feedback to visualization with human interactions. The primary\n\"the other hand, it may involve cultural issues in certain scenarios\",purpose is to help users better express their intents.\n\"(e.g.,\nred is\na\n“festive”\ncolor\nin China, whereas\nit may mean\",\n,\"8.1\nAmbiguity Widgets\"\n“warning” in many Western societies). So color needs to be used,\n,\"Due to the vague nature of natural\nlanguage,\nthe system may\"\nwith the discretion of color vision deﬁciencies and cultural meaning.,\n,\"fail\nto recognize the user intent and extract data attributes [161].\"\n,Considerable research has been devoted to addressing the ambiguity\n,and underspeciﬁcation of NL queries. Mainstream approaches can\n\"7\nVIEW TRANSFORMATION\",\n,\"be divided into two categories. One is\nto generate appropriate\"\n\"After visual mapping,\nthe generated visualization speciﬁcations\",\n,\"defaults by inferencing underspeciﬁed natural\nlanguage utterances,\"\n\"can be rendered through a library (e.g., D3 [23]). View transfor-\",\n,discussed in Section 4.4. The other is to return the decision right\nmations are also supported here. The three commonly used view,\n,\"to users through ambiguity widgets. Ambiguity widget\nis a kind\"\ntransformation types are location probes which reveal additional,\n,\"of interactive widget\nthat allows users to input\nthrough a mouse.\"\n\"information through locations, viewpoint controls\nthat\nscale or\",\n,\"Datatone [63] ﬁrst\nintegrates ambiguity widgets and presents a\"\n\"translate a view, and distortions that modify the visual structure\",\n,mixed-initiative approach to manage ambiguity in the user query.\n\"[28]. Surprisingly,\nthis stage is rarely used in the context of our\",\n,\"As shown in Figure 7, for the query in (b), DataTone detects data\"\n\"survey. To our knowledge, there have been few works that focus on\",\n,\"ambiguities about medal, hockey, and skating. Three corresponding\"\nnatural language interaction for view transformation. It is usually,\n,\"ambiguity widgets are presented for users to correct\nthe system\"\n\"realized through multimodal\ninteraction,\nas\nshown in Table 3\",\n,choices in (c). Additional design decision widgets like color and\n\"(column View Transformation). For example, Orko [231] facilitates\",\n,\"aggregation are also available in (e). Eviza [205], Evizeon [83],\"\n\"both natural\nlanguage and direct manipulation input\nto explore\",\n,\"NL4DV [174], and AUDiaL [169] all borrow the idea of DataTone\"\n\"network visualization. Figure 6 is the user interface of Orko, and\",\n,\"and expand the\nambiguity widgets\nto richer\nforms\n(see Table\"\nit shows an example that explores a network of European soccer,\n,\"3 (column Ambiguity Widgets)),\nsuch as maps\nand calendars.\"\nplayers. When the user says “Show connection between Ronaldo,\n,DIY [173] enables users to interactively assess the response from\n\"and Bale”,\nthe system will zoom in to display the details of nodes.\",\n,NLI4DB systems for correctness.\n\"It also allows users to perform view transformation (e.g., zooming\",\n,Discussion: Resolving underspeciﬁed queries through ambi-\n\"and panning)\nthrough a pen or ﬁnger. Similarly, Valletto [103],\",\n,guity widgets mainly consists of two steps: detecting ambiguity\n\"InChorus [226], Data@Hand [115], DataBreeze [227], Power BI’s\",\n,\"and presenting widgets to the user. For the ﬁrst step, most systems\"\n\"Q&A [5], and Tableau’s Ask data [2] all support standard view\",\n,now still leverage heuristics for algorithmic resolution of ambiguity.\n\"transformations. Besides,\nthe software visualization community\",\n,\"A general probabilistic framework may handle a broader\nrange\"\nalso presents several closely related works. Bieliauskas et al. [20],\n,\"of ambiguities. For the second, richer interactive widgets can be\"\nproposed an interaction approach with software visualizations,\n,adopted to enhance the user experience.\n\"based on a conversational\ninterface. It can automatically display\",\n,\"8.2\nAutocompletion and Command Suggestion\"\n\"best ﬁtting views\naccording to meta\ninformation from natural\",\nlanguage sentences. Seipel et al. [201] explored natural language,Users may be unaware of what operations the system can perform\ninteraction for software visualization in Augmented Reality (AR),\"and whether\na\nspeciﬁc\nlanguage\nstructure\nis preferred in the\"\n\"with Microsoft HoloLens device, which can provide various view\",system. Although advanced text understanding models give users\n\"transformation interactions through gesture, gaze, and speech.\",\"the freedom to express\ntheir\nintents,\nsystem discoverability to\"\nDiscussion: View transformation has attracted less attention in,help formulate analytical questions is still an indispensable part\nthe community. Only a few V-NLI systems support NL control over,of V-NLI. Discoverability entails awareness: making users aware\n\"view transformations, and they are mainly limited to viewpoint\",\"of what operations the system can perform, and understanding:\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 10,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n10\nFig. 8. Autocompletion in Sneak pique [206], The user is prompted with\nwidgets that provide appropriate previews of the underlying data.\neducating users about how to phrase queries that can be interpreted\ncorrectly by the system [224]. Generally, current V-NLI systems\npay relatively little attention to system discoverability. Major\nattempts include autocompletion and command suggestion. This\ncharacteristic offers interpretable hints or suggestions matching the\nvisualizations and datasets, which is considered a fruitful interaction\nparadigm for information sense-making. When using V-NLI of\nTableau [2] or Power BI [5], the autocompleted content will be\npresented as we are typing, especially when there is a parsing\nerror. They are mostly reminders of commonly used queries, such\nas data attributes and aggregation functions. Suggestions will not\nintrusively appear when the user has formulated a valid query.\nSimilarly, Bacci et al. [10] and Yu et al. [273] both adopted\ntemplate-based approaches to support autocompletion in their V-\nNLIs. In addition to text prompts, data previews can be more useful\nacross all autocompletion variants. Deeper into this area, three\ncrowdsourcing studies conducted by Setlur et al. [206] indicated\nthat users prefer widgets for previewing numerical, geospatial,\nand temporal data while textual autocompletion for hierarchical\nand categorical data. On this basis, they built a design probe,\nSneak Pique. As shown in Figure 8, when analyzing a dataset of\ncoronavirus cases, the user is prompted with widgets that provide\nappropriate previews of the underlying data in various types. The\nsystem also supports toggling from a widget to a corresponding\ntext autocompletion dropdown. Most recently, Srinivasan et al.\n[229] proposed Snowy, a prototype system that generates and\nrecommends utterance recommendations for conversational visual\nanalysis by suggesting data features while implicitly making users\naware of which input the NLI supports.\nDiscussion: Current utterance realization approaches are all\ntemplate-based, which just works effectively for a small set of\ntasks and can hardly apply to large-scale systems. Going forward,\nanother important consideration that needs further research is\nwhich commands to show and when and how the suggestions will\nbe presented. In addition, existing technologies can only handle\nkeyboard input. There remains an open area for the discoverability\nof speech-based V-NLI. The tone of voice can also provide insights\ninto the user’s sentiments [147].\n8.3\nMultimodal Interaction\nVan Dam [244] envisioned post-WIMP user interfaces as “one con-\ntaining at least one interaction technique not dependent on classical\n2D widgets such as menus and icons.” With the advancements in\nhardware and software technologies that can be used to support\nnovel interaction modalities, researchers are empowered to take a\nstep closer to post-WIMP user interfaces, enabling users to focus\nmore on their tasks [128]. A qualitative user study conducted by\nSaktheeswaran et al. [195] also found that participants strongly\nprefer multimodal input over unimodal input. In recent years,\nFig. 9. Multimodal interaction interface in InChorus [226], which allows\npen, touch, and speech input on tablet devices.\nmany works have examined how multiple input forms (e.g., mouse,\npen, touch, keyboard, and speech) can be combined to provide a\nmore natural and engaging user experience. For instance, Orko\n[231] is a prototype visualization system that combines both\nnatural language interface and direct manipulation to assist visual\nexploration and analysis of graph data. Valletto [103] allows users\nto specify visualizations through a speech-based conversational\ninterface, multitouch gestures, and a conventional GUI interface.\nInChorus [226] is designed to maintain interaction consistency\nacross different visualizations, and it supports pen, touch, and\nspeech input on tablet devices. Its interface components are shown\nin Figure 9, including typical WIMP components (A, B, C), speech\ncommand display area (D), and content supporting pen and touch\n(E, F, G). DataBreeze [227] couplings pen, touch, and speech-\nbased multimodal interactions with ﬂexible unit visualizations,\nenabling a novel data exploration experience. RIA [284] is an\nintelligent multimodal conversation system to aid users in exploring\nlarge and complex datasets, which is powered by an optimization-\nbased approach to visual context management. Data@Hand [115]\nleverages the synergy of speech and touch input modalities for\npersonal data exploration on the mobile phone. Srinivasan et\nal. [224] proposed to leverage multimodal input to enhance\ndiscoverability and the human-computer interaction experience.\nDiscussion: V-NLI is an essential part in the context of\npost-WIMP interaction with visualization systems. Typing and\nspeech are both commonly used modalities to input NL queries.\nHowever, speech has unique challenges from a system design\naspect, such as triggering speech input, lack of assistive features like\nautocompletion, and transcription errors [128], which deserve more\nattention in the future. Besides, limited research has incorporated\nhuman gesture recognition and tracking technology [94] to facilitate\nvisualization creation, especially with large displays.\n9\nDIALOGUE MANAGEMENT\nThis section will discuss dialogue management in V-NLI. Given the\nconversational nature of NLIs, users may frequently pose queries\ndepending on their prior queries. By iterating upon their questions,\nthe user can dive deep into their aspects of interest on a chart and\nreﬁne existing visualizations.\n9.1\nAnalytical Conversations\nConversional NLIs have been widely applied in many ﬁelds to\nprompt users to open-ended requests (e.g., recommender systems\n[102] and intelligent assistants [219]). With conversational agents,\nusers can also reinforce their understanding of how the system\nparses their queries. In the visualization community, Eviza [205]\nand Evizeon [83] are two representative visualization systems that\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 10\nFig.8.AutocompletioninSneakpique[206],Theuserispromptedwith\nwidgetsthatprovideappropriatepreviewsoftheunderlyingdata.\neducatingusersabouthowtophrasequeriesthatcanbeinterpreted Fig.9.MultimodalinteractioninterfaceinInChorus[226],whichallows\npen,touch,andspeechinputontabletdevices.\ncorrectlybythesystem[224].Generally,currentV-NLIsystems\npay relatively little attention to system discoverability. Major manyworkshaveexaminedhowmultipleinputforms(e.g.,mouse,\nattemptsincludeautocompletionandcommandsuggestion.This pen,touch,keyboard,andspeech)canbecombinedtoprovidea\ncharacteristicoffersinterpretablehintsorsuggestionsmatchingthe more natural and engaging user experience. For instance, Orko\nvisualizationsanddatasets,whichisconsideredafruitfulinteraction [231] is a prototype visualization system that combines both\nparadigm for information sense-making. When using V-NLI of naturallanguageinterfaceanddirectmanipulationtoassistvisual\nTableau [2] or Power BI [5], the autocompleted content will be explorationandanalysisofgraphdata.Valletto[103]allowsusers\npresented as we are typing, especially when there is a parsing to specify visualizations through a speech-based conversational\nerror.Theyaremostlyremindersofcommonlyusedqueries,such interface,multitouchgestures,andaconventionalGUIinterface.\nasdataattributesandaggregationfunctions.Suggestionswillnot InChorus [226] is designed to maintain interaction consistency\nintrusively appear when the user has formulated a valid query. across different visualizations, and it supports pen, touch, and\nSimilarly, Bacci et al. [10] and Yu et al. [273] both adopted speechinputontabletdevices.Itsinterfacecomponentsareshown\ntemplate-basedapproachestosupportautocompletionintheirV- inFigure9,includingtypicalWIMPcomponents(A,B,C),speech\nNLIs.Inadditiontotextprompts,datapreviewscanbemoreuseful commanddisplayarea(D),andcontentsupportingpenandtouch\nacross all autocompletion variants. Deeper into this area, three (E, F, G). DataBreeze [227] couplings pen, touch, and speech-\ncrowdsourcingstudiesconductedbySetluretal.[206]indicated based multimodal interactions with flexible unit visualizations,\nthat users prefer widgets for previewing numerical, geospatial, enabling a novel data exploration experience. RIA [284] is an\nand temporal data while textual autocompletion for hierarchical intelligentmultimodalconversationsystemtoaidusersinexploring\nand categorical data. On this basis, they built a design probe, largeandcomplexdatasets,whichispoweredbyanoptimization-\nSneakPique.AsshowninFigure8,whenanalyzingadatasetof basedapproachtovisualcontextmanagement.Data@Hand[115]\ncoronaviruscases,theuserispromptedwithwidgetsthatprovide leverages the synergy of speech and touch input modalities for\nappropriatepreviewsoftheunderlyingdatainvarioustypes.The personal data exploration on the mobile phone. Srinivasan et\nsystem also supports toggling from a widget to a corresponding al. [224] proposed to leverage multimodal input to enhance\ntext autocompletion dropdown. Most recently, Srinivasan et al. discoverabilityandthehuman-computerinteractionexperience.\n[229] proposed Snowy, a prototype system that generates and Discussion: V-NLI is an essential part in the context of\nrecommendsutterancerecommendationsforconversationalvisual post-WIMP interaction with visualization systems. Typing and\nanalysisbysuggestingdatafeatureswhileimplicitlymakingusers speech are both commonly used modalities to input NL queries.\nawareofwhichinputtheNLIsupports. However, speech has unique challenges from a system design\nDiscussion: Current utterance realization approaches are all aspect,suchastriggeringspeechinput,lackofassistivefeatureslike\ntemplate-based, which just works effectively for a small set of autocompletion,andtranscriptionerrors[128],whichdeservemore\ntasksandcanhardlyapplytolarge-scalesystems.Goingforward, attentioninthefuture.Besides,limitedresearchhasincorporated\nanother important consideration that needs further research is humangesturerecognitionandtrackingtechnology[94]tofacilitate\nwhichcommandstoshowandwhenandhowthesuggestionswill visualizationcreation,especiallywithlargedisplays.\nbe presented. In addition, existing technologies can only handle\nkeyboardinput.Thereremainsanopenareaforthediscoverability 9 DIALOGUE MANAGEMENT\nofspeech-basedV-NLI.Thetoneofvoicecanalsoprovideinsights ThissectionwilldiscussdialoguemanagementinV-NLI.Giventhe\nintotheuser’ssentiments[147]. conversationalnatureofNLIs,usersmayfrequentlyposequeries\ndependingontheirpriorqueries.Byiteratingupontheirquestions,\n8.3 MultimodalInteraction\ntheusercandivedeepintotheiraspectsofinterestonachartand\nVanDam[244]envisionedpost-WIMPuserinterfacesas“onecon- refineexistingvisualizations.\ntainingatleastoneinteractiontechniquenotdependentonclassical\n9.1 AnalyticalConversations\n2Dwidgetssuchasmenusandicons.”Withtheadvancementsin\nhardware and software technologies that can be used to support Conversional NLIs have been widely applied in many fields to\nnovelinteractionmodalities,researchersareempoweredtotakea promptuserstoopen-endedrequests(e.g.,recommendersystems\nstepclosertopost-WIMPuserinterfaces,enablinguserstofocus [102]andintelligentassistants[219]).Withconversationalagents,\nmoreontheirtasks[128].Aqualitativeuserstudyconductedby users can also reinforce their understanding of how the system\nSaktheeswaran et al. [195] also found that participants strongly parsestheirqueries.Inthevisualizationcommunity,Eviza[205]\nprefer multimodal input over unimodal input. In recent years, andEvizeon[83]aretworepresentativevisualizationsystemsthat",
    "tables_extraction_text_csv": "0,1\n\"Fig. 8. Autocompletion in Sneak pique [206], The user is prompted with\",\n\"widgets that provide appropriate previews of\nthe underlying data.\",\n,\"Fig. 9. Multimodal\ninteraction interface in InChorus [226], which allows\"\neducating users about how to phrase queries that can be interpreted,\n,\"pen,\ntouch, and speech input on tablet devices.\"\n\"correctly by the system [224]. Generally, current V-NLI systems\",\n\"pay relatively little\nattention to system discoverability. Major\",\"many works have examined how multiple input forms (e.g., mouse,\"\nattempts include autocompletion and command suggestion. This,\"pen,\ntouch, keyboard, and speech) can be combined to provide a\"\ncharacteristic offers interpretable hints or suggestions matching the,\"more natural and engaging user experience. For\ninstance, Orko\"\n\"visualizations and datasets, which is considered a fruitful interaction\",\"[231]\nis\na prototype visualization system that\ncombines both\"\n\"paradigm for\ninformation sense-making. When using V-NLI of\",\"natural\nlanguage interface and direct manipulation to assist visual\"\n\"Tableau [2] or Power BI\n[5],\nthe autocompleted content will be\",exploration and analysis of graph data. Valletto [103] allows users\n\"presented as we are typing, especially when there is a parsing\",to specify visualizations through a speech-based conversational\n\"error. They are mostly reminders of commonly used queries, such\",\"interface, multitouch gestures, and a conventional GUI interface.\"\nas data attributes and aggregation functions. Suggestions will not,\"InChorus\n[226]\nis designed to maintain interaction consistency\"\n\"intrusively appear when the user has\nformulated a valid query.\",\"across different visualizations, and it\nsupports pen,\ntouch, and\"\n\"et al.\nSimilarly, Bacci\n[10]\nand Yu et al.\n[273] both adopted\",speech input on tablet devices. Its interface components are shown\ntemplate-based approaches to support autocompletion in their V-,\"in Figure 9, including typical WIMP components (A, B, C), speech\"\n\"NLIs. In addition to text prompts, data previews can be more useful\",\"command display area (D), and content supporting pen and touch\"\n\"across all autocompletion variants. Deeper\ninto this area,\nthree\",\"(E, F, G). DataBreeze [227] couplings pen,\ntouch, and speech-\"\ncrowdsourcing studies conducted by Setlur et al. [206] indicated,\"based multimodal\ninteractions with ﬂexible unit visualizations,\"\n\"that users prefer widgets\nfor previewing numerical, geospatial,\",\"enabling a novel data exploration experience. RIA [284]\nis an\"\nand temporal data while textual autocompletion for hierarchical,intelligent multimodal conversation system to aid users in exploring\n\"and categorical data. On this basis,\nthey built a design probe,\",\"large and complex datasets, which is powered by an optimization-\"\n\"Sneak Pique. As shown in Figure 8, when analyzing a dataset of\",based approach to visual context management. Data@Hand [115]\n\"coronavirus cases,\nthe user is prompted with widgets that provide\",leverages the synergy of speech and touch input modalities for\nappropriate previews of the underlying data in various types. The,\"personal data\nexploration on the mobile phone. Srinivasan et\"\n\"system also supports toggling from a widget\nto a corresponding\",\"al.\n[224]\nproposed\nto\nleverage multimodal\ninput\nto\nenhance\"\n\"text autocompletion dropdown. Most\nrecently, Srinivasan et al.\",discoverability and the human-computer interaction experience.\n\"[229] proposed Snowy, a prototype system that generates and\",\"Discussion: V-NLI\nis\nan\nessential\npart\nin\nthe\ncontext\nof\"\nrecommends utterance recommendations for conversational visual,post-WIMP interaction with visualization systems. Typing and\nanalysis by suggesting data features while implicitly making users,speech are both commonly used modalities to input NL queries.\n\"aware of which input\nthe NLI supports.\",\"However,\nspeech has unique challenges\nfrom a system design\"\nDiscussion: Current utterance realization approaches are all,\"aspect, such as triggering speech input, lack of assistive features like\"\n\"template-based, which just works effectively for a small set of\",\"autocompletion, and transcription errors [128], which deserve more\"\n\"tasks and can hardly apply to large-scale systems. Going forward,\",\"attention in the future. Besides, limited research has incorporated\"\n\"another\nimportant\nconsideration that needs\nfurther\nresearch is\",human gesture recognition and tracking technology [94] to facilitate\nwhich commands to show and when and how the suggestions will,\"visualization creation, especially with large displays.\"\n\"be presented. In addition, existing technologies can only handle\",\nkeyboard input. There remains an open area for the discoverability,\"9\nDIALOGUE MANAGEMENT\"\nof speech-based V-NLI. The tone of voice can also provide insights,This section will discuss dialogue management in V-NLI. Given the\ninto the user’s sentiments [147].,\"conversational nature of NLIs, users may frequently pose queries\"\n,\"depending on their prior queries. By iterating upon their questions,\"\n\"8.3\nMultimodal\nInteraction\",\n,the user can dive deep into their aspects of interest on a chart and\nVan Dam [244] envisioned post-WIMP user interfaces as “one con-,reﬁne existing visualizations.\ntaining at least one interaction technique not dependent on classical,\n,\"9.1\nAnalytical Conversations\"\n2D widgets such as menus and icons.” With the advancements in,\nhardware and software technologies that can be used to support,Conversional NLIs have been widely applied in many ﬁelds to\n\"novel\ninteraction modalities, researchers are empowered to take a\",\"prompt users to open-ended requests (e.g., recommender systems\"\n\"step closer to post-WIMP user interfaces, enabling users to focus\",\"[102] and intelligent assistants [219]). With conversational agents,\"\nmore on their tasks [128]. A qualitative user study conducted by,users can also reinforce their understanding of how the system\n\"Saktheeswaran et al.\n[195] also found that participants strongly\",\"parses their queries. In the visualization community, Eviza [205]\"\n\"prefer multimodal\ninput over unimodal\ninput.\nIn recent years,\",and Evizeon [83] are two representative visualization systems that\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 11,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n11\nFig. 10. Conversational V-NLI in Evizeon [83]. The user has a back-and-\nforth information exchange with the system.\nprovide V-NLI for visual analysis cycles. Figure 10 shows a back-\nand-forth information exchange process between Evizeon [83] and\nthe user when analyzing the measles dataset. The system supports\nvarious forms of NL interactions with a dashboard applying\npragmatic principles. For example, when the user ﬁrst types\n“measles in the UK”, all the charts presented are ﬁltered to cases\nof measles in the UK (Figure 10(a)). Then, the user types “show\nme the orange spike”, and Evizeon will add details to the spike in\nthe orange line as it understands that this is a reference to visual\nproperties of the line chart (Figure 10(b)). Similarly, Articulate2\n[121] is intended to automatically transform the user queries into\nvisualizations using a full-ﬂedged conversational interface. Analyza\n[50] combines two-way conversation with a structured interface to\nenable effective data exploration. Besides, Fast et al. [58] proposed\na new conversational agent, Iris, which can generate visualizations\nfrom and combine the previous commands, even non-visualization\nrelated. Ava [134] uses controlled NL queries to program data\nscience workﬂow.\nConversational transitions model, which describes how to\ntransition visualization states during the analytical conversation,\nis an essential part to enable the aforementioned interactive\nconversations. In the early years, Grosz et al. [71] explored how the\ncontext of a conversation adjusts over time to maintain coherence\nthrough transitional states (retain, shift, continue, and reset). On this\nbasis, Tory and Setlur [242] introduced a conversational transitions\nmodel (see Figure 11) that emerged from their analysis in the\ndesign of Tableau’s V-NLI feature, Ask data. After interpreting a\ngenerated visualization, a user may formulate a new NL query to\ncontinue the analytical conversation. The user’s transitional goal\nmeans how the user wishes to transform the existing visualization\nto answer a new question. The model contains the following\ntransitional goals: elaborate, adjust/pivot, start new, retry, and undo,\nwhich in turn drive user actions. The visualization states (select\nattributes, transform, ﬁlter, and encode) are ﬁnally updated, and\nnew visualizations are presented to the user. An essential insight\nduring the analysis of Tory and Setlur was that applying transitions\nto ﬁlters alone (like Evizeon [83] and Orko [231]) is insufﬁcient.\nThe user’s intent around transitions may apply to any aspects of a\nvisualization state.\nDiscussion: Some V-NLI systems leverage a VisRec engine to\ngenerate visualizations. However, it is insufﬁcient for analytical\nconversation. A more intelligent system should infer the user’s\ntransitional goals based on their interactions and then respond to\neach visualization state accordingly. Besides, empirical studies [77]\nFig. 11. Conversational transitions model [242] that describes how to\ntransition a visualization state during an analytical conversation.\nTABLE 7\nExamples of co-reference types\nType\nExample\nResolution\nPronoun-\nNoun\nShow me the most popular movie in New York and\nits rating.\nits →movie’s\nNoun-\nNoun\nHow many Wal-Mart stores in Seattle and how’s\nthe annual proﬁts for the malls?\nmalls →Wal-Mart\nPronoun-\nPronoun\nThe manchurian tigers have been heavily hunted,\nwhich caused a dramatic drop of their existing\nnumber and they may ﬁnally get extinct.\nthey, their →\nmanchurian tigers\nPronoun-\nPronoun\nWe are all the Canadians while Lily was born in the\nU.S. and she immigrated to Canada two years ago.\nWe →Canadians,\nshe →Lily\nhave shown that users tend to prefer additional content beyond\nthe exact answers when asking questions in such conversational\ninterfaces, which had signiﬁcant implications for designing insight-\ndriven conversation interfaces in the future.\n9.2\nCo-reference Resolution\nCo-reference Resolution (CR) aims to ﬁnd linguistic expressions\n(called mentions) in a given text that refers to the same entity\n[236]. CR is an important sub-task in dialogue management as\nthe user usually uses pronouns to refer to certain visual elements.\nA typical scenario is to identify the entity to which a pronoun in\nthe text refers. The task also involves recognizing co-reference\nrelations between multiple noun phrases [181]. Table 7 shows\nexamples of co-reference types. Before the booming development\nof deep learning-based language models, human-designed rules,\nknowledge, and features dominated CR tasks [280]. Nowadays,\nmost state-of-the-art CR models are neural networks and employ an\nend-to-end fashion, where the pipeline includes encoding context,\ngenerating representations for all potential mentions, and predicting\nco-reference relations [131] [96]. Beneﬁting from the powerful\nunderstanding and predicting ability of language models like BERT\n[49] and GPT-3 [25], reliance on annotated span mentions is\nbecoming weaker [117]. A CR task can even be treated as a token\nprediction task [118]. Several existing models exist to deal with\nthis situation. Besides, the usage of structured knowledge bases\n[279] [145] and higher-order information [132] has been proven to\nbe beneﬁcial to the overall performance of CR tasks.\nCo-reference resolution is also an essential task in the multi-\nmodal sense as users may even pose queries that are the follow-\nup to their direct manipulations on the interface. When the user\nfaces a graphical representation, each element in the representation\ncan become a referent. The system should have a declarative\nrepresentation for all these potential referents and ﬁnd the best\nmatch to make multimodal interaction more smooth. Articulate2\n[120] addresses this issue by leveraging Kinect to detect deictic\ngestures on the virtual touch screen in front of a large display.\nIf referring expressions are detected, and Kinect has detected a\ngesture, information about any objects pointed to by the user will be\nstored. The system then can ﬁnd the best match between properties\nof each relevant entity. The properties of visualizations and objects\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 11\nFig.11.Conversationaltransitionsmodel[242]thatdescribeshowto\ntransitionavisualizationstateduringananalyticalconversation.\nTABLE7\nExamplesofco-referencetypes\nType Example Resolution\nPronoun- ShowmethemostpopularmovieinNewYorkand\nits→movie’s\nNoun itsrating.\nFig.10.ConversationalV-NLIinEvizeon[83].Theuserhasaback-and- Noun- HowmanyWal-MartstoresinSeattleandhow’s malls→Wal-Mart\nforthinformationexchangewiththesystem. Noun theannualprofitsforthemalls?\nThemanchurian tigershavebeenheavilyhunted,\nprovideV-NLIforvisualanalysiscycles.Figure10showsaback- P P r r o o n n o o u u n n - w nu h m ic b h er c a a n u d se t d he a ym dr a a y m fi a n ti a c lly dr g o e p te o x f tin t c h t e . ir existing t m he a y n , ch th u e r i i r an → tigers\nand-forthinformationexchangeprocessbetweenEvizeon[83]and\nPronoun- WearealltheCanadianswhileLilywasborninthe We→Canadians,\ntheuserwhenanalyzingthemeaslesdataset.Thesystemsupports Pronoun U.S.andsheimmigratedtoCanadatwoyearsago. she→Lily\nvarious forms of NL interactions with a dashboard applying\npragmatic principles. For example, when the user first types have shown that users tend to prefer additional content beyond\n“measlesintheUK”,allthechartspresentedarefilteredtocases the exact answers when asking questions in such conversational\nofmeaslesintheUK(Figure10(a)).Then,theusertypes“show interfaces,whichhadsignificantimplicationsfordesigninginsight-\nmetheorangespike”,andEvizeonwilladddetailstothespikein drivenconversationinterfacesinthefuture.\ntheorangelineasitunderstandsthatthisisareferencetovisual\npropertiesofthelinechart(Figure10(b)).Similarly,Articulate2 9.2 Co-referenceResolution\n[121]isintendedtoautomaticallytransformtheuserqueriesinto Co-referenceResolution(CR)aimstofindlinguisticexpressions\nvisualizationsusingafull-fledgedconversationalinterface.Analyza (called mentions) in a given text that refers to the same entity\n[50]combinestwo-wayconversationwithastructuredinterfaceto [236]. CR is an important sub-task in dialogue management as\nenableeffectivedataexploration.Besides,Fastetal.[58]proposed theuserusuallyusespronounstorefertocertainvisualelements.\nanewconversationalagent,Iris,whichcangeneratevisualizations Atypicalscenarioistoidentifytheentitytowhichapronounin\nfromandcombinethepreviouscommands,evennon-visualization the text refers. The task also involves recognizing co-reference\nrelated. Ava [134] uses controlled NL queries to program data relations between multiple noun phrases [181]. Table 7 shows\nscienceworkflow. examplesofco-referencetypes.Beforetheboomingdevelopment\nConversational transitions model, which describes how to of deep learning-based language models, human-designed rules,\ntransition visualization states during the analytical conversation, knowledge, and features dominated CR tasks [280]. Nowadays,\nis an essential part to enable the aforementioned interactive moststate-of-the-artCRmodelsareneuralnetworksandemployan\nconversations.Intheearlyyears,Groszetal.[71]exploredhowthe end-to-endfashion,wherethepipelineincludesencodingcontext,\ncontextofaconversationadjustsovertimetomaintaincoherence generatingrepresentationsforallpotentialmentions,andpredicting\nthroughtransitionalstates(retain,shift,continue,andreset).Onthis co-reference relations [131] [96]. Benefiting from the powerful\nbasis,ToryandSetlur[242]introducedaconversationaltransitions understandingandpredictingabilityoflanguagemodelslikeBERT\nmodel (see Figure 11) that emerged from their analysis in the [49] and GPT-3 [25], reliance on annotated span mentions is\ndesignofTableau’sV-NLIfeature,Askdata.Afterinterpretinga becomingweaker[117].ACRtaskcanevenbetreatedasatoken\ngeneratedvisualization,ausermayformulateanewNLqueryto prediction task [118]. Several existing models exist to deal with\ncontinuetheanalyticalconversation.Theuser’stransitionalgoal this situation. Besides, the usage of structured knowledge bases\nmeanshowtheuserwishestotransformtheexistingvisualization [279][145]andhigher-orderinformation[132]hasbeenprovento\nto answer a new question. The model contains the following bebeneficialtotheoverallperformanceofCRtasks.\ntransitionalgoals:elaborate,adjust/pivot,startnew,retry,andundo, Co-referenceresolutionisalsoanessentialtaskinthemulti-\nwhich in turn drive user actions. The visualization states (select modal sense as users may even pose queries that are the follow-\nattributes, transform, filter, and encode) are finally updated, and up to their direct manipulations on the interface. When the user\nnewvisualizationsarepresentedtotheuser.Anessentialinsight facesagraphicalrepresentation,eachelementintherepresentation\nduringtheanalysisofToryandSetlurwasthatapplyingtransitions can become a referent. The system should have a declarative\ntofiltersalone(likeEvizeon[83]andOrko[231])isinsufficient. representation for all these potential referents and find the best\nTheuser’sintentaroundtransitionsmayapplytoanyaspectsofa matchtomakemultimodalinteractionmoresmooth.Articulate2\nvisualizationstate. [120] addresses this issue by leveraging Kinect to detect deictic\nDiscussion:SomeV-NLIsystemsleverageaVisRecengineto gestures on the virtual touch screen in front of a large display.\ngenerate visualizations. However, it is insufficient for analytical If referring expressions are detected, and Kinect has detected a\nconversation. A more intelligent system should infer the user’s gesture,informationaboutanyobjectspointedtobytheuserwillbe\ntransitionalgoalsbasedontheirinteractionsandthenrespondto stored.Thesystemthencanfindthebestmatchbetweenproperties\neachvisualizationstateaccordingly.Besides,empiricalstudies[77] ofeachrelevantentity.Thepropertiesofvisualizationsandobjects",
    "tables_extraction_text_csv": "0,1\n,\"Noun\nits rating.\"\n,\"Noun-\nHow many Wal-Mart\nstores in Seattle and how’s\"\n\"Fig. 10. Conversational V-NLI\nin Evizeon [83]. The user has a back-and-\",malls → Wal-Mart\n,\"Noun\nthe annual proﬁts for the malls?\"\nforth information exchange with the system.,\n,\"The manchurian tigers have been heavily hunted,\"\n,\"Pronoun-\nthey,\ntheir →\"\n,\"their\nwhich\ncaused\na\ndramatic\ndrop\nof\nexisting\"\nprovide V-NLI for visual analysis cycles. Figure 10 shows a back-,Pronoun\n,\"manchurian tigers\nnumber and they may ﬁnally get extinct.\"\nand-forth information exchange process between Evizeon [83] and,\"Pronoun-\nWe are all the Canadians while Lily was born in the\"\n,\"We → Canadians,\"\nthe user when analyzing the measles dataset. The system supports,\"Pronoun\nU.S. and she immigrated to Canada two years ago.\"\n,she → Lily\n\"various\nforms\nof NL interactions with\na\ndashboard\napplying\",\n\"pragmatic\nprinciples. For\nexample, when\nthe\nuser ﬁrst\ntypes\",have shown that users tend to prefer additional content beyond\n\"“measles in the UK”, all the charts presented are ﬁltered to cases\",the exact answers when asking questions in such conversational\n\"of measles in the UK (Figure 10(a)). Then,\nthe user types “show\",\"interfaces, which had signiﬁcant\nimplications for designing insight-\"\n\"me the orange spike”, and Evizeon will add details to the spike in\",driven conversation interfaces in the future.\n\"the orange line as it understands that\nthis is a reference to visual\",\n\"properties of the line chart (Figure 10(b)). Similarly, Articulate2\",\"9.2\nCo-reference Resolution\"\n[121] is intended to automatically transform the user queries into,\n,Co-reference Resolution (CR) aims to ﬁnd linguistic expressions\nvisualizations using a full-ﬂedged conversational interface. Analyza,\n,\"(called mentions)\nin a given text\nthat\nrefers\nto the same entity\"\n[50] combines two-way conversation with a structured interface to,\n,[236]. CR is an important sub-task in dialogue management as\n\"enable effective data exploration. Besides, Fast et al. [58] proposed\",\n,the user usually uses pronouns to refer to certain visual elements.\n\"a new conversational agent, Iris, which can generate visualizations\",\n,A typical scenario is to identify the entity to which a pronoun in\n\"from and combine the previous commands, even non-visualization\",\n,\"the text\nrefers. The task also involves recognizing co-reference\"\nrelated. Ava [134] uses controlled NL queries to program data,\n,\"relations between multiple noun phrases\n[181]. Table 7 shows\"\nscience workﬂow.,\n,examples of co-reference types. Before the booming development\n\"Conversational\ntransitions model, which describes how to\",\"of deep learning-based language models, human-designed rules,\"\n\"transition visualization states during the analytical conversation,\",\"knowledge, and features dominated CR tasks [280]. Nowadays,\"\n\"is\nan\nessential\npart\nto\nenable\nthe\naforementioned\ninteractive\",most state-of-the-art CR models are neural networks and employ an\n\"conversations. In the early years, Grosz et al. [71] explored how the\",\"end-to-end fashion, where the pipeline includes encoding context,\"\ncontext of a conversation adjusts over time to maintain coherence,\"generating representations for all potential mentions, and predicting\"\n\"through transitional states (retain, shift, continue, and reset). On this\",\"co-reference relations [131]\n[96]. Beneﬁting from the powerful\"\n\"basis, Tory and Setlur [242] introduced a conversational\ntransitions\",understanding and predicting ability of language models like BERT\n\"model\n(see Figure 11)\nthat emerged from their analysis\nin the\",\"[49]\nand GPT-3 [25],\nreliance on annotated span mentions\nis\"\n\"design of Tableau’s V-NLI feature, Ask data. After interpreting a\",becoming weaker [117]. A CR task can even be treated as a token\n\"generated visualization, a user may formulate a new NL query to\",\"prediction task [118]. Several existing models exist\nto deal with\"\ncontinue the analytical conversation. The user’s transitional goal,\"this situation. Besides,\nthe usage of structured knowledge bases\"\nmeans how the user wishes to transform the existing visualization,[279] [145] and higher-order information [132] has been proven to\n\"to answer\na new question. The model\ncontains\nthe\nfollowing\",\"be beneﬁcial\nto the overall performance of CR tasks.\"\n\"transitional goals: elaborate, adjust/pivot, start new, retry, and undo,\",\"Co-reference resolution is also an essential\ntask in the multi-\"\nwhich in turn drive user actions. The visualization states (select,modal sense as users may even pose queries that are the follow-\n\"attributes,\ntransform, ﬁlter, and encode) are ﬁnally updated, and\",up to their direct manipulations on the interface. When the user\n\"new visualizations are presented to the user. An essential\ninsight\",\"faces a graphical representation, each element\nin the representation\"\nduring the analysis of Tory and Setlur was that applying transitions,\"can become\na\nreferent. The\nsystem should have\na declarative\"\nto ﬁlters alone (like Evizeon [83] and Orko [231]) is insufﬁcient.,\"representation for all\nthese potential\nreferents and ﬁnd the best\"\nThe user’s intent around transitions may apply to any aspects of a,\"match to make multimodal\ninteraction more smooth. Articulate2\"\nvisualization state.,\"[120] addresses this issue by leveraging Kinect\nto detect deictic\"\nDiscussion: Some V-NLI systems leverage a VisRec engine to,\"gestures on the virtual\ntouch screen in front of a large display.\"\n\"generate visualizations. However,\nit\nis insufﬁcient for analytical\",\"If\nreferring expressions are detected, and Kinect has detected a\"\n\"conversation. A more intelligent system should infer\nthe user’s\",\"gesture, information about any objects pointed to by the user will be\"\ntransitional goals based on their interactions and then respond to,stored. The system then can ﬁnd the best match between properties\n\"each visualization state accordingly. Besides, empirical studies [77]\",of each relevant entity. The properties of visualizations and objects\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 12,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n12\nFig. 12. Visualizations with annotations generated by Vis-Annotator [122].\nkeeping track of include statistics and trends in the data, title, mark,\nand any more prominent objects within the visualization (e.g., hot-\nspots, street names, and bus stops). In multimodal systems [103],\n[115], [195], [224], [226], [227], [231], users can focus on speciﬁc\nvisual elements in the visualization by selecting with mouse, pen,\nor ﬁnger. The follow-up queries can automatically link to related\nvisual elements or data.\nDiscussion: At the natural language level, existing V-NLIs\nmostly leverage NLP toolkits to perform co-reference resolution.\nAlthough useful, they lack detailed modeling of visualization\nelements, as discussed in Section 4.1. At the multimodal level,\nin addition to commonly used modalities (e.g., mouse, pen, and\nﬁnger), it may enable a better user experience by integrating more\nmodalities like eye-gaze and gesture.\n10\nPRESENTATION\nIn most cases, V-NLI accepts natural language as input and\noutputs well-designed visualizations. As shown in Table 3 (column\nVisualization Type), the output is not limited to traditional charts\nbut also involves other richer forms (e.g., maps, networks, and\ninfographics). In addition to visual presentation, an emerging theme\nis to complement visualizations with natural language. The previous\nsections have discussed natural language as an input modality. This\nsection will introduce using natural language as an output modality.\n10.1\nAnnotation\nAnnotation plays a vital role in explaining and emphasizing\nkey points in the dataset. The systems should generate valuable\nnatural language statements and map the text to a visualization\nappropriately. Annotation tools were ﬁrst applied to complement\nnews articles. Kandogan [101] introduced the concept of just-\nin-time descriptive analytics that helps users easily understand\nthe structure of data. Given a piece of news, Contextiﬁer [89]\nﬁrst computes clusters, outliers, and trends in line charts and\nthen automatically produces annotated stock visualizations based\non the information. Although it offers a promising example of\nhow human-produced news visualizations can be complemented\nin a speciﬁc context, it is only suitable for stock sequence data.\nNewsViews [64] later extends Contextiﬁer to support more types\nof appropriate data such as time series and georeferenced data.\nIn addition, several works incorporate the visual elements in the\nchart into annotations, synchronizing with textual descriptions. As\nshown in Figure 12, Vis-Annotator [122] leverages the Mask R-\nCNN model to identify visual elements in the target visualization,\nalong with their visual properties. Textual descriptions of the chart\nare synchronously interpreted to generate visual search requests.\nBased on the identiﬁed information, each descriptive sentence is\ndisplayed beside the described focal areas as annotations. Similarly,\nClick2Annotate [33] and Touch2Annotate [34] are both semi-\nautomatic annotations generators. To be interactive, Calliope [213]\nand ChartAccent [190] support creating stories via interactive\nFig. 13. Coupling from the text about NBA game report to stories [162].\nannotation generation and placement. Most recently, ADVISor\n[143] can generate visualizations with annotations to answer the\nuser’s NL questions on tabular data.\nDiscussion: Annotation can turn data into a story. However,\nthe scalability of current works is weak as most of them are\ntemplate-based. To improve the system usability, advanced NLG\nmodels [29] can be leveraged to assist generating appropriate\ntextual annotations. Extending existing systems to support richer\ndata types and annotation forms is also a meaningful direction.\nBesides, it would be helpful to devise an annotation speciﬁcation\nlanguage for additional ﬂexibility.\n10.2\nNarrative Storytelling\nNarrative storytelling gives data a voice and helps communicate\ninsights more effectively. A data story means a narrative to the\ndata that depicts actors and their interactions, along with times,\nlocations, and other entities [200]. Constructing various visual and\ngenerated textual (natural language) elements in visualizations is\nvital for narrative storytelling. To address this issue, the template\nis the most commonly applied method. For instance, Text-to-Viz\n[41] is designed to generate infographics from NL statements with\nproportion-related statistics. It builds 20 layout blueprints that\ndescribe the overall look of the resulting infographics. For each\nlayout blueprint, Text-to-Viz enumerates all extracted segments by\ntext analyzer and then generates all valid infographics. DataShot\n[254], TSIs [26], Chen et al. [36], and Retrieve-Then-Adapt [183]\nall use similar template-based approaches for narrative storytelling.\nWith the advances of deep learning, some works leverage generative\nadversarial networks (GAN) [69] to synthesize layouts [139],\n[286]. Those learning-based methods do not rely on handcrafted\nfeatures. However, due to limited training data, they often fail\nto achieve comparable performance with rule-based methods.\nLikewise, storytelling has been extensively used to visualize\nnarrative text produced by online writers and journalism media.\nMetoyer et al. [162] proposed a novel approach to generate data-\nrich stories. It ﬁrst extracts narrative components (who, what, when,\nwhere) from the text and then generates narrative visualizations by\nintegrating the supporting data evidence with the text. As shown\nin Figure 13, given a highlighted NBA game report sentence,\nthe system generates an initialized story dashboard based on\nthe interpretation information. Similarly, Story Analyzer [165]\nextracts subjects, actions, and objects from the narrative text to\nproduce interrelated and user-responsive story dashboards. RIA\n[284] dynamically derives a set of layout constraints (e.g., ensure\nvisual balance and avoid object occlusion) to tell spatial stories.\nDiscussion: Although the aforementioned systems are promis-\ning and work relatively well, they are restricted to one speciﬁc\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 12\nFig.12.VisualizationswithannotationsgeneratedbyVis-Annotator[122].\nkeepingtrackofincludestatisticsandtrendsinthedata,title,mark,\nandanymoreprominentobjectswithinthevisualization(e.g.,hot-\nspots,streetnames,andbusstops).Inmultimodalsystems[103],\nFig.13.CouplingfromthetextaboutNBAgamereporttostories[162].\n[115],[195],[224],[226],[227],[231],userscanfocusonspecific\nvisualelementsinthevisualizationbyselectingwithmouse,pen, annotation generation and placement. Most recently, ADVISor\norfinger.Thefollow-upqueriescanautomaticallylinktorelated [143]cangeneratevisualizationswithannotationstoanswerthe\nvisualelementsordata. user’sNLquestionsontabulardata.\nDiscussion: At the natural language level, existing V-NLIs Discussion: Annotation can turn data into a story. However,\nmostlyleverageNLPtoolkitstoperformco-referenceresolution. the scalability of current works is weak as most of them are\nAlthough useful, they lack detailed modeling of visualization template-based.Toimprovethesystemusability,advancedNLG\nelements, as discussed in Section 4.1. At the multimodal level, models [29] can be leveraged to assist generating appropriate\nin addition to commonly used modalities (e.g., mouse, pen, and textualannotations.Extendingexistingsystemstosupportricher\nfinger),itmayenableabetteruserexperiencebyintegratingmore data types and annotation forms is also a meaningful direction.\nmodalitieslikeeye-gazeandgesture. Besides,itwouldbehelpfultodeviseanannotationspecification\nlanguageforadditionalflexibility.\n10 PRESENTATION\n10.2 NarrativeStorytelling\nIn most cases, V-NLI accepts natural language as input and\noutputswell-designedvisualizations.AsshowninTable3(column Narrativestorytellinggivesdataavoiceandhelpscommunicate\nVisualizationType),theoutputisnotlimitedtotraditionalcharts insights more effectively. A data story means a narrative to the\nbut also involves other richer forms (e.g., maps, networks, and data that depicts actors and their interactions, along with times,\ninfographics).Inadditiontovisualpresentation,anemergingtheme locations,andotherentities[200].Constructingvariousvisualand\nistocomplementvisualizationswithnaturallanguage.Theprevious generatedtextual(naturallanguage)elementsinvisualizationsis\nsectionshavediscussednaturallanguageasaninputmodality.This vitalfornarrativestorytelling.Toaddressthisissue,thetemplate\nsectionwillintroduceusingnaturallanguageasanoutputmodality. isthemostcommonlyappliedmethod.Forinstance,Text-to-Viz\n[41]isdesignedtogenerateinfographicsfromNLstatementswith\n10.1 Annotation\nproportion-related statistics. It builds 20 layout blueprints that\nAnnotation plays a vital role in explaining and emphasizing describe the overall look of the resulting infographics. For each\nkey points in the dataset. The systems should generate valuable layoutblueprint,Text-to-Vizenumeratesallextractedsegmentsby\nnatural language statements and map the text to a visualization textanalyzerandthengeneratesallvalidinfographics.DataShot\nappropriately.Annotationtoolswerefirstappliedtocomplement [254],TSIs[26],Chenetal.[36],andRetrieve-Then-Adapt[183]\nnews articles. Kandogan [101] introduced the concept of just- allusesimilartemplate-basedapproachesfornarrativestorytelling.\nin-time descriptive analytics that helps users easily understand Withtheadvancesofdeeplearning,someworksleveragegenerative\nthe structure of data. Given a piece of news, Contextifier [89] adversarial networks (GAN) [69] to synthesize layouts [139],\nfirst computes clusters, outliers, and trends in line charts and [286].Thoselearning-basedmethodsdonotrelyonhandcrafted\nthenautomaticallyproducesannotatedstockvisualizationsbased features. However, due to limited training data, they often fail\non the information. Although it offers a promising example of to achieve comparable performance with rule-based methods.\nhowhuman-producednewsvisualizationscanbecomplemented Likewise, storytelling has been extensively used to visualize\nin a specific context, it is only suitable for stock sequence data. narrative text produced by online writers and journalism media.\nNewsViews[64]laterextendsContextifiertosupportmoretypes Metoyeretal.[162]proposedanovelapproachtogeneratedata-\nof appropriate data such as time series and georeferenced data. richstories.Itfirstextractsnarrativecomponents(who,what,when,\nInaddition,severalworksincorporatethevisualelementsinthe where)fromthetextandthengeneratesnarrativevisualizationsby\nchartintoannotations,synchronizingwithtextualdescriptions.As integratingthesupportingdataevidencewiththetext.Asshown\nshown in Figure 12, Vis-Annotator [122] leverages the Mask R- in Figure 13, given a highlighted NBA game report sentence,\nCNNmodeltoidentifyvisualelementsinthetargetvisualization, the system generates an initialized story dashboard based on\nalongwiththeirvisualproperties.Textualdescriptionsofthechart the interpretation information. Similarly, Story Analyzer [165]\naresynchronouslyinterpretedtogeneratevisualsearchrequests. extracts subjects, actions, and objects from the narrative text to\nBasedontheidentifiedinformation,eachdescriptivesentenceis produce interrelated and user-responsive story dashboards. RIA\ndisplayedbesidethedescribedfocalareasasannotations.Similarly, [284]dynamicallyderivesasetoflayoutconstraints(e.g.,ensure\nClick2Annotate [33] and Touch2Annotate [34] are both semi- visualbalanceandavoidobjectocclusion)totellspatialstories.\nautomaticannotationsgenerators.Tobeinteractive,Calliope[213] Discussion:Althoughtheaforementionedsystemsarepromis-\nand ChartAccent [190] support creating stories via interactive ing and work relatively well, they are restricted to one specific",
    "tables_extraction_text_csv": "0,1\nFig. 12. Visualizations with annotations generated by Vis-Annotator [122].,\n\"keeping track of include statistics and trends in the data, title, mark,\",\n\"and any more prominent objects within the visualization (e.g., hot-\",\n\"spots, street names, and bus stops). In multimodal systems [103],\",\n,\"Fig. 13. Coupling from the text about NBA game report\nto stories [162].\"\n\"[115], [195], [224], [226], [227], [231], users can focus on speciﬁc\",\n\"visual elements in the visualization by selecting with mouse, pen,\",\"annotation generation and placement. Most\nrecently, ADVISor\"\nor ﬁnger. The follow-up queries can automatically link to related,[143] can generate visualizations with annotations to answer the\nvisual elements or data.,user’s NL questions on tabular data.\n\"Discussion: At\nthe natural\nlanguage level, existing V-NLIs\",\"Discussion: Annotation can turn data into a story. However,\"\nmostly leverage NLP toolkits to perform co-reference resolution.,\"the\nscalability of\ncurrent works\nis weak as most of\nthem are\"\n\"Although useful,\nthey lack detailed modeling of visualization\",\"template-based. To improve the system usability, advanced NLG\"\n\"elements, as discussed in Section 4.1. At\nthe multimodal\nlevel,\",\"models\n[29] can be\nleveraged to assist generating appropriate\"\n\"in addition to commonly used modalities (e.g., mouse, pen, and\",textual annotations. Extending existing systems to support richer\n\"ﬁnger),\nit may enable a better user experience by integrating more\",data types and annotation forms is also a meaningful direction.\nmodalities like eye-gaze and gesture.,\"Besides,\nit would be helpful to devise an annotation speciﬁcation\"\n,language for additional ﬂexibility.\n\"10\nPRESENTATION\",\n,\"10.2\nNarrative Storytelling\"\n\"In most\ncases, V-NLI\naccepts\nnatural\nlanguage\nas\ninput\nand\",\n,Narrative storytelling gives data a voice and helps communicate\noutputs well-designed visualizations. As shown in Table 3 (column,\n,insights more effectively. A data story means a narrative to the\n\"Visualization Type),\nthe output\nis not\nlimited to traditional charts\",\n,\"data that depicts actors and their\ninteractions, along with times,\"\n\"but also involves other\nricher\nforms (e.g., maps, networks, and\",\n,\"locations, and other entities [200]. Constructing various visual and\"\n\"infographics). In addition to visual presentation, an emerging theme\",\n,generated textual (natural language) elements in visualizations is\nis to complement visualizations with natural language. The previous,\n,\"vital for narrative storytelling. To address this issue, the template\"\nsections have discussed natural language as an input modality. This,\n,\"is the most commonly applied method. For instance, Text-to-Viz\"\nsection will introduce using natural language as an output modality.,\n,[41] is designed to generate infographics from NL statements with\n\"10.1\nAnnotation\",\n,\"proportion-related statistics.\nIt builds 20 layout blueprints\nthat\"\n\"Annotation\nplays\na\nvital\nrole\nin\nexplaining\nand\nemphasizing\",\"describe the overall\nlook of the resulting infographics. For each\"\nkey points in the dataset. The systems should generate valuable,\"layout blueprint, Text-to-Viz enumerates all extracted segments by\"\n\"natural\nlanguage statements and map the text\nto a visualization\",text analyzer and then generates all valid infographics. DataShot\nappropriately. Annotation tools were ﬁrst applied to complement,\"[254], TSIs [26], Chen et al. [36], and Retrieve-Then-Adapt [183]\"\n\"news articles. Kandogan [101]\nintroduced the concept of\njust-\",all use similar template-based approaches for narrative storytelling.\n\"in-time descriptive analytics\nthat helps users easily understand\",\"With the advances of deep learning, some works leverage generative\"\n\"the structure of data. Given a piece of news, Contextiﬁer\n[89]\",\"adversarial networks\n(GAN)\n[69]\nto synthesize\nlayouts\n[139],\"\n\"ﬁrst\ncomputes\nclusters, outliers,\nand trends\nin line\ncharts\nand\",[286]. Those learning-based methods do not rely on handcrafted\nthen automatically produces annotated stock visualizations based,\"features. However, due to limited training data,\nthey often fail\"\non the information. Although it offers a promising example of,\"to\nachieve\ncomparable\nperformance with\nrule-based methods.\"\nhow human-produced news visualizations can be complemented,\"Likewise,\nstorytelling\nhas\nbeen\nextensively\nused\nto\nvisualize\"\n\"in a speciﬁc context,\nit\nis only suitable for stock sequence data.\",narrative text produced by online writers and journalism media.\nNewsViews [64] later extends Contextiﬁer to support more types,Metoyer et al. [162] proposed a novel approach to generate data-\nof appropriate data such as time series and georeferenced data.,\"rich stories. It ﬁrst extracts narrative components (who, what, when,\"\n\"In addition, several works incorporate the visual elements in the\",where) from the text and then generates narrative visualizations by\n\"chart\ninto annotations, synchronizing with textual descriptions. As\",integrating the supporting data evidence with the text. As shown\n\"shown in Figure 12, Vis-Annotator [122] leverages the Mask R-\",\"in Figure 13, given a highlighted NBA game report\nsentence,\"\n\"CNN model\nto identify visual elements in the target visualization,\",\"the\nsystem generates\nan initialized story dashboard based on\"\nalong with their visual properties. Textual descriptions of the chart,\"the interpretation information. Similarly, Story Analyzer\n[165]\"\nare synchronously interpreted to generate visual search requests.,\"extracts subjects, actions, and objects from the narrative text\nto\"\n\"Based on the identiﬁed information, each descriptive sentence is\",produce interrelated and user-responsive story dashboards. RIA\n\"displayed beside the described focal areas as annotations. Similarly,\",\"[284] dynamically derives a set of layout constraints (e.g., ensure\"\n\"Click2Annotate\n[33]\nand Touch2Annotate\n[34]\nare both semi-\",visual balance and avoid object occlusion) to tell spatial stories.\n\"automatic annotations generators. To be interactive, Calliope [213]\",Discussion: Although the aforementioned systems are promis-\n\"and ChartAccent\n[190]\nsupport\ncreating stories via\ninteractive\",\"ing and work relatively well,\nthey are restricted to one speciﬁc\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 13,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n13\nFig. 14. The automatic VQA pipeline [110] answers the three questions\ncorrectly (marked in green) and gives correct explanations.\ntype of information respectively. More types of information (e.g.,\ntimeline and map) can be combined to create more expressive\nstories. Besides, similar to annotation, template-based methods also\ndominate narrative storytelling. It will be interesting to explore\nlearning-based approaches to further improve the story quality as\nwell as formulating datasets specially for this topic.\n10.3\nNatural Language Generation for Visualization\nIn the past years, there has been a body of research on automatically\ngenerating descriptions or captions for charts. The majority of\nearly works are rule-based [39], [44], [57], [166], [167]. While\nmodern pipelines generally include a chart parsing module and a\nsubsequent caption generation module. Chart parsing module [8],\n[180], [199] deconstructs the original charts and extracts valuable\nelements such as text, axis, and lines using relevant techniques\nlike text localization, optical character recognition, and boundary\ntracing. Deshpande et al. [48] proposed a novel method for chart\nparsing which adopts a question answering approach to query\nkey elements in charts. The organized elements are subsequently\nfed into the caption generation module [144], [177] to output\ncaptions. A common shortcoming of the models above is the\ndemand for manually designed procedures or features. For example,\nAl-Zaidy et al. [8] relied on pre-deﬁned templates to generate\nsentences, and Chart-to-Text [177] needs additional annotated\ntabular data to describe elements in a given chart. Beneﬁting from\nthe development of deep learning, several end-to-end models have\nbeen proposed recently [177], [184], [223]. FigJAM [184] employs\nResNet-50 [76] to encode a chart as a whole image and uses OCR to\nencode text to generate slot values. They are along with the image\nfeature vectors as initialization of a LSTM network to generate\ncaptions. Spreaﬁco et al. [223] exploited the encoder-decoder\nLSTM architecture to take time-series data as input and generate\ncorresponding captions. Obeid et al. [177] applied a transformer-\nbased model [248] for generating chart summaries. Most recently,\nKim et al. [111] explored how readers gather takeaways when\nconsidering charts and captions together. Results suggest that the\ntakeaways differ when the caption mentions visual features of\ndiffering prominence levels, which provides valuable guidance for\nfuture research.\nDiscussion: So far, most related works in NLG4Vis can only\napply to simple charts (e.g., bar charts, scatterplots, and line charts).\nFurther research can pay attention to improving the chart type\ncoverage, as well as the language quality and descriptive ability.\nBesides, a more fundamental direction is to develop large datasets\ncovering diverse domains and chart types.\n10.4\nVisual Question Answering\nVisual Question Answering (VQA) is a semantic understanding\ntask that aims to answer questions based on a given visualization\nand possibly along with the informative text. In this subsection, we\nonly focus on images with charts or infographics rather than images\nwith natural objects and scenes in the computer vision community.\nGenerally, visualizations and questions are respectively encoded\nand then fused to generate answers [158]. For visualizations, Kim\net al. [110] leveraged a semantic parsing model, Sempre [282], to\ndevelop an automatic chart question answering pipeline with visual\nexplanations describing how the answer was produced. Figure\n14 shows that the pipeline answers all three questions correctly\n(marked in green) and gives correct explanations of how it obtains\nthe answer. While for infographics, the OCR module additionally\nneeds to identify and tokenize the text, which serves as facts for\nanswer generation [30], [97], [98], [158], [222]. Later models\nemploy more sophisticated fusion modules, among which the\nattention mechanism has reached great success. LEAF-Net [30]\ncomprises chart parsing, question, and answer encoding according\nto chart elements followed by an attention network is proposed.\nSTL-CQA [222] applies a structural transformer-based learning\napproach that emphasizes exploiting the structural properties of\ncharts. Recently, with the development of multimodal transformer\n[140], [237], Mathew et al. [158] proposed a pipeline where the\nquestion and infographic are mapped into the same vector space and\nsimply added as the input to stacks of transformer layers. Besides,\nthey also delivered a summary of VQA datasets for reference.\nDiscussion: Some systems extend existing QA engines (e.g.,\nSempre) to answer questions about charts; as a result, the interpre-\ntation ability is limited as they are not designed for visualization.\nA more proper way is to develop speciﬁc models based on\nvisualization datasets. However, the range of questions that these\nlearning-based approaches can handle may be restricted to the\ndiversity of datasets. So it may be promising to transfer existing\nmodels to other areas and iteratively extend the dataset.\n11\nRESEARCH OPPORTUNITY\nBy conducting a comprehensive survey under the guidance of an\ninformation visualization pipeline proposed by Card et al. [28],\nwe found that, as a rapidly growing ﬁeld, V-NLI faces many\nthorny challenges as well as open research opportunities. In this\nsection, we organize these from a macro-perspective into ﬁve\naspects: knowledge, model, interaction, presentation, and dataset,\nwith additional discussions on applications. Our target is to cover\nthe critical gaps and emerging topics that deserve more attention.\n11.1\nDomain Knowledge\nA major limitation of most existing V-NLI systems is the absence\nof domain knowledge. We have conducted an evaluation on four\nstate-of-the-art open-source V-NLIs, both academic [174], [273]\nand commercial [2], [5]. We found that none of the systems can\nrecognize that nitrogen dioxide and NO2 have the same meaning,\nnor can they recognize the relationship between age and birth\nyear. Therefore, the support of domain knowledge is crucial,\nespecially for extracting data attributes in the NL query. CogNet\n[249], a knowledge base dedicated to integrating various existing\nknowledge bases (e.g., FrameNet, YAGO, Freebase, DBpedia,\nWikidata, and ConceptNet), may be useful for broadening the\nrepertoire of utterances supported. Speciﬁcally, these knowledge\nbases can be wholly or partly used to create word embeddings.\nWith the help of embedding results, the similarity between N-grams\nof the NL query and data attributes names can be computed at both\nthe syntactic and semantic levels. Naturally, it is conceivable that\ndata attributes with high similarity or whose similarity exceeds a\ncertain threshold will be extracted.\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 13\nandpossiblyalongwiththeinformativetext.Inthissubsection,we\nonlyfocusonimageswithchartsorinfographicsratherthanimages\nwithnaturalobjectsandscenesinthecomputervisioncommunity.\nGenerally,visualizationsandquestionsarerespectivelyencoded\nandthenfusedtogenerateanswers[158].Forvisualizations,Kim\netal.[110]leveragedasemanticparsingmodel,Sempre[282],to\ndevelopanautomaticchartquestionansweringpipelinewithvisual\nexplanations describing how the answer was produced. Figure\n14 shows that the pipeline answers all three questions correctly\n(markedingreen)andgivescorrectexplanationsofhowitobtains\ntheanswer.Whileforinfographics,theOCRmoduleadditionally\nFig.14.TheautomaticVQApipeline[110]answersthethreequestions\nneedstoidentifyandtokenizethetext,whichservesasfactsfor\ncorrectly(markedingreen)andgivescorrectexplanations.\nanswer generation [30], [97], [98], [158], [222]. Later models\ntypeofinformationrespectively.Moretypesofinformation(e.g., employ more sophisticated fusion modules, among which the\ntimeline and map) can be combined to create more expressive attention mechanism has reached great success. LEAF-Net [30]\nstories.Besides,similartoannotation,template-basedmethodsalso compriseschartparsing,question,andanswerencodingaccording\ndominate narrative storytelling. It will be interesting to explore to chart elements followed by an attention network is proposed.\nlearning-basedapproachestofurtherimprovethestoryqualityas STL-CQA [222] applies a structural transformer-based learning\nwellasformulatingdatasetsspeciallyforthistopic. approach that emphasizes exploiting the structural properties of\ncharts.Recently,withthedevelopmentofmultimodaltransformer\n10.3 NaturalLanguageGenerationforVisualization\n[140],[237],Mathewetal.[158]proposedapipelinewherethe\nInthepastyears,therehasbeenabodyofresearchonautomatically\nquestionandinfographicaremappedintothesamevectorspaceand\ngenerating descriptions or captions for charts. The majority of\nsimplyaddedastheinputtostacksoftransformerlayers.Besides,\nearly works are rule-based [39], [44], [57], [166], [167]. While\ntheyalsodeliveredasummaryofVQAdatasetsforreference.\nmodernpipelinesgenerallyincludeachartparsingmoduleanda\nDiscussion:SomesystemsextendexistingQAengines(e.g.,\nsubsequentcaptiongenerationmodule.Chartparsingmodule[8],\nSempre)toanswerquestionsaboutcharts;asaresult,theinterpre-\n[180],[199]deconstructstheoriginalchartsandextractsvaluable\ntationabilityislimitedastheyarenotdesignedforvisualization.\nelements such as text, axis, and lines using relevant techniques\nA more proper way is to develop specific models based on\nliketextlocalization,opticalcharacterrecognition,andboundary\nvisualizationdatasets.However,therangeofquestionsthatthese\ntracing.Deshpandeetal.[48]proposedanovelmethodforchart\nlearning-based approaches can handle may be restricted to the\nparsing which adopts a question answering approach to query\ndiversityofdatasets.Soitmaybepromisingtotransferexisting\nkeyelementsincharts.Theorganizedelementsaresubsequently\nmodelstootherareasanditerativelyextendthedataset.\nfed into the caption generation module [144], [177] to output\ncaptions. A common shortcoming of the models above is the 11 RESEARCH OPPORTUNITY\ndemandformanuallydesignedproceduresorfeatures.Forexample,\nByconductingacomprehensivesurveyundertheguidanceofan\nAl-Zaidy et al. [8] relied on pre-defined templates to generate\ninformation visualization pipeline proposed by Card et al. [28],\nsentences, and Chart-to-Text [177] needs additional annotated\nwe found that, as a rapidly growing field, V-NLI faces many\ntabulardatatodescribeelementsinagivenchart.Benefitingfrom\nthornychallengesaswellasopenresearchopportunities.Inthis\nthedevelopmentofdeeplearning,severalend-to-endmodelshave\nsection, we organize these from a macro-perspective into five\nbeenproposedrecently[177],[184],[223].FigJAM[184]employs\naspects:knowledge,model,interaction,presentation,anddataset,\nResNet-50[76]toencodeachartasawholeimageandusesOCRto\nwithadditionaldiscussionsonapplications.Ourtargetistocover\nencodetexttogenerateslotvalues.Theyarealongwiththeimage\nthecriticalgapsandemergingtopicsthatdeservemoreattention.\nfeature vectors as initialization of a LSTM network to generate\ncaptions. Spreafico et al. [223] exploited the encoder-decoder 11.1 DomainKnowledge\nLSTMarchitecturetotaketime-seriesdataasinputandgenerate AmajorlimitationofmostexistingV-NLIsystemsistheabsence\ncorrespondingcaptions.Obeidetal.[177]appliedatransformer- ofdomainknowledge.Wehaveconductedanevaluationonfour\nbasedmodel[248]forgeneratingchartsummaries.Mostrecently, state-of-the-artopen-sourceV-NLIs,bothacademic[174],[273]\nKim et al. [111] explored how readers gather takeaways when andcommercial[2],[5].Wefoundthatnoneofthesystemscan\nconsideringchartsandcaptionstogether.Resultssuggestthatthe recognizethatnitrogendioxideandNO havethesamemeaning,\n2\ntakeaways differ when the caption mentions visual features of nor can they recognize the relationship between age and birth\ndifferingprominencelevels,whichprovidesvaluableguidancefor year. Therefore, the support of domain knowledge is crucial,\nfutureresearch. especiallyforextractingdataattributesintheNLquery.CogNet\nDiscussion:Sofar,mostrelatedworksinNLG4Viscanonly [249],aknowledgebasededicatedtointegratingvariousexisting\napplytosimplecharts(e.g.,barcharts,scatterplots,andlinecharts). knowledge bases (e.g., FrameNet, YAGO, Freebase, DBpedia,\nFurther research can pay attention to improving the chart type Wikidata, and ConceptNet), may be useful for broadening the\ncoverage, as well as the language quality and descriptive ability. repertoireofutterancessupported.Specifically,theseknowledge\nBesides,amorefundamentaldirectionistodeveloplargedatasets bases can be wholly or partly used to create word embeddings.\ncoveringdiversedomainsandcharttypes. Withthehelpofembeddingresults,thesimilaritybetweenN-grams\n10.4 VisualQuestionAnswering oftheNLqueryanddataattributesnamescanbecomputedatboth\nthesyntacticandsemanticlevels.Naturally,itisconceivablethat\nVisual Question Answering (VQA) is a semantic understanding\ndataattributeswithhighsimilarityorwhosesimilarityexceedsa\ntaskthataimstoanswerquestionsbasedonagivenvisualization\ncertainthresholdwillbeextracted.",
    "tables_extraction_text_csv": "0\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n\"\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 14,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n14\nTABLE 8\nSummary of existing V-NLI datasets.\nName\nPublication\nNL queries\nData tables\nBenchmark\nOther Contributions\nWebsite\nKim et al. [110]\nCHI’20\n629\n52\n×\nVQA with explanations\nhttps://github.com/dhkim16/VisQA-release\nQuda [61]\narXiv’20\n14,035\n36\n×\nThree Quda applications\nhttps://freenli.github.io/quda/\nNLV [228]\nCHI’21\n893\n3\n✓\nCharacterization of utterances\nhttps://nlvcorpus.github.io/\nnvBench [151]\nSIGMOD’21\n25,750\n780\n✓\nNL2SQL-to-NL2VIS and SEQ2VIS\nhttps://github.com/TsinghuaDatabaseGroup/nvBench\n11.2\nNLP Model\n11.2.1\nApplication of more advanced NLP models\nThe performance of V-NLI depends to a great extent on NLP\nmodels. As shown in Table 3 (column NLP Toolkit or Technology\nand column Recommendation Algorithm), most of the existing\nV-NLI systems just apply hand-crafted grammar rules or typical\nNLP toolkits for convenience. Recently, several state-of-the-art\nNLP models have reached human performance in speciﬁc tasks,\nsuch as ELMO [179], BERT [49], GPT-3 [25], and CPM-2 [283].\nSome works have leveraged the advances for data visualization\n[251], [262]. However, few works have applied them in V-NLI.\nDuring our survey, we also found that existing works provided\nlimited support for free-form NL input. To construct a more robust\nsystem, a promising direction is to apply more advanced NLP\nmodels to learn universal grammar patterns from a large corpus of\nconversations in visual analysis. In the process, we believe that the\nexisting high-quality text datasets will help train and evaluate robust\nNLP models for V-NLI. Besides, the community can pay more\nattention to the end-to-end approach due to its increased efﬁciency,\ncost cutting, ease of learning, and smarter visualization inference\nability (e.g., chart type selection, data insight mining, and missing\ndata prediction). To this end, advanced neural machine translation\nmodels [43] can be applied with various optimization schemes. NLP\nmodels of multiple languages also deserve attention since all V-NLI\nsystems we found can only support English. In fact, numerous NLP\nmodels can deal effectively with other languages (e.g., Chinese,\nFrench, and Spanish) and can be leveraged to develop V-NLI to\nserve more people who speak different languages.\n11.2.2\nDeep interpretation of dataset semantics\nSemantic information plays an important role in V-NLI. The state-\nof-the-art systems have considered leveraging semantic parsing\ntoolkits like SEMPRE [282] to parse NL queries [110], [273].\nHowever, just considering the semantics of the NL query is\nlimited, and the semantics of the dataset should also be taken\ninto consideration. The existing technologies for data attribute\nmatching only conﬁne to the letter-matching level but do not go\ndeep into the semantic-matching level, as described in Section 4.3.\nFor example, when the analyzed dataset is about movies, a full-\nﬂedged system should be able to recognize that the Name attribute\nin the dataset refers to the movie name and automatically associate\nit with other attributes appearing in the query. A promising way to\naugment semantic interpretation ability is to connect with recent\nsemantic data type detection models for visualization like Sherlock\n[90], Sato [278], ColNet [31], DCOM [156], EXACTA [267], and\nDoduo [238]. Incorporating such connections will help better infer\nattribute types upon initialization and may also reduce the need\nfor developers to manually conﬁgure attribute aliases. Besides,\nthe aforementioned models are limited to a ﬁxed set of semantic\ntypes. An additional essential direction is to extend the existing\nsemantic data type detection models to support more semantic\ntypes. Additionally, with the contextually deep interpretation of\ndataset semantics, supporting queries for multi-table schema would\nbe an interesting research hotspot.\n11.3\nInteraction\n11.3.1\nRicher prompt to improve system discoverability\nThe user may not be aware of what input is valid and what chart\ntypes are supported by the system with open-ended textboxes. As\ndescribed in Section 8.2, discoverability has received relatively\nlittle attention in current V-NLIs compared to other characteristics.\nThe existing practices have revealed that real-time prompts can\neffectively help the user better understand system features and\ncorrect input errors in time when typing queries [206], [224],\n[273] (see Table 3 (column Autocompletion)). The most common\nmethod is template-based text autocompletion [273]. However,\nthis method is relatively monotonous in practice and does not\nwork for spoken commands. Several works offer the user prompt\nwidgets of data previews to speed up the user’s input speed\n[206], but the supporting prompt widget types are limited. We\nbelieve that richer interactive widgets with prompts and multimodal\nsynergistic interaction can greatly improve the system’s usability.\nShowing provenance of prompt behavior can also enhance the\ninterpretability of visualization results. Concerning this issue, Setlur\net al. [206] conducted a crowdsourcing study regarding the efﬁcacy\nof autocompletion suggestions. The insights drawn from the studies\nare of great value to inspire the future design of V-NLI.\n11.3.2\nTake advantage of the user’s interaction history\nThe intent behind NL queries can be satisﬁed by various types\nof charts. These visualization results are too broad that existing\nsystems can hardly account for varying user preferences. Although\nseveral conversational V-NLI systems have been proposed [83],\n[121], [205] to analyze NL queries in context, few systems\nhave taken the user’s interaction history into account. Recently,\nZehrung et al. [276] conducted a crowdsourcing study analyzing\ntrust in humans versus algorithmically generated visualization\nrecommendations. Based on the results, they suggested that\nthe recommendation system should be customized according\nto the speciﬁc user’s information search strategy. Personalized\ninformation derived from historical user interaction and context\ncan provide a richer model to satisfy the user’s analytic tasks.\nA large number of innovative models in the recommendation\nsystem [281] can also be applied for reference. Besides, Lee et\nal. [130] recently deconstructed categorization in visualization\nrecommendation. Several works [79], [209] have studied the\nassociations and expectations about verbalization and visualization\nreported by users. Integrating the information for further modeling\nof the user’s interaction history is another interesting research topic.\n11.4\nPresentation\nWe hypothesize that we would not truly have succeeded in\ndemocratizing access to visual analysis through natural language\nuntil we design systems that use NL both as input and output\nmodality. For this vision, a promising direction is to combine all the\naforementioned characteristics (e.g., VQA, NLG, and annotation) to\ndesign a hybrid system. In addition, existing V-NLI systems mostly\nonly support 2D visualization. Nowadays, Immersive Analytics (IA)\nis a quickly evolving ﬁeld that leverages immersive technologies\nfor data analysis [56], [60]. In the visualization community, several\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 14\nTABLE8\nSummaryofexistingV-NLIdatasets.\nName Publication NLqueries Datatables Benchmark OtherContributions Website\nKimetal.[110] CHI’20 629 52 × VQAwithexplanations https://github.com/dhkim16/VisQA-release\nQuda[61] arXiv’20 14,035 36 × ThreeQudaapplications https://freenli.github.io/quda/\nNLV[228] CHI’21 893 3 (cid:88) Characterizationofutterances https://nlvcorpus.github.io/\nnvBench[151] SIGMOD’21 25,750 780 (cid:88) NL2SQL-to-NL2VISandSEQ2VIS https://github.com/TsinghuaDatabaseGroup/nvBench\n11.2 NLPModel 11.3 Interaction\n11.3.1 Richerprompttoimprovesystemdiscoverability\n11.2.1 ApplicationofmoreadvancedNLPmodels\nTheusermaynotbeawareofwhatinputisvalidandwhatchart\nThe performance of V-NLI depends to a great extent on NLP\ntypesaresupportedbythesystemwithopen-endedtextboxes.As\nmodels.AsshowninTable3(columnNLPToolkitorTechnology\ndescribed in Section 8.2, discoverability has received relatively\nand column Recommendation Algorithm), most of the existing\nlittleattentionincurrentV-NLIscomparedtoothercharacteristics.\nV-NLIsystemsjustapplyhand-craftedgrammarrulesortypical\nThe existing practices have revealed that real-time prompts can\nNLP toolkits for convenience. Recently, several state-of-the-art\neffectively help the user better understand system features and\nNLP models have reached human performance in specific tasks,\ncorrect input errors in time when typing queries [206], [224],\nsuchasELMO[179],BERT[49],GPT-3[25],andCPM-2[283].\n[273](seeTable3(columnAutocompletion)).Themostcommon\nSome works have leveraged the advances for data visualization\nmethod is template-based text autocompletion [273]. However,\n[251], [262]. However, few works have applied them in V-NLI.\nthis method is relatively monotonous in practice and does not\nDuring our survey, we also found that existing works provided\nworkforspokencommands.Severalworksoffertheuserprompt\nlimitedsupportforfree-formNLinput.Toconstructamorerobust\nwidgets of data previews to speed up the user’s input speed\nsystem, a promising direction is to apply more advanced NLP\n[206], but the supporting prompt widget types are limited. We\nmodelstolearnuniversalgrammarpatternsfromalargecorpusof\nbelievethatricherinteractivewidgetswithpromptsandmultimodal\nconversationsinvisualanalysis.Intheprocess,webelievethatthe\nsynergisticinteractioncangreatlyimprovethesystem’susability.\nexistinghigh-qualitytextdatasetswillhelptrainandevaluaterobust\nShowing provenance of prompt behavior can also enhance the\nNLP models for V-NLI. Besides, the community can pay more\ninterpretabilityofvisualizationresults.Concerningthisissue,Setlur\nattentiontotheend-to-endapproachduetoitsincreasedefficiency,\netal.[206]conductedacrowdsourcingstudyregardingtheefficacy\ncostcutting,easeoflearning,andsmartervisualizationinference\nofautocompletionsuggestions.Theinsightsdrawnfromthestudies\nability(e.g.,charttypeselection,datainsightmining,andmissing\nareofgreatvaluetoinspirethefuturedesignofV-NLI.\ndataprediction).Tothisend,advancedneuralmachinetranslation\nmodels[43]canbeappliedwithvariousoptimizationschemes.NLP 11.3.2 Takeadvantageoftheuser’sinteractionhistory\nmodelsofmultiplelanguagesalsodeserveattentionsinceallV-NLI The intent behind NL queries can be satisfied by various types\nsystemswefoundcanonlysupportEnglish.Infact,numerousNLP of charts. These visualization results are too broad that existing\nmodels can deal effectively with other languages (e.g., Chinese, systemscanhardlyaccountforvaryinguserpreferences.Although\nFrench, and Spanish) and can be leveraged to develop V-NLI to several conversational V-NLI systems have been proposed [83],\nservemorepeoplewhospeakdifferentlanguages. [121], [205] to analyze NL queries in context, few systems\nhave taken the user’s interaction history into account. Recently,\n11.2.2 Deepinterpretationofdatasetsemantics Zehrungetal.[276]conductedacrowdsourcingstudyanalyzing\ntrust in humans versus algorithmically generated visualization\nSemanticinformationplaysanimportantroleinV-NLI.Thestate-\nrecommendations. Based on the results, they suggested that\nof-the-art systems have considered leveraging semantic parsing\nthe recommendation system should be customized according\ntoolkits like SEMPRE [282] to parse NL queries [110], [273].\nto the specific user’s information search strategy. Personalized\nHowever, just considering the semantics of the NL query is\ninformation derived from historical user interaction and context\nlimited, and the semantics of the dataset should also be taken\ncan provide a richer model to satisfy the user’s analytic tasks.\ninto consideration. The existing technologies for data attribute\nA large number of innovative models in the recommendation\nmatchingonlyconfinetotheletter-matchinglevelbutdonotgo\nsystem [281] can also be applied for reference. Besides, Lee et\ndeepintothesemantic-matchinglevel,asdescribedinSection4.3.\nal. [130] recently deconstructed categorization in visualization\nFor example, when the analyzed dataset is about movies, a full-\nrecommendation. Several works [79], [209] have studied the\nfledgedsystemshouldbeabletorecognizethattheNameattribute\nassociationsandexpectationsaboutverbalizationandvisualization\ninthedatasetreferstothemovienameandautomaticallyassociate\nreportedbyusers.Integratingtheinformationforfurthermodeling\nitwithotherattributesappearinginthequery.Apromisingwayto\noftheuser’sinteractionhistoryisanotherinterestingresearchtopic.\naugmentsemanticinterpretationabilityistoconnectwithrecent\nsemanticdatatypedetectionmodelsforvisualizationlikeSherlock 11.4 Presentation\n[90],Sato[278],ColNet[31],DCOM[156],EXACTA[267],and\nWe hypothesize that we would not truly have succeeded in\nDoduo[238].Incorporatingsuchconnectionswillhelpbetterinfer\ndemocratizingaccesstovisualanalysisthroughnaturallanguage\nattribute types upon initialization and may also reduce the need\nuntil we design systems that use NL both as input and output\nfor developers to manually configure attribute aliases. Besides,\nmodality.Forthisvision,apromisingdirectionistocombineallthe\ntheaforementionedmodelsarelimitedtoafixedsetofsemantic\naforementionedcharacteristics(e.g.,VQA,NLG,andannotation)to\ntypes. An additional essential direction is to extend the existing\ndesignahybridsystem.Inaddition,existingV-NLIsystemsmostly\nsemantic data type detection models to support more semantic\nonlysupport2Dvisualization.Nowadays,ImmersiveAnalytics(IA)\ntypes. Additionally, with the contextually deep interpretation of\nisaquicklyevolvingfieldthatleveragesimmersivetechnologies\ndatasetsemantics,supportingqueriesformulti-tableschemawould\nfordataanalysis[56],[60].Inthevisualizationcommunity,several\nbeaninterestingresearchhotspot.",
    "tables_extraction_text_csv": "0,1\n\"Name\nPublication\nNL queries\nData tables\nBenchmark\",\"Other Contributions\nWebsite\"\n\"Kim et al. [110]\nCHI’20\n629\n52\n×\",\"VQA with explanations\nhttps://github.com/dhkim16/VisQA-release\"\n\"Quda [61]\narXiv’20\n14,035\n36\n×\",\"Three Quda applications\nhttps://freenli.github.io/quda/\"\n\"(cid:88)\nNLV [228]\nCHI’21\n893\n3\",\"Characterization of utterances\nhttps://nlvcorpus.github.io/\"\n\"(cid:88)\nnvBench [151]\nSIGMOD’21\n25,750\n780\",\"NL2SQL-to-NL2VIS and SEQ2VIS\nhttps://github.com/TsinghuaDatabaseGroup/nvBench\"\n\"11.2\nNLP Model\",\"11.3\nInteraction\"\n,\"11.3.1\nRicher prompt\nto improve system discoverability\"\n\"11.2.1\nApplication of more advanced NLP models\",\n,\"The user may not be aware of what\ninput\nis valid and what chart\"\n\"The performance of V-NLI depends\nto a great extent on NLP\",\n,types are supported by the system with open-ended textboxes. As\nmodels. As shown in Table 3 (column NLP Toolkit or Technology,\n,\"described in Section 8.2, discoverability has received relatively\"\n\"and column Recommendation Algorithm), most of\nthe existing\",\n,little attention in current V-NLIs compared to other characteristics.\nV-NLI systems just apply hand-crafted grammar rules or typical,\n,\"The existing practices have revealed that\nreal-time prompts can\"\n\"NLP toolkits for convenience. Recently, several state-of-the-art\",\n,effectively help the user better understand system features and\n\"NLP models have reached human performance in speciﬁc tasks,\",\n,\"correct\ninput errors\nin time when typing queries\n[206],\n[224],\"\n\"such as ELMO [179], BERT [49], GPT-3 [25], and CPM-2 [283].\",\n,[273] (see Table 3 (column Autocompletion)). The most common\nSome works have leveraged the advances for data visualization,\n,\"method is\ntemplate-based text autocompletion [273]. However,\"\n\"[251],\n[262]. However,\nfew works have applied them in V-NLI.\",\n,\"this method is\nrelatively monotonous\nin practice and does not\"\n\"During our survey, we also found that existing works provided\",\n,work for spoken commands. Several works offer the user prompt\nlimited support for free-form NL input. To construct a more robust,\n,\"widgets of data previews\nto speed up the user’s\ninput\nspeed\"\n\"system, a promising direction is\nto apply more advanced NLP\",\n,\"[206], but\nthe supporting prompt widget\ntypes are limited. We\"\nmodels to learn universal grammar patterns from a large corpus of,\n,believe that richer interactive widgets with prompts and multimodal\n\"conversations in visual analysis. In the process, we believe that\nthe\",\n,synergistic interaction can greatly improve the system’s usability.\nexisting high-quality text datasets will help train and evaluate robust,\n,Showing provenance of prompt behavior can also enhance the\n\"NLP models for V-NLI. Besides,\nthe community can pay more\",\n,\"interpretability of visualization results. Concerning this issue, Setlur\"\n\"attention to the end-to-end approach due to its increased efﬁciency,\",\n,et al. [206] conducted a crowdsourcing study regarding the efﬁcacy\n\"cost cutting, ease of learning, and smarter visualization inference\",\n,of autocompletion suggestions. The insights drawn from the studies\n\"ability (e.g., chart\ntype selection, data insight mining, and missing\",\n,are of great value to inspire the future design of V-NLI.\n\"data prediction). To this end, advanced neural machine translation\",\n,\"11.3.2\nTake advantage of\nthe user’s interaction history\"\nmodels [43] can be applied with various optimization schemes. NLP,\nmodels of multiple languages also deserve attention since all V-NLI,The intent behind NL queries can be satisﬁed by various types\n\"systems we found can only support English. In fact, numerous NLP\",of charts. These visualization results are too broad that existing\n\"models can deal effectively with other languages (e.g., Chinese,\",systems can hardly account for varying user preferences. Although\n\"French, and Spanish) and can be leveraged to develop V-NLI to\",\"several conversational V-NLI systems have been proposed [83],\"\n\"serve more people who speak different\nlanguages.\",\"[121],\n[205]\nto\nanalyze NL queries\nin\ncontext,\nfew systems\"\n,\"have taken the user’s interaction history into account. Recently,\"\n\"11.2.2\nDeep interpretation of dataset semantics\",\n,Zehrung et al. [276] conducted a crowdsourcing study analyzing\n,\"trust\nin humans versus\nalgorithmically generated visualization\"\nSemantic information plays an important role in V-NLI. The state-,\n,\"recommendations. Based\non\nthe\nresults,\nthey\nsuggested\nthat\"\nof-the-art systems have considered leveraging semantic parsing,\n,\"the\nrecommendation\nsystem should\nbe\ncustomized\naccording\"\n\"toolkits\nlike SEMPRE [282]\nto parse NL queries\n[110],\n[273].\",\n,\"to the speciﬁc user’s\ninformation search strategy. Personalized\"\n\"However,\njust\nconsidering\nthe\nsemantics\nof\nthe NL query\nis\",\n,\"information derived from historical user\ninteraction and context\"\n\"limited, and the semantics of\nthe dataset\nshould also be taken\",\n,\"can provide a richer model\nto satisfy the user’s analytic tasks.\"\n\"into consideration. The existing technologies\nfor data attribute\",\n,\"A large number of\ninnovative models\nin the\nrecommendation\"\nmatching only conﬁne to the letter-matching level but do not go,\n,\"system [281] can also be applied for\nreference. Besides, Lee et\"\n\"deep into the semantic-matching level, as described in Section 4.3.\",\n,\"al.\n[130]\nrecently deconstructed categorization in visualization\"\n\"For example, when the analyzed dataset\nis about movies, a full-\",\n,\"recommendation. Several works\n[79],\n[209]\nhave\nstudied\nthe\"\n\"ﬂedged system should be able to recognize that\nthe Name attribute\",\n,associations and expectations about verbalization and visualization\nin the dataset refers to the movie name and automatically associate,\n,reported by users. Integrating the information for further modeling\nit with other attributes appearing in the query. A promising way to,\n,of the user’s interaction history is another interesting research topic.\naugment semantic interpretation ability is to connect with recent,\nsemantic data type detection models for visualization like Sherlock,\n,\"11.4\nPresentation\"\n\"[90], Sato [278], ColNet [31], DCOM [156], EXACTA [267], and\",\n,\"We\nhypothesize\nthat we would\nnot\ntruly\nhave\nsucceeded\nin\"\nDoduo [238]. Incorporating such connections will help better infer,\n,democratizing access to visual analysis through natural language\nattribute types upon initialization and may also reduce the need,\n,\"until we design systems\nthat use NL both as\ninput and output\"\n\"for developers\nto manually conﬁgure attribute aliases. Besides,\",\n,\"modality. For this vision, a promising direction is to combine all the\"\nthe aforementioned models are limited to a ﬁxed set of semantic,\n,\"aforementioned characteristics (e.g., VQA, NLG, and annotation) to\"\ntypes. An additional essential direction is to extend the existing,\n,\"design a hybrid system. In addition, existing V-NLI systems mostly\"\n\"semantic data type detection models\nto support more semantic\",\n,\"only support 2D visualization. Nowadays, Immersive Analytics (IA)\"\n\"types. Additionally, with the contextually deep interpretation of\",\n,is a quickly evolving ﬁeld that leverages immersive technologies\n\"dataset semantics, supporting queries for multi-table schema would\",\n,\"for data analysis [56], [60]. In the visualization community, several\"\nbe an interesting research hotspot.,\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 15,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n15\nworks have augmented static visualizations with virtual content\n[35], [124]. Reinders et al. [189] studied blind and low vision\npeople’s preferences when exploring interactive 3D printed models\n(I3Ms). However, there have been no systems that support NLI\nfor data visualization in an immersive way. A related work for\nreference is that Lee et al. [125] proposed the concept of data\nvisceralization and introduced a conceptual pipeline. It would be\ninteresting to enrich this pipeline with the integration of V-NLI.\nBesides, as described in Section 7, view transformations are rarely\ninvolved in existing systems. With immersive technologies, more\nview transformations can be explored and integrated into V-NLI to\nprovide an immersive interactive experience.\n11.5\nDataset\nThere is a widely recognized consensus that large-scale data\ncollection is an effective way to facilitate community development\n(e.g., ImageNet [47] for image processing and GEM [66] for\nnatural language generation). Although there are various datasets\nfor general NLP tasks, they can hardly be directly applied to\nprovide training samples for V-NLI models. In the visualization\ncommunity, several works have begun to collect large datasets\nfor V-NLI, as shown in Table 8. To assist the deployment of\nlearning-based techniques for parsing human language, Fu et al.\n[61] proposed Quda, containing diverse user queries annotated\nwith analytic tasks. Srinivasan et al. [228] conducted an online\nstudy to collect NL utterances and characterized them based on\ntheir phrasing type. VizNet [85] is a step forward in addressing the\nneed for large-scale data and visualization repositories. Kim et al.\n[110] collected questions people posed about various bar charts\nand line charts, along with their answers and explanations to the\nquestions. Fully considering the user’s characteristics, Kassel et al.\n[104] introduced a linguistically motivated two-dimensional answer\nspace that varies in the level of both support and information to\nmatch the human language in visual analysis. The major limitation\nof them is that the query types contained are not rich enough.\nBesides, unfortunately, a benchmark or the ground truth of a given\ndataset is usually unavailable. Although Srinivasan et al. [228]\nmade an initial attempt to present a collection of NL utterances\nwith the mapping visualizations, Luo et al. [151] synthesized\nNL-to-Vis benchmarks by piggybacking NL-to-SQL benchmarks\nand produced a NL-to-Vis benchmark, nvBench, the supporting\nvisualization types and tasks are limited. Therefore, collecting\nlarge-scale datasets and creating new benchmarks that support\nmore tasks, domains, and interactions will be an indispensable\nresearch direction in the future.\n11.6\nApplication\nApart from facilitating data analysis, much attention has been paid\nto beneﬁt real-world applications by integrating visual analysis\nand natural language interface. Huang et al. [88] proposed a\nquery engine to convert, store and retrieve spatial uncertain mobile\ntrajectories via intuitive NL input. Leo John et al. [133] proposed a\nnovel V-NLI to promote medical imaging and biomedical research.\nWith an audio travel podcast as input, Crosscast [266] identiﬁes\ngeographic locations and descriptive keywords within the podcast\ntranscript through NLP and text mining techniques. The information\nis later used to select relevant photos from online repositories\nand synchronize their display to align with the audio narration.\nMeetingVis [214] leverages ASR and NLP techniques to promote\neffective meeting summaries in team-based workplaces. PathViewer\n[255] leverages ideas from ﬂow diagrams and NLP to visualize the\nsequences of intermediate steps that students take. Since V-NLI\ncan be easily integrated as a module into the visualization system,\nmore applications can be explored in the future.\n12\nCONCLUSION\nThe past two decades have witnessed the rapid development of\nvisualization-oriented natural language interfaces, which act as\na complementary input modality for visual analytics. However,\nthe community lacks a comprehensive survey of related works\nto guide follow-up research. We ﬁll the gap, and the resulting\nsurvey gives a comprehensive overview of what characteristics are\ncurrently concerned and supported by V-NLI. We also propose\nseveral promising directions for future work. To our knowledge,\nthis paper is the ﬁrst step towards reviewing V-NLI in a novel and\nsystematic manner. We hope that this paper can better guide future\nresearch and encourage the community to think further about NLI\nfor data visualization.\nACKNOWLEDGMENTS\nThe authors would like to thank all the reviewers for their\nvaluable comments. They also thank all the authors of related\npapers, especially those who kindly provide images covered in this\nsurvey. The work was supported by the National Natural Science\nFoundation of China (No. 71690231) and Beijing Key Laboratory\nof Industrial Bigdata System and Application.\nREFERENCES\n[1]\nApache OpenNLP. http://opennlp.apache.org/.\n[2]\nAsk data. https://www.tableau.com/products/new-features/ask-data.\n[3]\nGoogle NLP. https://cloud.google.com/natural-language/.\n[4]\nIBM Watson Analytics. http://www.ibm. com/analytics/watson-analytics.\n[5]\nMicrosoft Power BI. https://docs.microsoft.com/en-us/power-bi/create-\nreports/power-bi-tutorial-q-and-a.\n[6]\nK. Affolter, K. Stockinger, and A. Bernstein. A comparative survey of\nrecent natural language interfaces for databases. VLDB J., 28(5), 2019.\n[7]\nA. Akbik, D. Blythe, and R. Vollgraf. Contextual String Embeddings for\nSequence Labeling. In Proc. COLING’19. ACM, 2018.\n[8]\nR. A. Al-Zaidy and C. L. Giles. Automatic Extraction of Data from Bar\nCharts. In Proc. ICKC’15. ACM, 2015.\n[9]\nR. Amar, J. Eagan, and J. Stasko. Low-level components of analytic\nactivity in information visualization. In Proc. INFOVIS’05. IEEE, 2005.\n[10]\nF. Bacci, F. M. Cau, and L. D. Spano. Inspecting Data Using Natural\nLanguage Queries. Lect. Notes Comput. Sci., 12254:771–782, 2020.\n[11]\nB. Bach, Z. Wang, M. Farinella, and et al. Design Patterns for Data\nComics. In Proc. CHI’18. ACM, 2018.\n[12]\nD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by\njointly learning to align and translate. In Proc. ICLR’15, 2015.\n[13]\nC. Baik, H. V. Jagadish, and Y. Li. Bridging the semantic gap with SQL\nquery logs in natural language interfaces to databases. In Proc. ICDE’19.\nIEEE, 2019.\n[14]\nF. Basik, B. H¨attasch, A. Ilkhechi, and et al. DBPal: A learned NL-\ninterface for databases. In Proc. SIGMOD’18. ACM, 2018.\n[15]\nH. Bast and E. Haussmann. More Accurate Question Answering on\nFreebase. In Proc. CIKM’15. ACM, 2015.\n[16]\nL. Battle, R. J. Crouser, A. Nakeshimana, and et al. The Role of Latency\nand Task Complexity in Predicting Visual Search Behavior. IEEE Trans.\nVis. Comput. Graph., 26(1):1246–1255, 2020.\n[17]\nL. Battle and C. Scheidegger. A Structured Review of Data Management\nTechnology for Interactive Visualization and Analysis. IEEE Trans. Vis.\nComput. Graph., 27(2):1128–1138, 2021.\n[18]\nY. Belinkov and J. Glass.\nAnalysis Methods in Neural Language\nProcessing: A Survey. Trans. Assoc. Comput. Linguist., 7:49–72, 2019.\n[19]\nS. Bergamaschi, F. Guerra, M. Interlandi, and et al. Combining user\nand database perspective for solving keyword queries over relational\ndatabases. Inf. Syst., 55:1–19, 2016.\n[20]\nS. Bieliauskas and A. Schreiber. A Conversational User Interface for\nSoftware Visualization. In Proc. VISSOFT’17. IEEE, 2017.\n[21]\nS. Bird. NLTK: the natural language toolkit. In Proc. COLING-ACL’06.\nACL, 2006.\n[22]\nL. Blunschi, C. Jossen, D. Kossmann, and et al. SODA: Generating SQL\nfor business users. Proc. VLDB Endow., 5(10):932–943, 2012.\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 15\nworks have augmented static visualizations with virtual content sequences of intermediate steps that students take. Since V-NLI\n[35], [124]. Reinders et al. [189] studied blind and low vision canbeeasilyintegratedasamoduleintothevisualizationsystem,\npeople’spreferenceswhenexploringinteractive3Dprintedmodels moreapplicationscanbeexploredinthefuture.\n(I3Ms). However, there have been no systems that support NLI\nfor data visualization in an immersive way. A related work for\n12 CONCLUSION\nreference is that Lee et al. [125] proposed the concept of data The past two decades have witnessed the rapid development of\nvisceralizationandintroducedaconceptualpipeline.Itwouldbe visualization-oriented natural language interfaces, which act as\ninteresting to enrich this pipeline with the integration of V-NLI. a complementary input modality for visual analytics. However,\nBesides,asdescribedinSection7,viewtransformationsarerarely the community lacks a comprehensive survey of related works\ninvolvedinexistingsystems.Withimmersivetechnologies,more to guide follow-up research. We fill the gap, and the resulting\nviewtransformationscanbeexploredandintegratedintoV-NLIto surveygivesacomprehensiveoverviewofwhatcharacteristicsare\nprovideanimmersiveinteractiveexperience. currently concerned and supported by V-NLI. We also propose\nseveral promising directions for future work. To our knowledge,\n11.5 Dataset\nthispaperisthefirststeptowardsreviewingV-NLIinanoveland\nThere is a widely recognized consensus that large-scale data systematicmanner.Wehopethatthispapercanbetterguidefuture\ncollectionisaneffectivewaytofacilitatecommunitydevelopment researchandencouragethecommunitytothinkfurtheraboutNLI\n(e.g., ImageNet [47] for image processing and GEM [66] for fordatavisualization.\nnaturallanguagegeneration).Althoughtherearevariousdatasets\nACKNOWLEDGMENTS\nfor general NLP tasks, they can hardly be directly applied to\nThe authors would like to thank all the reviewers for their\nprovide training samples for V-NLI models. In the visualization\nvaluable comments. They also thank all the authors of related\ncommunity, several works have begun to collect large datasets\npapers,especiallythosewhokindlyprovideimagescoveredinthis\nfor V-NLI, as shown in Table 8. To assist the deployment of\nsurvey.TheworkwassupportedbytheNationalNaturalScience\nlearning-based techniques for parsing human language, Fu et al.\nFoundationofChina(No.71690231)andBeijingKeyLaboratory\n[61] proposed Quda, containing diverse user queries annotated\nofIndustrialBigdataSystemandApplication.\nwith analytic tasks. Srinivasan et al. [228] conducted an online\nstudy to collect NL utterances and characterized them based on REFERENCES\ntheirphrasingtype.VizNet[85]isastepforwardinaddressingthe\n[1] ApacheOpenNLP.http://opennlp.apache.org/.\nneedforlarge-scaledataandvisualizationrepositories.Kimetal.\n[2] Askdata.https://www.tableau.com/products/new-features/ask-data.\n[110] collected questions people posed about various bar charts [3] GoogleNLP.https://cloud.google.com/natural-language/.\nandlinecharts,alongwiththeiranswersandexplanationstothe [4] IBMWatsonAnalytics.http://www.ibm.com/analytics/watson-analytics.\n[5] MicrosoftPowerBI.https://docs.microsoft.com/en-us/power-bi/create-\nquestions.Fullyconsideringtheuser’scharacteristics,Kasseletal.\nreports/power-bi-tutorial-q-and-a.\n[104]introducedalinguisticallymotivatedtwo-dimensionalanswer [6] K.Affolter,K.Stockinger,andA.Bernstein. Acomparativesurveyof\nspace that varies in the level of both support and information to recentnaturallanguageinterfacesfordatabases. VLDBJ.,28(5),2019.\nmatchthehumanlanguageinvisualanalysis.Themajorlimitation [7] A.Akbik,D.Blythe,andR.Vollgraf.ContextualStringEmbeddingsfor\nSequenceLabeling. InProc.COLING’19.ACM,2018.\nof them is that the query types contained are not rich enough.\n[8] R.A.Al-ZaidyandC.L.Giles. AutomaticExtractionofDatafromBar\nBesides,unfortunately,abenchmarkorthegroundtruthofagiven Charts. InProc.ICKC’15.ACM,2015.\ndataset is usually unavailable. Although Srinivasan et al. [228] [9] R.Amar,J.Eagan,andJ.Stasko. Low-levelcomponentsofanalytic\nactivityininformationvisualization. InProc.INFOVIS’05.IEEE,2005.\nmade an initial attempt to present a collection of NL utterances\n[10] F.Bacci,F.M.Cau,andL.D.Spano. InspectingDataUsingNatural\nwith the mapping visualizations, Luo et al. [151] synthesized LanguageQueries. Lect.NotesComput.Sci.,12254:771–782,2020.\nNL-to-VisbenchmarksbypiggybackingNL-to-SQLbenchmarks [11] B.Bach,Z.Wang,M.Farinella,andetal. DesignPatternsforData\nand produced a NL-to-Vis benchmark, nvBench, the supporting Comics. InProc.CHI’18.ACM,2018.\n[12] D.Bahdanau,K.Cho,andY.Bengio. Neuralmachinetranslationby\nvisualization types and tasks are limited. Therefore, collecting\njointlylearningtoalignandtranslate. InProc.ICLR’15,2015.\nlarge-scale datasets and creating new benchmarks that support [13] C.Baik,H.V.Jagadish,andY.Li. BridgingthesemanticgapwithSQL\nmore tasks, domains, and interactions will be an indispensable querylogsinnaturallanguageinterfacestodatabases.InProc.ICDE’19.\nIEEE,2019.\nresearchdirectioninthefuture.\n[14] F.Basik,B.Ha¨ttasch,A.Ilkhechi,andetal. DBPal:AlearnedNL-\ninterfacefordatabases. InProc.SIGMOD’18.ACM,2018.\n11.6 Application\n[15] H.BastandE.Haussmann. MoreAccurateQuestionAnsweringon\nApartfromfacilitatingdataanalysis,muchattentionhasbeenpaid Freebase. InProc.CIKM’15.ACM,2015.\n[16] L.Battle,R.J.Crouser,A.Nakeshimana,andetal.TheRoleofLatency\nto benefit real-world applications by integrating visual analysis\nandTaskComplexityinPredictingVisualSearchBehavior. IEEETrans.\nand natural language interface. Huang et al. [88] proposed a Vis.Comput.Graph.,26(1):1246–1255,2020.\nqueryenginetoconvert,storeandretrievespatialuncertainmobile [17] L.BattleandC.Scheidegger.AStructuredReviewofDataManagement\ntrajectoriesviaintuitiveNLinput.LeoJohnetal.[133]proposeda TechnologyforInteractiveVisualizationandAnalysis. IEEETrans.Vis.\nComput.Graph.,27(2):1128–1138,2021.\nnovelV-NLItopromotemedicalimagingandbiomedicalresearch.\n[18] Y. Belinkov and J. Glass. Analysis Methods in Neural Language\nWith an audio travel podcast as input, Crosscast [266] identifies Processing:ASurvey. Trans.Assoc.Comput.Linguist.,7:49–72,2019.\ngeographiclocationsanddescriptivekeywordswithinthepodcast [19] S.Bergamaschi,F.Guerra,M.Interlandi,andetal. Combininguser\nanddatabaseperspectiveforsolvingkeywordqueriesoverrelational\ntranscriptthroughNLPandtextminingtechniques.Theinformation\ndatabases. Inf.Syst.,55:1–19,2016.\nis later used to select relevant photos from online repositories [20] S.BieliauskasandA.Schreiber. AConversationalUserInterfacefor\nand synchronize their display to align with the audio narration. SoftwareVisualization. InProc.VISSOFT’17.IEEE,2017.\nMeetingVis[214]leveragesASRandNLPtechniquestopromote [21] S.Bird. NLTK:thenaturallanguagetoolkit. InProc.COLING-ACL’06.\nACL,2006.\neffectivemeetingsummariesinteam-basedworkplaces.PathViewer\n[22] L.Blunschi,C.Jossen,D.Kossmann,andetal.SODA:GeneratingSQL\n[255]leveragesideasfromflowdiagramsandNLPtovisualizethe forbusinessusers. Proc.VLDBEndow.,5(10):932–943,2012.",
    "tables_extraction_text_csv": "0\n\"IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n15\"\n\"works have augmented static visualizations with virtual content\nsequences of intermediate steps that students take. Since V-NLI\"\n\"[35],\n[124]. Reinders et al.\n[189]\nstudied blind and low vision\ncan be easily integrated as a module into the visualization system,\"\n\"people’s preferences when exploring interactive 3D printed models\nmore applications can be explored in the future.\"\n\"(I3Ms). However,\nthere have been no systems that support NLI\"\n\"12\nCONCLUSION\"\nfor data visualization in an immersive way. A related work for\n\"reference is that Lee et al.\n[125] proposed the concept of data\nThe past\ntwo decades have witnessed the rapid development of\"\n\"visceralization and introduced a conceptual pipeline. It would be\nvisualization-oriented natural\nlanguage interfaces, which act as\"\n\"interesting to enrich this pipeline with the integration of V-NLI.\na complementary input modality for visual analytics. However,\"\n\"Besides, as described in Section 7, view transformations are rarely\nthe community lacks a comprehensive survey of\nrelated works\"\n\"involved in existing systems. With immersive technologies, more\nto guide follow-up research. We ﬁll\nthe gap, and the resulting\"\n\"view transformations can be explored and integrated into V-NLI to\nsurvey gives a comprehensive overview of what characteristics are\"\n\"provide an immersive interactive experience.\ncurrently concerned and supported by V-NLI. We also propose\"\n\"several promising directions for future work. To our knowledge,\"\n\"11.5\nDataset\"\nthis paper is the ﬁrst step towards reviewing V-NLI in a novel and\n\"There\nis\na widely recognized consensus\nthat\nlarge-scale data\nsystematic manner. We hope that\nthis paper can better guide future\"\n\"collection is an effective way to facilitate community development\nresearch and encourage the community to think further about NLI\"\n\"(e.g.,\nImageNet\n[47]\nfor\nimage processing and GEM [66]\nfor\nfor data visualization.\"\nnatural language generation). Although there are various datasets\nACKNOWLEDGMENTS\n\"for general NLP tasks,\nthey can hardly be directly applied to\"\n\"The\nauthors would\nlike\nto\nthank\nall\nthe\nreviewers\nfor\ntheir\"\nprovide training samples for V-NLI models. In the visualization\n\"valuable comments. They also thank all\nthe authors of\nrelated\"\n\"community,\nseveral works have begun to collect\nlarge datasets\"\n\"papers, especially those who kindly provide images covered in this\"\n\"for V-NLI, as\nshown in Table 8. To assist\nthe deployment of\"\nsurvey. The work was supported by the National Natural Science\n\"learning-based techniques for parsing human language, Fu et al.\"\nFoundation of China (No. 71690231) and Beijing Key Laboratory\n\"[61] proposed Quda, containing diverse user queries annotated\"\nof Industrial Bigdata System and Application.\n\"with analytic tasks. Srinivasan et al.\n[228] conducted an online\"\n\"study to collect NL utterances and characterized them based on\nREFERENCES\"\ntheir phrasing type. VizNet [85] is a step forward in addressing the\n\"[1]\nApache OpenNLP. http://opennlp.apache.org/.\"\nneed for large-scale data and visualization repositories. Kim et al.\n\"[2]\nAsk data. https://www.tableau.com/products/new-features/ask-data.\"\n\"[3]\nGoogle NLP. https://cloud.google.com/natural-language/.\n[110] collected questions people posed about various bar charts\"\n\"IBM Watson Analytics. http://www.ibm. com/analytics/watson-analytics.\n[4]\"\n\"and line charts, along with their answers and explanations to the\"\n\"[5]\nMicrosoft Power BI. https://docs.microsoft.com/en-us/power-bi/create-\"\n\"questions. Fully considering the user’s characteristics, Kassel et al.\"\nreports/power-bi-tutorial-q-and-a.\n\"[104] introduced a linguistically motivated two-dimensional answer\n[6]\nK. Affolter, K. Stockinger, and A. Bernstein. A comparative survey of\"\n\"recent natural\nlanguage interfaces for databases. VLDB J., 28(5), 2019.\nspace that varies in the level of both support and information to\"\n\"A. Akbik, D. Blythe, and R. Vollgraf. Contextual String Embeddings for\n[7]\"\nmatch the human language in visual analysis. The major limitation\n\"Sequence Labeling.\nIn Proc. COLING’19. ACM, 2018.\"\n\"of\nthem is\nthat\nthe query types contained are not\nrich enough.\"\n\"[8]\nR. A. Al-Zaidy and C. L. Giles. Automatic Extraction of Data from Bar\"\n\"Charts.\nIn Proc. ICKC’15. ACM, 2015.\nBesides, unfortunately, a benchmark or the ground truth of a given\"\n\"[9]\nR. Amar, J. Eagan, and J. Stasko. Low-level components of analytic\"\n\"dataset\nis usually unavailable. Although Srinivasan et al.\n[228]\"\n\"activity in information visualization.\nIn Proc. INFOVIS’05. IEEE, 2005.\"\n\"made an initial attempt\nto present a collection of NL utterances\"\n\"[10]\nF. Bacci, F. M. Cau, and L. D. Spano.\nInspecting Data Using Natural\"\n\"with the mapping visualizations, Luo et al.\n[151]\nsynthesized\nLanguage Queries. Lect. Notes Comput. Sci., 12254:771–782, 2020.\"\n\"[11]\nB. Bach, Z. Wang, M. Farinella, and et al. Design Patterns for Data\nNL-to-Vis benchmarks by piggybacking NL-to-SQL benchmarks\"\n\"Comics.\nIn Proc. CHI’18. ACM, 2018.\"\n\"and produced a NL-to-Vis benchmark, nvBench,\nthe supporting\"\n\"[12]\nD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by\"\n\"visualization types and tasks are limited. Therefore, collecting\"\n\"jointly learning to align and translate.\nIn Proc. ICLR’15, 2015.\"\n\"large-scale datasets and creating new benchmarks\nthat\nsupport\n[13]\nC. Baik, H. V. Jagadish, and Y. Li. Bridging the semantic gap with SQL\"\n\"query logs in natural language interfaces to databases.\nIn Proc. ICDE’19.\"\n\"more tasks, domains, and interactions will be an indispensable\"\n\"IEEE, 2019.\"\nresearch direction in the future.\n\"[14]\nF. Basik, B. H¨attasch, A.\nIlkhechi, and et al. DBPal: A learned NL-\"\n\"interface for databases.\nIn Proc. SIGMOD’18. ACM, 2018.\"\n\"11.6\nApplication\"\n\"[15]\nH. Bast and E. Haussmann. More Accurate Question Answering on\"\n\"Freebase.\nIn Proc. CIKM’15. ACM, 2015.\"\n\"Apart from facilitating data analysis, much attention has been paid\"\n\"[16]\nL. Battle, R. J. Crouser, A. Nakeshimana, and et al. The Role of Latency\"\n\"to beneﬁt\nreal-world applications by integrating visual analysis\"\n\"IEEE Trans.\nand Task Complexity in Predicting Visual Search Behavior.\"\n\"and natural\nlanguage\ninterface. Huang et al.\n[88] proposed a\nVis. Comput. Graph., 26(1):1246–1255, 2020.\"\n\"[17]\nL. Battle and C. Scheidegger. A Structured Review of Data Management\"\n\"query engine to convert, store and retrieve spatial uncertain mobile\"\n\"IEEE Trans. Vis.\nTechnology for Interactive Visualization and Analysis.\"\ntrajectories via intuitive NL input. Leo John et al. [133] proposed a\n\"Comput. Graph., 27(2):1128–1138, 2021.\"\n\"novel V-NLI to promote medical\nimaging and biomedical research.\n[18]\nY\n. Belinkov and J. Glass.\nAnalysis Methods\nin Neural Language\"\n\"Processing: A Survey. Trans. Assoc. Comput. Linguist., 7:49–72, 2019.\nWith an audio travel podcast as input, Crosscast [266] identiﬁes\"\n\"[19]\nS. Bergamaschi, F. Guerra, M.\nInterlandi, and et al. Combining user\"\ngeographic locations and descriptive keywords within the podcast\n\"and database perspective for solving keyword queries over\nrelational\"\ntranscript through NLP and text mining techniques. The information\n\"databases.\nInf. Syst., 55:1–19, 2016.\"\n\"is\nlater used to select\nrelevant photos\nfrom online repositories\n[20]\nS. Bieliauskas and A. Schreiber. A Conversational User Interface for\"\n\"Software Visualization.\nIn Proc. VISSOFT’17. IEEE, 2017.\nand synchronize their display to align with the audio narration.\"\n\"[21]\nS. Bird. NLTK:\nthe natural\nlanguage toolkit.\nIn Proc. COLING-ACL’06.\"\nMeetingVis [214] leverages ASR and NLP techniques to promote\n\"ACL, 2006.\"\neffective meeting summaries in team-based workplaces. PathViewer\n\"[22]\nL. Blunschi, C. Jossen, D. Kossmann, and et al. SODA: Generating SQL\"\n\"for business users. Proc. VLDB Endow., 5(10):932–943, 2012.\n[255] leverages ideas from ﬂow diagrams and NLP to visualize the\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 16,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n16\n[23]\nM. Bostock, V. Ogievetsky, and J. Heer. D3: Data-Driven Documents.\nIEEE Trans. Vis. Comput. Graph., 17(12):2301–2309, 2011.\n[24]\nM. Brehmer and T. Munzner. A Multi-Level Typology of Abstract\nVisualization Tasks. IEEE Trans. Vis. Comput. Graph., 19(12), 2013.\n[25]\nT. Brown, B. Mann, N. Ryder, and et al. Language Models are Few-Shot\nLearners. In Proc. NeurIPS’20. MIT Press, 2020.\n[26]\nC. Bryan, K. L. Ma, and J. Woodring. Temporal Summary Images:\nAn Approach to Narrative Visualization via Interactive Annotation\nGeneration and Placement. IEEE Trans. Vis. Comput. Graph., 2017.\n[27]\nZ. Bylinskii, S. Alsheikh, S. Madan, and et al.\nUnderstanding\nInfographics through Textual and Visual Tag Prediction. arXiv, 2017.\n[28]\nS. Card, J. Mackinlay, and B. Shneiderman. Readings in Information\nVisualization: Using Vision to Think. Morgan Kaufmann, 1999.\n[29]\nA. Celikyilmaz, E. Clark, and J. Gao. Evaluation of Text Generation: A\nSurvey. arxiv, pages 1–75, 2021.\n[30]\nR. Chaudhry, S. Shekhar, U. Gupta, and et al. LEAF-QA: Locate, encode\nattend for ﬁgure question answering. In Proc. WACV’20, 2020.\n[31]\nJ. Chen, E. Jim´enez-Ruiz, I. Horrocks, and C. Sutton.\nColNet:\nEmbedding the Semantics of Web Tables for Column Type Prediction.\nIn Proc. AAAI’19. AAAI, 2019.\n[32]\nS. Chen, J. Li, G. Andrienko, and et al. Supporting Story Synthesis:\nBridging the Gap between Visual Analytics and Storytelling. IEEE Trans.\nVis. Comput. Graph., 26(7):2499–2516, 2020.\n[33]\nY. Chen, S. Barlowe, and J. Yang. Click2Annotate: Automated Insight\nExternalization with rich semantics. In Proc. VAST’10. IEEE, 2010.\n[34]\nY. Chen, J. Yang, S. Barlowe, and D. H. Jeong.\nTouch2Annotate:\nGenerating better annotations with less human effort on multi-touch\ninterfaces. In Proc. CHI’10. ACM, 2010.\n[35]\nZ. Chen, W. Tong, Q. Wang, and et al. Augmenting Static Visualizations\nwith PapARVis Designer. In Proc. CHI’20. ACM, 2020.\n[36]\nZ. Chen, Y. Wang, Q. Wang, and et al. Towards automated infographic\ndesign: Deep learning-based auto-extraction of extensible timeline. IEEE\nTrans. Vis. Comput. Graph., 26(1):917–926, 2020.\n[37]\nJ. Choo, C. Lee, H. Kim, and et al.\nVisIRR: Visual analytics for\ninformation retrieval and recommendation with large-scale document\ndata. In Proc. VAST’14. IEEE, 2014.\n[38]\nK. Cook, N. Cramer, D. Israel, and et al. Mixed-initiative visual analytics\nusing task-driven recommendations. In Proc. VAST’15. IEEE, 2015.\n[39]\nM. Corio and G. Lapalme. Generation of texts for information graphics.\nIn Proc. EWNLG’99. ACL, 1999.\n[40]\nK. Cox, R. E. Grinter, S. L. Hibino, and et al. A multi-modal natural\nlanguage interface to an information visualization environment. Int. J.\nSpeech Technol., 4(3):297–314, 2001.\n[41]\nW. Cui, X. Zhang, Y. Wang, and et al. Text-to-Viz: Automatic Generation\nof Infographics from Proportion-Related Natural Language Statements.\nIEEE Trans. Vis. Comput. Graph., 26(1):906–916, 2020.\n[42]\nZ. Cui, S. K. Badam, M. A. Yalc¸in, and N. Elmqvist.\nDataSite:\nProactive visual data exploration with computation of insight-based\nrecommendations. Inf. Vis., 18(2):251–267, 2019.\n[43]\nR. Dabre, C. Chu, and A. Kunchukuttan. A Survey of Multilingual\nNeural Machine Translation. ACM Comput. Surv., 53(5), 2020.\n[44]\nS. Demir, S. Carberry, and K. F. McCoy. Generating textual summaries\nof bar charts. In Proc. INLG’08. ACL, 2008.\n[45]\nS. Demir, S. Carberry, and K. F. McCoy. Summarizing Information\nGraphics Textually. Comput. Linguist., 38(3):527–574, 2012.\n[46]\nC¸ . Demiralp, P. J. Haas, S. Parthasarathy, and T. Pedapati. Foresight:\nRecommending visual insights. VLDB Endow., 10(12):1937–1940, 2017.\n[47]\nJ. Deng, W. Dong, R. Socher, and et al.\nImageNet: A large-scale\nhierarchical image database. In Proc. CVPR’09. IEEE, 2009.\n[48]\nA. P. Deshpande and C. N. Mahender. Summarization of Graph Using\nQuestion Answer Approach. In Adv. Intell. Syst. Comput., pages 205–216.\nSpringer, 2020.\n[49]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding. In\nProc. NAACL’19. ACL, 2019.\n[50]\nK. Dhamdhere, K. S. McCurley, R. Nahmias, and et al.\nAnalyza:\nExploring data with conversation. In Proc. IUI’17. ACM, 2017.\n[51]\nV. Dibia and C. Demiralp. Data2Vis: Automatic Generation of Data\nVisualizations Using Sequence-to-Sequence Recurrent Neural Networks.\nIEEE Comput. Graph. Appl., 39(5):33–46, 2019.\n[52]\nE. Dimara and C. Perin. What is Interaction for Data Visualization?\nIEEE Trans. Vis. Comput. Graph., 26(1):119–129, 2020.\n[53]\nR. Ding, S. Han, Y. Xu, and et al. Quickinsights: Quick and automatic\ndiscovery of insights from multi-dimensional data. In Proc. SIGMOD’19.\nACM, 2019.\n[54]\nI. K. Duncan, S. Tingsheng, S. T. Perrault, and M. T. Gastner. Task-\nBased Effectiveness of Interactive Contiguous Area Cartograms. IEEE\nTrans. Vis. Comput. Graph., 27(3):2136–2152, 2021.\n[55]\nJ. Eisenschlos, S. Krichene, and T. M¨uller. Understanding tables with\nintermediate pre-training. In Find. Assoc. Comput. Linguist. EMNLP\n2020. ACL, 2020.\n[56]\nB. Ens, B. Bach, M. Cordeil, and et al. Grand Challenges in Immersive\nAnalytics. In Proc. CHI’21. ACM, 2021.\n[57]\nM. Fasciano and G. Lapalme. PostGraphe: a system for the generation of\nstatistical graphics and text Important factors in the generation process.\nIn Proc. INLG’96. ACL, 1996.\n[58]\nE. Fast, B. Chen, J. Mendelsohn, and et al. Iris: A conversational agent\nfor complex tasks. In Proc. CHI’18. ACM, 2018.\n[59]\nL. Ferres, G. Lindgaard, and L. Sumegi. Evaluating a tool for improving\naccessibility to charts and graphs. In Proc. ASSETS’10. ACM, 2010.\n[60]\nA. Fonnet and Y. Prie. Survey of Immersive Analytics. IEEE Trans. Vis.\nComput. Graph., 27(3):2101–2122, 2019.\n[61]\nS. Fu, K. Xiong, X. Ge, and et al. Quda: Natural Language Queries for\nVisual Data Analytics. arXiv.\n[62]\nJ. Fulda, M. Brehmel, and T. Munzner. TimeLineCurator: Interactive\nAuthoring of Visual Timelines from Unstructured Text. IEEE Trans. Vis.\nComput. Graph., 22(1):300–309, 2016.\n[63]\nT. Gao, M. Dontcheva, E. Adar, and et al. Datatone: Managing ambiguity\nin natural language interfaces for data visualization. In Proc. UIST’15.\nACM, 2015.\n[64]\nT. Gao, J. Hullman, E. Adar, and et al. NewsViews: An automated\npipeline for creating custom geovisualizations for news. In Proc. CHI’14.\nACM, 2014.\n[65]\nT. Ge, B. Lee, and Y. Wang. CAST: Authoring Data-Driven Chart\nAnimations. In Proc. CHI’21. ACM, 2021.\n[66]\nS. Gehrmann, T. Adewumi, K. Aggarwal, and et al.\nThe GEM\nBenchmark: Natural Language Generation, its Evaluation and Metrics.\nIn Proc. GEM’21, pages 96–120. ACL, 2021.\n[67]\nA. Ghosh, M. Nashaat, J. Miller, and et al. A comprehensive review\nof tools for exploratory analysis of tabular industrial datasets.\nVis.\nInformatics, 2(4):235–253, 2018.\n[68]\nM. Gingerich and C. Conati.\nConstructing models of user and\ntask characteristics from eye gaze data for user-adaptive information\nhighlighting. In Proc. AAAI’15. AAAI, 2015.\n[69]\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, and et al.\nGenerative\nAdversarial Nets. In Proc. NIPS’14. MIT Press, 2014.\n[70]\nD. Gotz and Z. Wen. Behavior-driven visualization recommendation. In\nProc. IUI’09. ACM, 2009.\n[71]\nB. J. Grosz and C. L. Sidner. Attention, intentions, and the structure of\ndiscourse. Comput. Linguist., 12(3):175–204, 1986.\n[72]\nJ. Gu, Z. Lu, H. Li, and V. O. Li. Incorporating Copying Mechanism in\nSequence-to-Sequence Learning. In Proc. ACL’16. ACL, 2016.\n[73]\nJ. Guo, Z. Zhan, Y. Gao, and et al. Towards Complex Text-to-SQL\nin Cross-Domain Database with Intermediate Representation. In Proc.\nACL’19. ACL, 2019.\n[74]\nI. Gur, S. Yavuz, Y. Su, and X. Yan. DialSQL: Dialogue Based Structured\nQuery Generation. In Proc. ACL’18. ACL, 2018.\n[75]\nC. Harris, R. A. Rossi, S. Malik, and et al. Insight-centric Visualization\nRecommendation. arXiv, 2021.\n[76]\nK. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image\nRecognition. In IEEE, editor, Proc. CVPR’16. IEEE, 2016.\n[77]\nM. Hearst and M. Tory.\nWould You Like A Chart With That?\nIncorporating Visualizations into Conversational Interfaces. In Proc.\nVIS’19. IEEE, 2019.\n[78]\nM. Hearst, M. Tory, and V. Setlur. Toward Interface Defaults for Vague\nModiﬁers in Natural Language Interfaces for Visual Analysis. In Proc.\nVIS’19. IEEE, 2019.\n[79]\nR. Henkin and C. Turkay. Words of Estimative Correlation: Studying\nVerbalizations of Scatterplots. IEEE Trans. Vis. Comput. Graph., 2020.\n[80]\nJ. Herzig, P. K. Nowak, T. M¨uller, and et al. TaPas: Weakly Supervised\nTable Parsing via Pre-training. In Proc. ACL’20. ACL, 2020.\n[81]\nM. Honnibal and I. Montani. spacy 2: Natural language understanding\nwith Bloom embeddings.\nConvolutional Neural Networks Increm.\nParsing, 2017.\n[82]\nA. K. Hopkins, M. Correll, and A. Satyanarayan. VisuaLint: Sketchy In\nSitu Annotations of Chart Construction Errors. Comput. Graph. Forum,\n39(3):219–228, 2020.\n[83]\nE. Hoque, V. Setlur, M. Tory, and I. Dykeman. Applying Pragmatics\nPrinciples for Interaction with Visual Analytics. IEEE Trans. Vis. Comput.\nGraph., 24(1):309–318, 2018.\n[84]\nK. Hu, M. A. Bakker, S. Li, and et al. VizML: A machine learning\napproach to visualization recommendation. In Proc. CHI’19. ACM.\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 16\n[23] M.Bostock,V.Ogievetsky,andJ.Heer. D3:Data-DrivenDocuments. [54] I.K.Duncan,S.Tingsheng,S.T.Perrault,andM.T.Gastner. Task-\nIEEETrans.Vis.Comput.Graph.,17(12):2301–2309,2011. BasedEffectivenessofInteractiveContiguousAreaCartograms. IEEE\n[24] M. Brehmer and T. Munzner. A Multi-Level Typology of Abstract Trans.Vis.Comput.Graph.,27(3):2136–2152,2021.\nVisualizationTasks. IEEETrans.Vis.Comput.Graph.,19(12),2013. [55] J.Eisenschlos,S.Krichene,andT.Mu¨ller. Understandingtableswith\n[25] T.Brown,B.Mann,N.Ryder,andetal.LanguageModelsareFew-Shot intermediatepre-training. InFind.Assoc.Comput.Linguist.EMNLP\nLearners. InProc.NeurIPS’20.MITPress,2020. 2020.ACL,2020.\n[26] C. Bryan, K. L. Ma, and J. Woodring. Temporal Summary Images: [56] B.Ens,B.Bach,M.Cordeil,andetal. GrandChallengesinImmersive\nAn Approach to Narrative Visualization via Interactive Annotation Analytics. InProc.CHI’21.ACM,2021.\nGenerationandPlacement. IEEETrans.Vis.Comput.Graph.,2017. [57] M.FascianoandG.Lapalme.PostGraphe:asystemforthegenerationof\n[27] Z. Bylinskii, S. Alsheikh, S. Madan, and et al. Understanding statisticalgraphicsandtextImportantfactorsinthegenerationprocess.\nInfographicsthroughTextualandVisualTagPrediction. arXiv,2017. InProc.INLG’96.ACL,1996.\n[58] E.Fast,B.Chen,J.Mendelsohn,andetal. Iris:Aconversationalagent\n[28] S.Card,J.Mackinlay,andB.Shneiderman. ReadingsinInformation\nforcomplextasks. InProc.CHI’18.ACM,2018.\nVisualization:UsingVisiontoThink. MorganKaufmann,1999.\n[59] L.Ferres,G.Lindgaard,andL.Sumegi. Evaluatingatoolforimproving\n[29] A.Celikyilmaz,E.Clark,andJ.Gao. EvaluationofTextGeneration:A\naccessibilitytochartsandgraphs. InProc.ASSETS’10.ACM,2010.\nSurvey. arxiv,pages1–75,2021.\n[60] A.FonnetandY.Prie. SurveyofImmersiveAnalytics. IEEETrans.Vis.\n[30] R.Chaudhry,S.Shekhar,U.Gupta,andetal.LEAF-QA:Locate,encode\nComput.Graph.,27(3):2101–2122,2019.\nattendforfigurequestionanswering. InProc.WACV’20,2020.\n[61] S.Fu,K.Xiong,X.Ge,andetal. Quda:NaturalLanguageQueriesfor\n[31] J. Chen, E. Jime´nez-Ruiz, I. Horrocks, and C. Sutton. ColNet:\nVisualDataAnalytics. arXiv.\nEmbeddingtheSemanticsofWebTablesforColumnTypePrediction.\n[62] J.Fulda,M.Brehmel,andT.Munzner. TimeLineCurator:Interactive\nInProc.AAAI’19.AAAI,2019.\nAuthoringofVisualTimelinesfromUnstructuredText. IEEETrans.Vis.\n[32] S.Chen,J.Li,G.Andrienko,andetal. SupportingStorySynthesis:\nComput.Graph.,22(1):300–309,2016.\nBridgingtheGapbetweenVisualAnalyticsandStorytelling.IEEETrans.\n[63] T.Gao,M.Dontcheva,E.Adar,andetal.Datatone:Managingambiguity\nVis.Comput.Graph.,26(7):2499–2516,2020.\ninnaturallanguageinterfacesfordatavisualization. InProc.UIST’15.\n[33] Y.Chen,S.Barlowe,andJ.Yang. Click2Annotate:AutomatedInsight ACM,2015.\nExternalizationwithrichsemantics. InProc.VAST’10.IEEE,2010. [64] T. Gao, J. Hullman, E. Adar, and et al. NewsViews: An automated\n[34] Y. Chen, J. Yang, S. Barlowe, and D. H. Jeong. Touch2Annotate: pipelineforcreatingcustomgeovisualizationsfornews.InProc.CHI’14.\nGenerating better annotations with less human effort on multi-touch ACM,2014.\ninterfaces. InProc.CHI’10.ACM,2010. [65] T. Ge, B. Lee, and Y. Wang. CAST: Authoring Data-Driven Chart\n[35] Z.Chen,W.Tong,Q.Wang,andetal.AugmentingStaticVisualizations Animations. InProc.CHI’21.ACM,2021.\nwithPapARVisDesigner. InProc.CHI’20.ACM,2020. [66] S. Gehrmann, T. Adewumi, K. Aggarwal, and et al. The GEM\n[36] Z.Chen,Y.Wang,Q.Wang,andetal. Towardsautomatedinfographic Benchmark:NaturalLanguageGeneration,itsEvaluationandMetrics.\ndesign:Deeplearning-basedauto-extractionofextensibletimeline.IEEE InProc.GEM’21,pages96–120.ACL,2021.\nTrans.Vis.Comput.Graph.,26(1):917–926,2020. [67] A.Ghosh,M.Nashaat,J.Miller,andetal. Acomprehensivereview\n[37] J. Choo, C. Lee, H. Kim, and et al. VisIRR: Visual analytics for of tools for exploratory analysis of tabular industrial datasets. Vis.\ninformationretrievalandrecommendationwithlarge-scaledocument Informatics,2(4):235–253,2018.\ndata. InProc.VAST’14.IEEE,2014. [68] M. Gingerich and C. Conati. Constructing models of user and\n[38] K.Cook,N.Cramer,D.Israel,andetal.Mixed-initiativevisualanalytics taskcharacteristicsfromeyegazedataforuser-adaptiveinformation\nusingtask-drivenrecommendations. InProc.VAST’15.IEEE,2015. highlighting. InProc.AAAI’15.AAAI,2015.\n[39] M.CorioandG.Lapalme. Generationoftextsforinformationgraphics. [69] I. Goodfellow, J. Pouget-Abadie, M. Mirza, and et al. Generative\nInProc.EWNLG’99.ACL,1999. AdversarialNets. InProc.NIPS’14.MITPress,2014.\n[40] K.Cox,R.E.Grinter,S.L.Hibino,andetal. Amulti-modalnatural [70] D.GotzandZ.Wen. Behavior-drivenvisualizationrecommendation. In\nlanguageinterfacetoaninformationvisualizationenvironment. Int.J. Proc.IUI’09.ACM,2009.\nSpeechTechnol.,4(3):297–314,2001. [71] B.J.GroszandC.L.Sidner. Attention,intentions,andthestructureof\n[41] W.Cui,X.Zhang,Y.Wang,andetal.Text-to-Viz:AutomaticGeneration discourse. Comput.Linguist.,12(3):175–204,1986.\nofInfographicsfromProportion-RelatedNaturalLanguageStatements. [72] J.Gu,Z.Lu,H.Li,andV.O.Li. IncorporatingCopyingMechanismin\nIEEETrans.Vis.Comput.Graph.,26(1):906–916,2020. Sequence-to-SequenceLearning. InProc.ACL’16.ACL,2016.\n[42] Z. Cui, S. K. Badam, M. A. Yalc¸in, and N. Elmqvist. DataSite: [73] J. Guo, Z. Zhan, Y. Gao, and et al. Towards Complex Text-to-SQL\nProactive visual data exploration with computation of insight-based inCross-DomainDatabasewithIntermediateRepresentation. InProc.\nrecommendations. Inf.Vis.,18(2):251–267,2019. ACL’19.ACL,2019.\n[43] R. Dabre, C. Chu, and A. Kunchukuttan. A Survey of Multilingual [74] I.Gur,S.Yavuz,Y.Su,andX.Yan.DialSQL:DialogueBasedStructured\nNeuralMachineTranslation. ACMComput.Surv.,53(5),2020. QueryGeneration. InProc.ACL’18.ACL,2018.\n[75] C.Harris,R.A.Rossi,S.Malik,andetal. Insight-centricVisualization\n[44] S.Demir,S.Carberry,andK.F.McCoy. Generatingtextualsummaries\nRecommendation. arXiv,2021.\nofbarcharts. InProc.INLG’08.ACL,2008.\n[76] K.He,X.Zhang,S.Ren,andJ.Sun. DeepResidualLearningforImage\n[45] S.Demir,S.Carberry,and K.F.McCoy. SummarizingInformation\nRecognition. InIEEE,editor,Proc.CVPR’16.IEEE,2016.\nGraphicsTextually. Comput.Linguist.,38(3):527–574,2012.\n[77] M. Hearst and M. Tory. Would You Like A Chart With That?\n[46] C¸.Demiralp,P.J.Haas,S.Parthasarathy,andT.Pedapati. Foresight:\nIncorporating Visualizations into Conversational Interfaces. In Proc.\nRecommendingvisualinsights. VLDBEndow.,10(12):1937–1940,2017.\nVIS’19.IEEE,2019.\n[47] J. Deng, W. Dong, R. Socher, and et al. ImageNet: A large-scale\n[78] M.Hearst,M.Tory,andV.Setlur. TowardInterfaceDefaultsforVague\nhierarchicalimagedatabase. InProc.CVPR’09.IEEE,2009.\nModifiersinNaturalLanguageInterfacesforVisualAnalysis. InProc.\n[48] A.P.DeshpandeandC.N.Mahender. SummarizationofGraphUsing VIS’19.IEEE,2019.\nQuestionAnswerApproach.InAdv.Intell.Syst.Comput.,pages205–216.\n[79] R.HenkinandC.Turkay. WordsofEstimativeCorrelation:Studying\nSpringer,2020. VerbalizationsofScatterplots. IEEETrans.Vis.Comput.Graph.,2020.\n[49] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova. BERT:Pre-training [80] J.Herzig,P.K.Nowak,T.Mu¨ller,andetal. TaPas:WeaklySupervised\nofDeepBidirectionalTransformersforLanguageUnderstanding. In TableParsingviaPre-training. InProc.ACL’20.ACL,2020.\nProc.NAACL’19.ACL,2019. [81] M.HonnibalandI.Montani. spacy2:Naturallanguageunderstanding\n[50] K. Dhamdhere, K. S. McCurley, R. Nahmias, and et al. Analyza: with Bloom embeddings. Convolutional Neural Networks Increm.\nExploringdatawithconversation. InProc.IUI’17.ACM,2017. Parsing,2017.\n[51] V.DibiaandC.Demiralp. Data2Vis:AutomaticGenerationofData [82] A.K.Hopkins,M.Correll,andA.Satyanarayan. VisuaLint:SketchyIn\nVisualizationsUsingSequence-to-SequenceRecurrentNeuralNetworks. SituAnnotationsofChartConstructionErrors. Comput.Graph.Forum,\nIEEEComput.Graph.Appl.,39(5):33–46,2019. 39(3):219–228,2020.\n[52] E.DimaraandC.Perin. WhatisInteractionforDataVisualization? [83] E.Hoque,V.Setlur,M.Tory,andI.Dykeman. ApplyingPragmatics\nIEEETrans.Vis.Comput.Graph.,26(1):119–129,2020. PrinciplesforInteractionwithVisualAnalytics.IEEETrans.Vis.Comput.\n[53] R.Ding,S.Han,Y.Xu,andetal. Quickinsights:Quickandautomatic Graph.,24(1):309–318,2018.\ndiscoveryofinsightsfrommulti-dimensionaldata.InProc.SIGMOD’19. [84] K.Hu,M.A.Bakker,S.Li,andetal. VizML:Amachinelearning\nACM,2019. approachtovisualizationrecommendation. InProc.CHI’19.ACM.",
    "tables_extraction_text_csv": "0\n\"IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n16\"\n\"[23]\nM. Bostock, V. Ogievetsky, and J. Heer. D3: Data-Driven Documents.\nI. K. Duncan, S. Tingsheng, S. T. Perrault, and M. T. Gastner. Task-\n[54]\"\n\"IEEE\nIEEE Trans. Vis. Comput. Graph., 17(12):2301–2309, 2011.\nBased Effectiveness of Interactive Contiguous Area Cartograms.\"\n\"Trans. Vis. Comput. Graph., 27(3):2136–2152, 2021.\n[24]\nM. Brehmer and T. Munzner.\nA Multi-Level Typology of Abstract\"\n\"[55]\nJ. Eisenschlos, S. Krichene, and T. M¨uller. Understanding tables with\nVisualization Tasks.\nIEEE Trans. Vis. Comput. Graph., 19(12), 2013.\"\n\"intermediate pre-training.\nIn Find. Assoc. Comput. Linguist. EMNLP\n[25]\nT. Brown, B. Mann, N. Ryder, and et al. Language Models are Few-Shot\"\n\"2020. ACL, 2020.\nLearners.\nIn Proc. NeurIPS’20. MIT Press, 2020.\"\n\"[56]\nB. Ens, B. Bach, M. Cordeil, and et al. Grand Challenges in Immersive\"\n\"[26]\nC. Bryan, K. L. Ma, and J. Woodring.\nTemporal Summary Images:\"\n\"Analytics.\nIn Proc. CHI’21. ACM, 2021.\"\n\"An Approach to Narrative Visualization via\nInteractive Annotation\"\n\"[57]\nM. Fasciano and G. Lapalme. PostGraphe: a system for the generation of\nGeneration and Placement.\nIEEE Trans. Vis. Comput. Graph., 2017.\"\nstatistical graphics and text Important factors in the generation process.\n\"[27]\nZ. Bylinskii,\nS. Alsheikh,\nS. Madan,\nand\net\nal.\nUnderstanding\"\n\"In Proc. INLG’96. ACL, 1996.\"\n\"Infographics through Textual and Visual Tag Prediction. arXiv, 2017.\"\n\"[58]\nE. Fast, B. Chen, J. Mendelsohn, and et al.\nIris: A conversational agent\"\n\"[28]\nS. Card, J. Mackinlay, and B. Shneiderman. Readings in Information\"\n\"for complex tasks.\nIn Proc. CHI’18. ACM, 2018.\"\n\"Visualization: Using Vision to Think. Morgan Kaufmann, 1999.\"\n\"[59]\nL. Ferres, G. Lindgaard, and L. Sumegi. Evaluating a tool for improving\"\n\"[29]\nA. Celikyilmaz, E. Clark, and J. Gao. Evaluation of Text Generation: A\"\n\"accessibility to charts and graphs.\nIn Proc. ASSETS’10. ACM, 2010.\"\n\"Survey. arxiv, pages 1–75, 2021.\"\n\"IEEE Trans. Vis.\n[60]\nA. Fonnet and Y. Prie. Survey of Immersive Analytics.\"\n\"[30]\nR. Chaudhry, S. Shekhar, U. Gupta, and et al. LEAF-QA: Locate, encode\"\n\"Comput. Graph., 27(3):2101–2122, 2019.\"\n\"attend for ﬁgure question answering.\nIn Proc. WACV’20, 2020.\"\n\"[61]\nS. Fu, K. Xiong, X. Ge, and et al. Quda: Natural Language Queries for\"\n\"[31]\nJ. Chen, E.\nJim´enez-Ruiz,\nI. Horrocks,\nand C.\nSutton.\nColNet:\"\nVisual Data Analytics. arXiv.\nEmbedding the Semantics of Web Tables for Column Type Prediction.\n\"[62]\nJ. Fulda, M. Brehmel, and T. Munzner. TimeLineCurator: Interactive\"\n\"In Proc. AAAI’19. AAAI, 2019.\"\n\"IEEE Trans. Vis.\nAuthoring of Visual Timelines from Unstructured Text.\"\n\"[32]\nS. Chen, J. Li, G. Andrienko, and et al.\nSupporting Story Synthesis:\"\n\"Comput. Graph., 22(1):300–309, 2016.\"\nBridging the Gap between Visual Analytics and Storytelling. IEEE Trans.\n\"[63]\nT. Gao, M. Dontcheva, E. Adar, and et al. Datatone: Managing ambiguity\"\n\"Vis. Comput. Graph., 26(7):2499–2516, 2020.\"\n\"in natural language interfaces for data visualization.\nIn Proc. UIST’15.\"\n\"[33]\nY\n. Chen, S. Barlowe, and J. Yang. Click2Annotate: Automated Insight\"\n\"ACM, 2015.\"\n\"Externalization with rich semantics.\nIn Proc. VAST’10. IEEE, 2010.\"\n\"[64]\nT. Gao, J. Hullman, E. Adar, and et al. NewsViews: An automated\"\n\"[34]\nY\n. Chen,\nJ. Yang, S. Barlowe,\nand D. H.\nJeong.\nTouch2Annotate:\"\npipeline for creating custom geovisualizations for news. In Proc. CHI’14.\nGenerating better annotations with less human effort on multi-touch\n\"ACM, 2014.\"\n\"interfaces.\nIn Proc. CHI’10. ACM, 2010.\"\n\"[65]\nT. Ge, B. Lee, and Y. Wang.\nCAST: Authoring Data-Driven Chart\"\n\"[35]\nZ. Chen, W. Tong, Q. Wang, and et al. Augmenting Static Visualizations\nAnimations.\nIn Proc. CHI’21. ACM, 2021.\"\n\"with PapARVis Designer.\nIn Proc. CHI’20. ACM, 2020.\n[66]\nS. Gehrmann, T. Adewumi, K. Aggarwal,\nand\net\nal.\nThe GEM\"\n\"[36]\nZ. Chen, Y. Wang, Q. Wang, and et al. Towards automated infographic\nBenchmark: Natural Language Generation, its Evaluation and Metrics.\"\n\"design: Deep learning-based auto-extraction of extensible timeline. IEEE\nIn Proc. GEM’21, pages 96–120. ACL, 2021.\"\n\"Trans. Vis. Comput. Graph., 26(1):917–926, 2020.\n[67]\nA. Ghosh, M. Nashaat, J. Miller, and et al. A comprehensive review\"\n\"[37]\nJ. Choo, C. Lee, H. Kim,\nand et\nal.\nVisIRR: Visual\nanalytics\nfor\nVis.\nof\ntools\nfor exploratory analysis of\ntabular\nindustrial datasets.\"\n\"information retrieval and recommendation with large-scale document\nInformatics, 2(4):235–253, 2018.\"\n\"data.\nIn Proc. VAST’14. IEEE, 2014.\n[68]\nM. Gingerich\nand C. Conati.\nConstructing models\nof\nuser\nand\"\n\"task characteristics from eye gaze data for user-adaptive information\n[38]\nK. Cook, N. Cramer, D. Israel, and et al. Mixed-initiative visual analytics\"\n\"highlighting.\nIn Proc. AAAI’15. AAAI, 2015.\nusing task-driven recommendations.\nIn Proc. VAST’15. IEEE, 2015.\"\n\"[69]\nI. Goodfellow,\nJ. Pouget-Abadie, M. Mirza,\nand et\nal.\nGenerative\n[39]\nM. Corio and G. Lapalme. Generation of texts for information graphics.\"\n\"Adversarial Nets.\nIn Proc. NIPS’14. MIT Press, 2014.\nIn Proc. EWNLG’99. ACL, 1999.\"\n\"[70]\nD. Gotz and Z. Wen. Behavior-driven visualization recommendation.\nIn\n[40]\nK. Cox, R. E. Grinter, S. L. Hibino, and et al. A multi-modal natural\"\n\"Proc. IUI’09. ACM, 2009.\nInt. J.\nlanguage interface to an information visualization environment.\"\n\"[71]\nB. J. Grosz and C. L. Sidner. Attention,\nintentions, and the structure of\nSpeech Technol., 4(3):297–314, 2001.\"\n\"discourse. Comput. Linguist., 12(3):175–204, 1986.\"\n\"[41]\nW. Cui, X. Zhang, Y. Wang, and et al. Text-to-Viz: Automatic Generation\"\n\"[72]\nJ. Gu, Z. Lu, H. Li, and V. O. Li.\nIncorporating Copying Mechanism in\nof Infographics from Proportion-Related Natural Language Statements.\"\n\"Sequence-to-Sequence Learning.\nIn Proc. ACL’16. ACL, 2016.\nIEEE Trans. Vis. Comput. Graph., 26(1):906–916, 2020.\"\n\"[73]\nJ. Guo, Z. Zhan, Y. Gao, and et al.\nTowards Complex Text-to-SQL\"\n\"[42]\nZ. Cui, S. K. Badam, M. A. Yalc¸in,\nand N. Elmqvist.\nDataSite:\"\n\"in Cross-Domain Database with Intermediate Representation.\nIn Proc.\"\n\"Proactive visual data exploration with computation of\ninsight-based\"\n\"ACL’19. ACL, 2019.\"\n\"recommendations.\nInf. Vis., 18(2):251–267, 2019.\"\n\"[74]\nI. Gur, S. Yavuz, Y. Su, and X. Yan. DialSQL: Dialogue Based Structured\"\n\"[43]\nR. Dabre, C. Chu, and A. Kunchukuttan. A Survey of Multilingual\"\n\"Query Generation.\nIn Proc. ACL’18. ACL, 2018.\"\n\"Neural Machine Translation. ACM Comput. Surv., 53(5), 2020.\"\n\"[75]\nC. Harris, R. A. Rossi, S. Malik, and et al.\nInsight-centric Visualization\"\n\"[44]\nS. Demir, S. Carberry, and K. F. McCoy. Generating textual summaries\"\n\"Recommendation. arXiv, 2021.\"\n\"of bar charts.\nIn Proc. INLG’08. ACL, 2008.\"\n\"[76]\nK. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image\"\n\"[45]\nS. Demir, S. Carberry, and K. F. McCoy.\nSummarizing Information\"\n\"Recognition.\nIn IEEE, editor, Proc. CVPR’16. IEEE, 2016.\"\n\"Graphics Textually. Comput. Linguist., 38(3):527–574, 2012.\"\n\"[77]\nM. Hearst\nand M. Tory.\nWould You Like A Chart With That?\"\n\"[46]\nC\n¸ . Demiralp, P. J. Haas, S. Parthasarathy, and T. Pedapati. Foresight:\"\n\"Incorporating Visualizations into Conversational\nInterfaces.\nIn Proc.\"\n\"Recommending visual\ninsights. VLDB Endow., 10(12):1937–1940, 2017.\"\n\"VIS’19. IEEE, 2019.\"\n\"[47]\nJ. Deng, W. Dong, R. Socher, and et al.\nImageNet: A large-scale\"\n\"[78]\nM. Hearst, M. Tory, and V. Setlur. Toward Interface Defaults for Vague\"\n\"hierarchical\nimage database.\nIn Proc. CVPR’09. IEEE, 2009.\"\n\"Modiﬁers in Natural Language Interfaces for Visual Analysis.\nIn Proc.\"\n\"[48]\nA. P. Deshpande and C. N. Mahender. Summarization of Graph Using\"\n\"VIS’19. IEEE, 2019.\"\n\"Question Answer Approach. In Adv. Intell. Syst. Comput., pages 205–216.\"\n\"[79]\nR. Henkin and C. Turkay. Words of Estimative Correlation: Studying\"\n\"Springer, 2020.\"\n\"Verbalizations of Scatterplots.\nIEEE Trans. Vis. Comput. Graph., 2020.\"\n\"[49]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training\"\n\"[80]\nJ. Herzig, P. K. Nowak, T. M¨uller, and et al. TaPas: Weakly Supervised\"\n\"of Deep Bidirectional Transformers for Language Understanding.\nIn\"\n\"Table Parsing via Pre-training.\nIn Proc. ACL’20. ACL, 2020.\"\n\"Proc. NAACL’19. ACL, 2019.\"\n\"[81]\nM. Honnibal and I. Montani.\nspacy 2: Natural\nlanguage understanding\"\n\"[50]\nK. Dhamdhere, K. S. McCurley, R. Nahmias,\nand et\nal.\nAnalyza:\nConvolutional Neural Networks\nIncrem.\nwith Bloom embeddings.\"\n\"Exploring data with conversation.\nIn Proc. IUI’17. ACM, 2017.\nParsing, 2017.\"\n\"[51]\nV\n. Dibia and C. Demiralp. Data2Vis: Automatic Generation of Data\n[82]\nA. K. Hopkins, M. Correll, and A. Satyanarayan. VisuaLint: Sketchy In\"\n\"Visualizations Using Sequence-to-Sequence Recurrent Neural Networks.\nSitu Annotations of Chart Construction Errors. Comput. Graph. Forum,\"\n\"IEEE Comput. Graph. Appl., 39(5):33–46, 2019.\n39(3):219–228, 2020.\"\n\"[52]\nE. Dimara and C. Perin. What\nis Interaction for Data Visualization?\n[83]\nE. Hoque, V. Setlur, M. Tory, and I. Dykeman. Applying Pragmatics\"\n\"IEEE Trans. Vis. Comput. Graph., 26(1):119–129, 2020.\nPrinciples for Interaction with Visual Analytics. IEEE Trans. Vis. Comput.\"\n\"Graph., 24(1):309–318, 2018.\n[53]\nR. Ding, S. Han, Y. Xu, and et al. Quickinsights: Quick and automatic\"\n\"discovery of insights from multi-dimensional data.\nIn Proc. SIGMOD’19.\n[84]\nK. Hu, M. A. Bakker, S. Li, and et al. VizML: A machine learning\"\n\"ACM, 2019.\napproach to visualization recommendation.\nIn Proc. CHI’19. ACM.\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 17,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n17\n[85]\nK. Hu, S. S. Gaikwad, M. Hulsebos, and et al. VizNet: Towards a\nlarge-scale visualization learning and benchmarking repository. In Proc.\nCHI’19. ACM, 2019.\n[86]\nK. Hu, D. Orghian, and C. Hidalgo. DIVE: A mixed-initiative system\nsupporting integrated data exploration workﬂows. In Proc. HILDA’2018.\nACM, 2018.\n[87]\nB. Huang, G. Zhang, and P. C.-Y. Sheu. A Natural Language Database\nInterface Based on a Probabilistic Context Free Grammar. In Proc.\nWSCS’08. IEEE, 2008.\n[88]\nZ. Huang, Y. Zhao, W. Chen, and et al. A Natural-language-based Visual\nQuery Approach of Uncertain Human Trajectories. IEEE Trans. Vis.\nComput. Graph., 26(1):1–11, 2019.\n[89]\nJ. Hullman, N. Diakopoulos, and E. Adar. Contextiﬁer: Automatic\ngeneration of annotated stock visualizations. In Proc. CHI’13. ACM.\n[90]\nM. Hulsebos, A. Satyanarayan, and et al. Sherlock: A deep learning\napproach to semantic data type detection. In Proc. KDD’19. ACM, 2019.\n[91]\nR. Ingria, R. Sauri, J. Pustejovsky, and et al.\nTimeML: Robust\nSpeciﬁcation of Event and Temporal Expressions in Text. New Dir.\nQuest. answering, 3:28–34, 2003.\n[92]\nS. Iyer, I. Konstas, A. Cheung, and et al. Learning a Neural Semantic\nParser from User Feedback. In Proc. ACL’17. ACL, 2017.\n[93]\nQ. Jiang, G. Sun, Y. Dong, and R. Liang. DT2VIS: A Focus+Context\nAnswer Generation System to Facilitate Visual Exploration of Tabular\nData. IEEE Comput. Graph. Appl., 41(5):45–56, 2021.\n[94]\nS. Jiang, P. Kang, X. Song, and et al. Emerging Wearable Interfaces and\nAlgorithms for Hand Gesture Recognition: A Survey. IEEE Rev. Biomed.\nEng., pages 1–11, 2021.\n[95]\nM. Joshi, D. Chen, Y. Liu, and et al. SpanBERT: Improving Pre-training\nby Representing and Predicting Spans. Trans. Assoc. Comput. Linguist.,\n8:64–77, 2020.\n[96]\nM. Joshi, O. Levy, L. Zettlemoyer, and D. Weld. BERT for Coreference\nResolution: Baselines and Analysis. In Proc. EMNLP’19. ACL, 2019.\n[97]\nK. Kaﬂe, B. Price, S. Cohen, and C. Kanan. DVQA: Understanding Data\nVisualizations via Question Answering. In Proc. CVPR’18. IEEE, 2018.\n[98]\nS. E. Kahou, V. Michalski, A. Atkinson, and et al. FigureQA: An\nAnnotated Figure Dataset for Visual Reasoning. In Proc. ICLR’18, 2018.\n[99]\nS. Kandel, A. Paepcke, J. Hellerstein, and J. Heer. Wrangler: Interactive\nvisual speciﬁcation of data transformation scripts. In Proc. CHI’11.\nACM, 2011.\n[100] S. Kandel, R. Parikh, A. Paepcke, and et al. Proﬁler: Integrated statistical\nanalysis and visualization for data quality assessment. In Proc. AVI’12.\nACM, 2012.\n[101] E. Kandogan. Just-in-time annotation of clusters, outliers, and trends in\npoint-based data visualizations. In Proc. VAST’12. IEEE, 2012.\n[102] J. Kang, K. Condiff, S. Chang, and et al. Understanding How People\nUse Natural Language to Ask for Recommendations. In Proc. RecSys’17.\nACM, 2017.\n[103] J. F. Kassel and M. Rohs. Valletto: A multimodal interface for ubiquitous\nvisual analytics. In Proc. CHI EA’18. ACM, 2018.\n[104] J. F. Kassel and M. Rohs. Talk to me intelligibly: Investigating an answer\nspace to match the user’s language in visual analysis. In Proc. DIS’19.\nACM, 2019.\n[105] T. Kato, M. Matsushita, and E. Maeda. Answering it with charts: dialogue\nin natural language and charts. In Proc. COLING’02. ACM, 2002.\n[106] P. Kaur, M. Owonibi, and B. Koenig-Ries.\nTowards visualization\nrecommendation-a semi-automated domain-speciﬁc learning approach.\nCEUR Workshop Proc., 1366:30–35, 2015.\n[107] S. Kerpedjiev, G. Carenini, S. F. Roth, and J. D. Moore. AutoBrief:\na multimedia presentation system for assisting data analysis. Comput.\nStand. Interfaces, 18(6-7):583–593, 1997.\n[108] N. Kerracher and J. Kennedy. Constructing and Evaluating Visualisation\nTask Classiﬁcations: Process and Considerations. Comput. Graph. Forum,\n36(3):47–59, 2017.\n[109] A. Key, B. Howe, D. Perry, and C. Aragon. VizDeck: Self-organizing\ndashboards for visual analytics. In Proc. SIGMOD’12. ACM, 2012.\n[110] D. H. Kim, E. Hoque, and M. Agrawala. Answering Questions about\nCharts and Generating Visual Explanations. In Proc. CHI’20. ACM.\n[111] D. H. Kim, V. Setlur, and M. Agrawala. Towards Understanding How\nReaders Integrate Charts and Captions: A Case Study with Line Charts.\nIn Proc. CHI’21. ACM, 2021.\n[112] H. Kim, J. Oh, Y. Han, and et al. Thumbnails for Data Stories: A Survey\nof Current Practices. In Proc. VIS’19. IEEE, 2019.\n[113] Y. Kim and J. Heer. Assessing Effects of Task and Data Distribution\non the Effectiveness of Visual Encodings.\nComput. Graph. Forum,\n37(3):157–167, 2018.\n[114] Y. Kim and J. Heer. Gemini: A Grammar and Recommender System for\nAnimated Transitions in Statistical Graphics. IEEE Trans. Vis. Comput.\nGraph., 27(2):485–494, 2021.\n[115] Y.-H. Kim, B. Lee, A. Srinivasan, and E. K. Choe. Data@Hand: Fostering\nVisual Exploration of Personal Data on Smartphones Leveraging Speech\nand Touch Interaction. In Proc. CHI’21. ACM, 2021.\n[116] R. Kincaid and G. Pollock. Nicky: Toward a Virtual Assistant for Test\nand Measurement Instrument Recommendations. In Proc. ICSC’17.\nIEEE, 2017.\n[117] Y. Kirstain, O. Ram, and O. Levy. Coreference Resolution without Span\nRepresentations. arXiv, 2021.\n[118] V. Kocijan, A.-M. Cretu, O.-M. Camburu, and et al. A Surprisingly\nRobust Trick for the Winograd Schema Challenge. Proc. 57th Annu.\nMeet. Assoc. Comput. Linguist. ACL’19, pages 4837–4842, 2019.\n[119] N. Kong and M. Agrawala. Graphical Overlays: Using Layered Elements\nto Aid Chart Reading. IEEE Trans. Vis. Comput. Graph., 18(12), 2012.\n[120] A. Kumar, J. Aurisano, B. Di Eugenio, and et al. Multimodal Coreference\nResolution for Exploratory Data Visualization Dialogue: Context-Based\nAnnotation and Gesture Identiﬁcation. In Proc. SEMDIAL’17. ISCA.\n[121] A. Kumar, J. Aurisano, and et al. Towards a dialogue system that\nsupports rich visualizations of data. In Proc. SIGDIAL’16. ACL, 2016.\n[122] C. Lai, Z. Lin, R. Jiang, and et al. Automatic Annotation Synchronizing\nwith Textual Description for Visualization. In Proc. CHI’20. ACM, 2020.\n[123] S. Lalle, D. Toker, and C. Conati. Gaze-Driven Adaptive Interventions\nfor Magazine-Style Narrative Visualizations. IEEE Trans. Vis. Comput.\nGraph., 27(6):2941–2952, 2019.\n[124] R. Langner, M. Satkowski, W. B¨uschel, and R. Dachselt. MARVIS:\nCombining Mobile Devices and Augmented Reality for Visual Data\nAnalysis. In Proc. CHI’21. ACM, 2021.\n[125] B. Lee, D. Brown, B. Lee, and et al. Data Visceralization: Enabling\nDeeper Understanding of Data Using Virtual Reality. IEEE Trans. Vis.\nComput. Graph., 27(2):1095–1105, 2021.\n[126] B. Lee, P. Isenberg, N. H. Riche, and S. Carpendale. Beyond Mouse\nand Keyboard: Expanding Design Considerations for Information\nVisualization Interactions. IEEE Trans. Vis. Comput. Graph., 2012.\n[127] B. Lee, C. Plaisant, C. S. Parr, and et al. Task taxonomy for graph\nvisualization. In Proc. BELIV’06. ACM, 2006.\n[128] B. Lee, A. Srinivasan, P. Isenberg, and J. Stasko. Post-wimp interaction\nfor information visualization. Found. Trends Human-Computer Interact.,\n14(1):1–95, 2021.\n[129] D. J. L. Lee, A. Quamar, E. Kandogan, and F. ¨Ozcan. Boomerang:\nProactive Insight-Based Recommendations for Guiding Conversational\nData Analysis. In Proc. SIGMOD’21 demo, 2021.\n[130] D. J.-L. Lee, V. Setlur, M. Tory, and et al. Deconstructing Categorization\nin Visualization Recommendation: A Taxonomy and Comparative Study.\nIEEE Trans. Vis. Comput. Graph., 2626:1–14, 2021.\n[131] K. Lee, L. He, M. Lewis, and L. Zettlemoyer.\nEnd-to-end Neural\nCoreference Resolution. In Proc. EMNLP’17. ACL, 2017.\n[132] K. Lee, L. He, and L. Zettlemoyer. Higher-Order Coreference Resolution\nwith Coarse-to-Fine Inference. In Proc. NAACL’18. ACL, 2018.\n[133] R. J. Leo John, J. M. Patel, A. L. Alexander, and et al. A Natural\nLanguage Interface for Dissemination of Reproducible Biomedical Data\nScience. Lect. Notes Comput. Sci., 11073:197–205, 2018.\n[134] R. J. Leo John, N. Potti, and J. M. Patel. Ava: From data to insights\nthrough conversation. In Proc. CIDR’17, 2017.\n[135] F. Li and H. V. Jagadish.\nNaLIR: An interactive natural language\ninterface for querying relational databases. In Proc. SIGMOD’14. ACM.\n[136] F. Li and H. V. Jagadish. Constructing an interactive natural language\ninterface for relational databases. Proc. VLDB Endow., 8(1):73–84, 2014.\n[137] F. Li and H. V. Jagadish. Understanding Natural Language Queries over\nRelational Databases. ACM SIGMOD Rec., 45(1):6–13, 2016.\n[138] H. Li, Y. Wang, S. Zhang, and et al. KG4Vis: A Knowledge Graph-Based\nApproach for Visualization Recommendation. IEEE Trans. Vis. Comput.\nGraph., pages 1–11, 2021.\n[139] J. Li, J. Yang, A. Hertzmann, and et al. LayoutGAN: Generating Graphic\nLayouts with Wireframe Discriminators. arXiv, 2019.\n[140] L. H. Li, M. Yatskar, D. Yin, and et al. VisualBERT: A Simple and\nPerformant Baseline for Vision and Language. arXiv, 2019.\n[141] P. Li, L. Liu, J. Xu, and et al. Application of Hidden Markov Model in\nSQL Injection Detection. In Proc. COMPSAC’17. IEEE, 2017.\n[142] H. Lin, D. Moritz, and J. Heer. Dziban: Balancing Agency & Automation\nin Visualization Design via Anchored Recommendations.\nIn Proc.\nCHI’20. ACM, 2020.\n[143] C. Liu, Y. Han, R. Jiang, and X. Yuan.\nADVISor: Automatic\nVisualization Answer for Natural-Language Question on Tabular Data.\nIn Proc. PaciﬁcVis’21. IEEE, 2021.\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 17\n[85] K. Hu, S. S. Gaikwad, M. Hulsebos, and et al. VizNet: Towards a [114] Y.KimandJ.Heer. Gemini:AGrammarandRecommenderSystemfor\nlarge-scalevisualizationlearningandbenchmarkingrepository. InProc. AnimatedTransitionsinStatisticalGraphics. IEEETrans.Vis.Comput.\nCHI’19.ACM,2019. Graph.,27(2):485–494,2021.\n[86] K.Hu,D.Orghian,andC.Hidalgo. DIVE:Amixed-initiativesystem [115] Y.-H.Kim,B.Lee,A.Srinivasan,andE.K.Choe.Data@Hand:Fostering\nsupportingintegrateddataexplorationworkflows. InProc.HILDA’2018. VisualExplorationofPersonalDataonSmartphonesLeveragingSpeech\nACM,2018. andTouchInteraction. InProc.CHI’21.ACM,2021.\n[87] B.Huang,G.Zhang,andP.C.-Y.Sheu. ANaturalLanguageDatabase [116] R.KincaidandG.Pollock. Nicky:TowardaVirtualAssistantforTest\nInterface Based on a Probabilistic Context Free Grammar. In Proc. and Measurement Instrument Recommendations. In Proc. ICSC’17.\nWSCS’08.IEEE,2008. IEEE,2017.\n[88] Z.Huang,Y.Zhao,W.Chen,andetal.ANatural-language-basedVisual [117] Y.Kirstain,O.Ram,andO.Levy. CoreferenceResolutionwithoutSpan\nQueryApproachofUncertainHumanTrajectories. IEEETrans.Vis. Representations. arXiv,2021.\nComput.Graph.,26(1):1–11,2019. [118] V.Kocijan,A.-M.Cretu,O.-M.Camburu,andetal. ASurprisingly\n[89] J. Hullman, N. Diakopoulos, and E. Adar. Contextifier: Automatic RobustTrickfortheWinogradSchemaChallenge. Proc.57thAnnu.\ngenerationofannotatedstockvisualizations. InProc.CHI’13.ACM. Meet.Assoc.Comput.Linguist.ACL’19,pages4837–4842,2019.\n[90] M.Hulsebos,A.Satyanarayan,andetal. Sherlock:Adeeplearning [119] N.KongandM.Agrawala.GraphicalOverlays:UsingLayeredElements\napproachtosemanticdatatypedetection.InProc.KDD’19.ACM,2019. toAidChartReading. IEEETrans.Vis.Comput.Graph.,18(12),2012.\n[91] R. Ingria, R. Sauri, J. Pustejovsky, and et al. TimeML: Robust [120] A.Kumar,J.Aurisano,B.DiEugenio,andetal.MultimodalCoreference\nSpecification of Event and Temporal Expressions in Text. New Dir. ResolutionforExploratoryDataVisualizationDialogue:Context-Based\nQuest.answering,3:28–34,2003. AnnotationandGestureIdentification. InProc.SEMDIAL’17.ISCA.\n[92] S.Iyer,I.Konstas,A.Cheung,andetal. LearningaNeuralSemantic [121] A. Kumar, J. Aurisano, and et al. Towards a dialogue system that\nParserfromUserFeedback. InProc.ACL’17.ACL,2017. supportsrichvisualizationsofdata. InProc.SIGDIAL’16.ACL,2016.\n[93] Q.Jiang,G.Sun,Y.Dong,andR.Liang. DT2VIS:AFocus+Context [122] C.Lai,Z.Lin,R.Jiang,andetal. AutomaticAnnotationSynchronizing\nAnswerGenerationSystemtoFacilitateVisualExplorationofTabular withTextualDescriptionforVisualization.InProc.CHI’20.ACM,2020.\nData. IEEEComput.Graph.Appl.,41(5):45–56,2021. [123] S.Lalle,D.Toker,andC.Conati. Gaze-DrivenAdaptiveInterventions\nforMagazine-StyleNarrativeVisualizations. IEEETrans.Vis.Comput.\n[94] S.Jiang,P.Kang,X.Song,andetal. EmergingWearableInterfacesand\nAlgorithmsforHandGestureRecognition:ASurvey.IEEERev.Biomed.\nGraph.,27(6):2941–2952,2019.\nEng.,pages1–11,2021. [124] R.Langner,M.Satkowski,W.Bu¨schel,andR.Dachselt. MARVIS:\nCombining Mobile Devices and Augmented Reality for Visual Data\n[95] M.Joshi,D.Chen,Y.Liu,andetal. SpanBERT:ImprovingPre-training\nAnalysis. InProc.CHI’21.ACM,2021.\nbyRepresentingandPredictingSpans. Trans.Assoc.Comput.Linguist.,\n8:64–77,2020. [125] B.Lee,D.Brown,B.Lee,andetal. DataVisceralization:Enabling\nDeeperUnderstandingofDataUsingVirtualReality. IEEETrans.Vis.\n[96] M.Joshi,O.Levy,L.Zettlemoyer,andD.Weld. BERTforCoreference\nComput.Graph.,27(2):1095–1105,2021.\nResolution:BaselinesandAnalysis. InProc.EMNLP’19.ACL,2019.\n[126] B.Lee,P.Isenberg,N.H.Riche,andS.Carpendale. BeyondMouse\n[97] K.Kafle,B.Price,S.Cohen,andC.Kanan.DVQA:UnderstandingData\nand Keyboard: Expanding Design Considerations for Information\nVisualizationsviaQuestionAnswering. InProc.CVPR’18.IEEE,2018.\nVisualizationInteractions. IEEETrans.Vis.Comput.Graph.,2012.\n[98] S. E. Kahou, V. Michalski, A. Atkinson, and et al. FigureQA: An\n[127] B. Lee, C. Plaisant, C. S. Parr, and et al. Task taxonomy for graph\nAnnotatedFigureDatasetforVisualReasoning.InProc.ICLR’18,2018.\nvisualization. InProc.BELIV’06.ACM,2006.\n[99] S.Kandel,A.Paepcke,J.Hellerstein,andJ.Heer. Wrangler:Interactive\n[128] B.Lee,A.Srinivasan,P.Isenberg,andJ.Stasko. Post-wimpinteraction\nvisual specification of data transformation scripts. In Proc. CHI’11.\nforinformationvisualization. Found.TrendsHuman-ComputerInteract.,\nACM,2011.\n14(1):1–95,2021.\n[100] S.Kandel,R.Parikh,A.Paepcke,andetal.Profiler:Integratedstatistical [129] D. J. L. Lee, A. Quamar, E. Kandogan, and F. O¨zcan. Boomerang:\nanalysisandvisualizationfordataqualityassessment. InProc.AVI’12.\nProactiveInsight-BasedRecommendationsforGuidingConversational\nACM,2012.\nDataAnalysis. InProc.SIGMOD’21demo,2021.\n[101] E.Kandogan. Just-in-timeannotationofclusters,outliers,andtrendsin\n[130] D.J.-L.Lee,V.Setlur,M.Tory,andetal.DeconstructingCategorization\npoint-baseddatavisualizations. InProc.VAST’12.IEEE,2012.\ninVisualizationRecommendation:ATaxonomyandComparativeStudy.\n[102] J.Kang,K.Condiff,S.Chang,andetal. UnderstandingHowPeople IEEETrans.Vis.Comput.Graph.,2626:1–14,2021.\nUseNaturalLanguagetoAskforRecommendations.InProc.RecSys’17.\n[131] K. Lee, L. He, M. Lewis, and L. Zettlemoyer. End-to-end Neural\nACM,2017. CoreferenceResolution. InProc.EMNLP’17.ACL,2017.\n[103] J.F.KasselandM.Rohs.Valletto:Amultimodalinterfaceforubiquitous [132] K.Lee,L.He,andL.Zettlemoyer.Higher-OrderCoreferenceResolution\nvisualanalytics. InProc.CHIEA’18.ACM,2018. withCoarse-to-FineInference. InProc.NAACL’18.ACL,2018.\n[104] J.F.KasselandM.Rohs.Talktomeintelligibly:Investigatingananswer [133] R. J. Leo John, J. M. Patel, A. L. Alexander, and et al. A Natural\nspacetomatchtheuser’slanguageinvisualanalysis. InProc.DIS’19. LanguageInterfaceforDisseminationofReproducibleBiomedicalData\nACM,2019. Science. Lect.NotesComput.Sci.,11073:197–205,2018.\n[105] T.Kato,M.Matsushita,andE.Maeda.Answeringitwithcharts:dialogue [134] R.J.LeoJohn,N.Potti,andJ.M.Patel. Ava:Fromdatatoinsights\ninnaturallanguageandcharts. InProc.COLING’02.ACM,2002. throughconversation. InProc.CIDR’17,2017.\n[106] P. Kaur, M. Owonibi, and B. Koenig-Ries. Towards visualization [135] F. Li and H. V. Jagadish. NaLIR: An interactive natural language\nrecommendation-asemi-automateddomain-specificlearningapproach. interfaceforqueryingrelationaldatabases. InProc.SIGMOD’14.ACM.\nCEURWorkshopProc.,1366:30–35,2015. [136] F.LiandH.V.Jagadish. Constructinganinteractivenaturallanguage\n[107] S.Kerpedjiev,G.Carenini,S.F.Roth,andJ.D.Moore. AutoBrief: interfaceforrelationaldatabases.Proc.VLDBEndow.,8(1):73–84,2014.\namultimediapresentationsystemforassistingdataanalysis. Comput. [137] F.LiandH.V.Jagadish. UnderstandingNaturalLanguageQueriesover\nStand.Interfaces,18(6-7):583–593,1997. RelationalDatabases. ACMSIGMODRec.,45(1):6–13,2016.\n[108] N.KerracherandJ.Kennedy. ConstructingandEvaluatingVisualisation [138] H.Li,Y.Wang,S.Zhang,andetal.KG4Vis:AKnowledgeGraph-Based\nTaskClassifications:ProcessandConsiderations.Comput.Graph.Forum, ApproachforVisualizationRecommendation. IEEETrans.Vis.Comput.\n36(3):47–59,2017. Graph.,pages1–11,2021.\n[109] A.Key,B.Howe,D.Perry,andC.Aragon. VizDeck:Self-organizing [139] J.Li,J.Yang,A.Hertzmann,andetal.LayoutGAN:GeneratingGraphic\ndashboardsforvisualanalytics. InProc.SIGMOD’12.ACM,2012. LayoutswithWireframeDiscriminators. arXiv,2019.\n[110] D.H.Kim,E.Hoque,andM.Agrawala. AnsweringQuestionsabout [140] L.H.Li,M.Yatskar,D.Yin,andetal. VisualBERT:ASimpleand\nChartsandGeneratingVisualExplanations. InProc.CHI’20.ACM. PerformantBaselineforVisionandLanguage. arXiv,2019.\n[111] D.H.Kim,V.Setlur,andM.Agrawala. TowardsUnderstandingHow [141] P.Li,L.Liu,J.Xu,andetal. ApplicationofHiddenMarkovModelin\nReadersIntegrateChartsandCaptions:ACaseStudywithLineCharts. SQLInjectionDetection. InProc.COMPSAC’17.IEEE,2017.\nInProc.CHI’21.ACM,2021. [142] H.Lin,D.Moritz,andJ.Heer.Dziban:BalancingAgency&Automation\n[112] H.Kim,J.Oh,Y.Han,andetal.ThumbnailsforDataStories:ASurvey in Visualization Design via Anchored Recommendations. In Proc.\nofCurrentPractices. InProc.VIS’19.IEEE,2019. CHI’20.ACM,2020.\n[113] Y.KimandJ.Heer. AssessingEffectsofTaskandDataDistribution [143] C. Liu, Y. Han, R. Jiang, and X. Yuan. ADVISor: Automatic\non the Effectiveness of Visual Encodings. Comput. Graph. Forum, VisualizationAnswerforNatural-LanguageQuestiononTabularData.\n37(3):157–167,2018. InProc.PacificVis’21.IEEE,2021.",
    "tables_extraction_text_csv": "0\n\"IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n17\"\n\"[85]\nK. Hu, S. S. Gaikwad, M. Hulsebos, and et al. VizNet: Towards a\n[114] Y. Kim and J. Heer. Gemini: A Grammar and Recommender System for\"\n\"IEEE Trans. Vis. Comput.\nlarge-scale visualization learning and benchmarking repository.\nIn Proc.\nAnimated Transitions in Statistical Graphics.\"\n\"CHI’19. ACM, 2019.\nGraph., 27(2):485–494, 2021.\"\n\"[115] Y.-H. Kim, B. Lee, A. Srinivasan, and E. K. Choe. Data@Hand: Fostering\n[86]\nK. Hu, D. Orghian, and C. Hidalgo. DIVE: A mixed-initiative system\"\n\"Visual Exploration of Personal Data on Smartphones Leveraging Speech\nsupporting integrated data exploration workﬂows.\nIn Proc. HILDA’2018.\"\n\"and Touch Interaction.\nIn Proc. CHI’21. ACM, 2021.\nACM, 2018.\"\n\"[116] R. Kincaid and G. Pollock. Nicky: Toward a Virtual Assistant for Test\n[87]\nB. Huang, G. Zhang, and P. C.-Y. Sheu. A Natural Language Database\"\n\"and Measurement\nInstrument Recommendations.\nIn Proc.\nICSC’17.\nInterface Based on a Probabilistic Context Free Grammar.\nIn Proc.\"\n\"IEEE, 2017.\nWSCS’08. IEEE, 2008.\"\n\"[117] Y. Kirstain, O. Ram, and O. Levy. Coreference Resolution without Span\n[88]\nZ. Huang, Y. Zhao, W. Chen, and et al. A Natural-language-based Visual\"\n\"Representations. arXiv, 2021.\nIEEE Trans. Vis.\nQuery Approach of Uncertain Human Trajectories.\"\n\"[118] V. Kocijan, A.-M. Cretu, O.-M. Camburu, and et al. A Surprisingly\nComput. Graph., 26(1):1–11, 2019.\"\n\"Robust Trick for\nthe Winograd Schema Challenge. Proc. 57th Annu.\n[89]\nJ. Hullman, N. Diakopoulos, and E. Adar.\nContextiﬁer: Automatic\"\n\"Meet. Assoc. Comput. Linguist. ACL’19, pages 4837–4842, 2019.\ngeneration of annotated stock visualizations.\nIn Proc. CHI’13. ACM.\"\n\"[119] N. Kong and M. Agrawala. Graphical Overlays: Using Layered Elements\n[90]\nM. Hulsebos, A. Satyanarayan, and et al.\nSherlock: A deep learning\"\n\"to Aid Chart Reading.\nIEEE Trans. Vis. Comput. Graph., 18(12), 2012.\napproach to semantic data type detection.\nIn Proc. KDD’19. ACM, 2019.\"\n\"[120] A. Kumar, J. Aurisano, B. Di Eugenio, and et al. Multimodal Coreference\"\n\"[91]\nR.\nIngria, R. Sauri,\nJ. Pustejovsky,\nand\net\nal.\nTimeML: Robust\"\n\"Resolution for Exploratory Data Visualization Dialogue: Context-Based\nNew Dir.\"\n\"Speciﬁcation of Event and Temporal Expressions\nin Text.\"\n\"Annotation and Gesture Identiﬁcation.\nIn Proc. SEMDIAL’17. ISCA.\"\n\"Quest. answering, 3:28–34, 2003.\"\n\"[121] A. Kumar,\nJ. Aurisano, and et al.\nTowards a dialogue system that\"\n\"[92]\nS. Iyer, I. Konstas, A. Cheung, and et al. Learning a Neural Semantic\"\n\"supports rich visualizations of data.\nIn Proc. SIGDIAL’16. ACL, 2016.\"\n\"Parser from User Feedback.\nIn Proc. ACL’17. ACL, 2017.\"\n\"[122] C. Lai, Z. Lin, R. Jiang, and et al. Automatic Annotation Synchronizing\"\n\"[93]\nQ. Jiang, G. Sun, Y. Dong, and R. Liang. DT2VIS: A Focus+Context\"\n\"with Textual Description for Visualization.\nIn Proc. CHI’20. ACM, 2020.\"\nAnswer Generation System to Facilitate Visual Exploration of Tabular\n\"[123]\nS. Lalle, D. Toker, and C. Conati. Gaze-Driven Adaptive Interventions\"\n\"Data.\nIEEE Comput. Graph. Appl., 41(5):45–56, 2021.\"\n\"IEEE Trans. Vis. Comput.\nfor Magazine-Style Narrative Visualizations.\"\n\"[94]\nS. Jiang, P. Kang, X. Song, and et al. Emerging Wearable Interfaces and\"\n\"Graph., 27(6):2941–2952, 2019.\"\n\"IEEE Rev. Biomed.\nAlgorithms for Hand Gesture Recognition: A Survey.\"\n\"[124] R. Langner, M. Satkowski, W. B¨uschel, and R. Dachselt. MARVIS:\"\n\"Eng., pages 1–11, 2021.\"\nCombining Mobile Devices and Augmented Reality for Visual Data\n\"[95]\nM. Joshi, D. Chen, Y. Liu, and et al. SpanBERT: Improving Pre-training\"\n\"Analysis.\nIn Proc. CHI’21. ACM, 2021.\"\n\"by Representing and Predicting Spans. Trans. Assoc. Comput. Linguist.,\"\n\"[125] B. Lee, D. Brown, B. Lee, and et al. Data Visceralization: Enabling\"\n\"8:64–77, 2020.\"\n\"IEEE Trans. Vis.\nDeeper Understanding of Data Using Virtual Reality.\"\n\"[96]\nM. Joshi, O. Levy, L. Zettlemoyer, and D. Weld. BERT for Coreference\"\n\"Comput. Graph., 27(2):1095–1105, 2021.\"\n\"Resolution: Baselines and Analysis.\nIn Proc. EMNLP’19. ACL, 2019.\"\n\"[126] B. Lee, P. Isenberg, N. H. Riche, and S. Carpendale. Beyond Mouse\"\n\"[97]\nK. Kaﬂe, B. Price, S. Cohen, and C. Kanan. DVQA: Understanding Data\"\n\"and Keyboard: Expanding Design Considerations\nfor\nInformation\"\n\"Visualizations via Question Answering.\nIn Proc. CVPR’18. IEEE, 2018.\"\n\"Visualization Interactions.\nIEEE Trans. Vis. Comput. Graph., 2012.\"\n\"[98]\nS. E. Kahou, V. Michalski, A. Atkinson, and et al.\nFigureQA: An\"\n\"[127] B. Lee, C. Plaisant, C. S. Parr, and et al.\nTask taxonomy for graph\"\n\"Annotated Figure Dataset for Visual Reasoning.\nIn Proc. ICLR’18, 2018.\"\n\"visualization.\nIn Proc. BELIV’06. ACM, 2006.\"\n\"[99]\nS. Kandel, A. Paepcke, J. Hellerstein, and J. Heer. Wrangler: Interactive\"\n\"[128] B. Lee, A. Srinivasan, P. Isenberg, and J. Stasko. Post-wimp interaction\"\n\"visual speciﬁcation of data transformation scripts.\nIn Proc. CHI’11.\"\n\"for information visualization. Found. Trends Human-Computer Interact.,\"\n\"ACM, 2011.\"\n\"14(1):1–95, 2021.\"\n\"[100]\nS. Kandel, R. Parikh, A. Paepcke, and et al. Proﬁler: Integrated statistical\"\n\"¨\n[129] D. J. L. Lee, A. Quamar, E. Kandogan, and F.\nOzcan.\nBoomerang:\"\n\"analysis and visualization for data quality assessment.\nIn Proc. AVI’12.\"\nProactive Insight-Based Recommendations for Guiding Conversational\n\"ACM, 2012.\"\n\"Data Analysis.\nIn Proc. SIGMOD’21 demo, 2021.\"\n\"[101] E. Kandogan.\nJust-in-time annotation of clusters, outliers, and trends in\"\n\"[130] D. J.-L. Lee, V. Setlur, M. Tory, and et al. Deconstructing Categorization\"\n\"point-based data visualizations.\nIn Proc. VAST’12. IEEE, 2012.\"\nin Visualization Recommendation: A Taxonomy and Comparative Study.\n\"[102]\nJ. Kang, K. Condiff, S. Chang, and et al. Understanding How People\"\n\"IEEE Trans. Vis. Comput. Graph., 2626:1–14, 2021.\"\n\"Use Natural Language to Ask for Recommendations.\nIn Proc. RecSys’17.\"\n\"[131] K. Lee, L. He, M. Lewis, and L. Zettlemoyer.\nEnd-to-end Neural\"\n\"ACM, 2017.\"\n\"Coreference Resolution.\nIn Proc. EMNLP’17. ACL, 2017.\"\n\"[103]\nJ. F. Kassel and M. Rohs. Valletto: A multimodal interface for ubiquitous\"\n\"[132] K. Lee, L. He, and L. Zettlemoyer. Higher-Order Coreference Resolution\"\n\"visual analytics.\nIn Proc. CHI EA’18. ACM, 2018.\"\n\"with Coarse-to-Fine Inference.\nIn Proc. NAACL’18. ACL, 2018.\"\n\"[104]\nJ. F. Kassel and M. Rohs. Talk to me intelligibly: Investigating an answer\"\n\"J. Leo John,\nJ. M. Patel, A. L. Alexander, and et al. A Natural\n[133] R.\"\n\"space to match the user’s language in visual analysis.\nIn Proc. DIS’19.\"\nLanguage Interface for Dissemination of Reproducible Biomedical Data\n\"ACM, 2019.\"\n\"Science. Lect. Notes Comput. Sci., 11073:197–205, 2018.\"\n\"[105] T. Kato, M. Matsushita, and E. Maeda. Answering it with charts: dialogue\"\n\"[134] R. J. Leo John, N. Potti, and J. M. Patel. Ava: From data to insights\"\n\"in natural\nlanguage and charts.\nIn Proc. COLING’02. ACM, 2002.\"\n\"through conversation.\nIn Proc. CIDR’17, 2017.\"\n\"P. Kaur, M. Owonibi,\nand B. Koenig-Ries.\nTowards visualization\n[106]\n[135]\nF. Li and H. V.\nJagadish.\nNaLIR: An interactive natural\nlanguage\"\n\"recommendation-a semi-automated domain-speciﬁc learning approach.\ninterface for querying relational databases.\nIn Proc. SIGMOD’14. ACM.\"\n\"CEUR Workshop Proc., 1366:30–35, 2015.\"\n\"[136]\nF. Li and H. V. Jagadish. Constructing an interactive natural\nlanguage\"\n\"[107]\nS. Kerpedjiev, G. Carenini, S. F. Roth, and J. D. Moore. AutoBrief:\ninterface for relational databases. Proc. VLDB Endow., 8(1):73–84, 2014.\"\na multimedia presentation system for assisting data analysis. Comput.\n\"[137]\nF. Li and H. V. Jagadish. Understanding Natural Language Queries over\"\n\"Stand. Interfaces, 18(6-7):583–593, 1997.\"\n\"Relational Databases. ACM SIGMOD Rec., 45(1):6–13, 2016.\"\n\"[108] N. Kerracher and J. Kennedy. Constructing and Evaluating Visualisation\n[138] H. Li, Y. Wang, S. Zhang, and et al. KG4Vis: A Knowledge Graph-Based\"\n\"Task Classiﬁcations: Process and Considerations. Comput. Graph. Forum,\nIEEE Trans. Vis. Comput.\nApproach for Visualization Recommendation.\"\n\"36(3):47–59, 2017.\nGraph., pages 1–11, 2021.\"\n\"[109] A. Key, B. Howe, D. Perry, and C. Aragon. VizDeck: Self-organizing\n[139]\nJ. Li, J. Yang, A. Hertzmann, and et al. LayoutGAN: Generating Graphic\"\n\"dashboards for visual analytics.\nIn Proc. SIGMOD’12. ACM, 2012.\nLayouts with Wireframe Discriminators. arXiv, 2019.\"\n\"[110] D. H. Kim, E. Hoque, and M. Agrawala. Answering Questions about\n[140] L. H. Li, M. Yatskar, D. Yin, and et al. VisualBERT: A Simple and\"\n\"Charts and Generating Visual Explanations.\nIn Proc. CHI’20. ACM.\nPerformant Baseline for Vision and Language. arXiv, 2019.\"\n\"[111] D. H. Kim, V. Setlur, and M. Agrawala. Towards Understanding How\n[141]\nP. Li, L. Liu, J. Xu, and et al. Application of Hidden Markov Model\nin\"\n\"Readers Integrate Charts and Captions: A Case Study with Line Charts.\nSQL Injection Detection.\nIn Proc. COMPSAC’17. IEEE, 2017.\"\n\"In Proc. CHI’21. ACM, 2021.\n[142] H. Lin, D. Moritz, and J. Heer. Dziban: Balancing Agency & Automation\"\n\"[112] H. Kim, J. Oh, Y. Han, and et al. Thumbnails for Data Stories: A Survey\nin Visualization Design via Anchored Recommendations.\nIn Proc.\"\n\"of Current Practices.\nIn Proc. VIS’19. IEEE, 2019.\nCHI’20. ACM, 2020.\"\n\"[113] Y. Kim and J. Heer. Assessing Effects of Task and Data Distribution\n[143] C. Liu, Y. Han, R.\nJiang,\nand X. Yuan.\nADVISor: Automatic\"\n\"on the Effectiveness of Visual Encodings.\nComput. Graph. Forum,\nVisualization Answer for Natural-Language Question on Tabular Data.\"\n\"37(3):157–167, 2018.\nIn Proc. PaciﬁcVis’21. IEEE, 2021.\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 18,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n18\n[144] C. Liu, L. Xie, Y. Han, and et al. AutoCaption: An Approach to Generate\nNatural Language Description from Visualization Automatically. In Proc.\nPaciﬁcVis’20. IEEE, 2020.\n[145] Q. Liu, H. Jiang, Z.-H. Ling, and et al. Commonsense Knowledge\nEnhanced Embeddings for Solving Pronoun Disambiguation Problems\nin Winograd Schema Challenge. arXiv, 2016.\n[146] T. Liu, X. Li, C. Bao, and et al. Data-Driven Mark Orientation for Trend\nEstimation in Scatterplots. In Proc. CHI’21. ACM, 2021.\n[147] G. L´opez, L. Quesada, and L. A. Guerrero. Alexa vs. Siri vs. Cortana\nvs. Google Assistant: A Comparison of Speech-Based Natural User\nInterfaces. In Proc. AHFE’17. Springer, 2017.\n[148] Y. Luo, X. Qin, N. Tang, and et al. DeepEye: Creating good data\nvisualizations by keyword search. In Proc. SIGMOD’18. ACM, 2018.\n[149] Y. Luo, X. Qin, N. Tang, and G. Li. Deepeye: towards automatic data\nvisualization. In Proc. ICDE’18. IEEE, 2018.\n[150] Y. Luo, N. Tang, G. Li, and et al. Natural Language to Visualization by\nNeural Machine Translation. In Proc. VIS’21. IEEE, 2021.\n[151] Y. Luo, N. Tang, G. Li, and et al. Synthesizing Natural Language to\nVisualization (NL2VIS) Benchmarks from NL2SQL Benchmarks. In\nProc. SIGMOD’21. ACM, 2021.\n[152] Y. Ma, A. K. H. Tung, W. Wang, and et al. ScatterNet: A Deep Subjective\nSimilarity Model for Visual Analysis of Scatterplots. IEEE Trans. Vis.\nComput. Graph., 26(3):1562–1576, 2020.\n[153] J. Mackinlay.\nAutomating the design of graphical presentations of\nrelational information. ACM Trans. Graph., 5(2):110–141, 1986.\n[154] J. Mackinlay, P. Hanrahan, and C. Stolte.\nShow Me: Automatic\nPresentation for Visual Analysis. IEEE Trans. Vis. Comput. Graph.,\n13(6):1137–1144, 2007.\n[155] S. Madan, Z. Bylinskii, M. Tancik, and et al. Synthetically Trained Icon\nProposals for Parsing and Summarizing Infographics. arXiv, 2018.\n[156] S. Maji, S. S. Rout, and S. Choudhary. DCoM: A Deep Column Mapper\nfor Semantic Data Type Detection. arXiv, 2021.\n[157] C. Manning, M. Surdeanu, J. Bauer, and et al. The Stanford CoreNLP\nNatural Language Processing Toolkit. In Proc. ACL’14. ACL, 2014.\n[158] M. Mathew, V. Bagal, and et al. InfographicVQA. arXiv, 2021.\n[159] M. Matsushita, E. Maeda, and T. Kato. An interactive visualization\nmethod of numerical data based on natural language requirements. Int. J.\nHum. Comput. Stud., 60(4):469–488, 2004.\n[160] L. McNabb and R. S. Laramee. Survey of Surveys (SoS) - Mapping\nThe Landscape of Survey Papers in Information Visualization. Comput.\nGraph. Forum, 36(3):589–617, 2017.\n[161] R. Metoyer, B. Lee, N. Henry Riche, and M. Czerwinski. Understanding\nthe verbal language and structure of end-user descriptions of data\nvisualizations. In Proc. CHI’12. ACM, 2012.\n[162] R. Metoyer, Q. Zhi, B. Janczuk, and W. Scheirer.\nCoupling story\nto visualization: Using textual analysis as a bridge between data and\ninterpretation. In Proc. IUI’18. ACM, 2018.\n[163] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient estimation of\nword representations in vector space. In Proc. ICLR’13, 2013.\n[164] G. A. Miller. WordNet: A Lexical Database for English. Commun. ACM,\n38(11):39–41, 1995.\n[165] M. Mitri.\nStory Analysis Using Natural Language Processing and\nInteractive Dashboards. J. Comput. Inf. Syst., pages 1–11, 2020.\n[166] V. O. Mittal, J. D. Moore, G. Carenini, and S. Roth. Describing Complex\nCharts in Natural Language: A Caption Generation System. Comput.\nLinguist., 24(3):431–467, 1998.\n[167] P. Moraes, G. Sina, K. McCoy, and S. Carberry. Generating Summaries\nof Line Graphs. In Proc. INLG’14. ACL, 2014.\n[168] D. Moritz, C. Wang, G. L. Nelson, and et al. Formalizing Visualization\nDesign Knowledge as Constraints: Actionable and Extensible Models in\nDraco. IEEE Trans. Vis. Comput. Graph., 25(1):438–448, 2019.\n[169] T. Murillo-Morales and K. Miesenberger. AUDiaL: A Natural Language\nInterface to Make Statistical Charts Accessible to Blind Persons. In Lect.\nNotes Comput. Sci., pages 373–384. Springer, 2020.\n[170] B. Mutlu, E. Veas, and C. Trattner. VizRec: Recommending personalized\nvisualizations. ACM Trans. Interact. Intell. Syst., 6(4):1–39, 2016.\n[171] M. Nafari and C. Weaver.\nAugmenting Visualization with Natural\nLanguage Translation of Interaction: A Usability Study. Comput. Graph.\nForum, 32(3):391–400, 2013.\n[172] M. Nafari and C. Weaver. Query2Question: Translating visualization\ninteraction into natural language. IEEE Trans. Vis. Comput. Graph.,\n21(6):756–769, 2015.\n[173] A. Narechania, A. Fourney, and et al. DIY: Assessing the Correctness of\nNatural Language to SQL Systems. In Proc. IUI’21. ACM, 2021.\n[174] A. Narechania, A. Srinivasan, and J. Stasko. NL4DV: A Toolkit for\nGenerating Analytic Speciﬁcations for Data Visualization from Natural\nLanguage Queries. IEEE Trans. Vis. Comput. Graph., 27(2), 2021.\n[175] N. Nihalani, S. Silakari, and M. Motwani. Natural language Interface\nfor Database: A Brief review. Int. J. Comput. Sci., 8(2):600–608, 2011.\n[176] L. G. Nonato and M. Aupetit. Multidimensional Projection for Visual\nAnalytics: Linking Techniques with Distortions, Tasks, and Layout\nEnrichment. IEEE Trans. Vis. Comput. Graph., 25(8):2650–2673, 2019.\n[177] J. Obeid and E. Hoque. Chart-to-Text: Generating Natural Language\nDescriptions for Charts by Adapting the Transformer Model. In Proc.\nINLG’20. ACL, 2020.\n[178] M. Oppermann, R. Kincaid, and T. Munzner. VizCommender: Com-\nputing Text-Based Similarity in Visualization Repositories for Content-\nBased Recommendations. IEEE Trans. Vis. Comput. Graph., 2021.\n[179] M. Peters, M. Neumann, M. Iyyer, and et al. Deep Contextualized Word\nRepresentations. In Proc. NAACL’18. ACL, 2018.\n[180] J. Poco and J. Heer. Reverse-Engineering Visualizations: Recovering\nVisual Encodings from Chart Images. Comput. Graph. Forum, 2017.\n[181] S. Pradhan, A. Moschitti, N. Xue, and et al. CoNLL-2012 Shared Task:\nModeling Multilingual Unrestricted Coreference in OntoNotes. In Proc.\nEMNLP’12. ACL, 2012.\n[182] P. Qi, Y. Zhang, Y. Zhang, and et al. Stanza: A Python Natural Language\nProcessing Toolkit for Many Human Languages. In Proc. ACL’20. ACL.\n[183] C. Qian, S. Sun, W. Cui, and et al. Retrieve-Then-Adapt: Example-based\nAutomatic Generation for Proportion-related Infographics. IEEE Trans.\nVis. Comput. Graph., 27(2):443–452, 2021.\n[184] X. Qian, E. Koh, F. Du, and et al. Generating Accurate Caption Units\nfor Figure Captioning. In Proc. WWW’21. ACM, 2021.\n[185] X. Qian, R. A. Rossi, F. Du, and et al.\nLearning to Recommend\nVisualizations from Data. In Proc. KDD’21. ACM, 2021.\n[186] X. Qian, R. A. Rossi, F. Du, and et al. Personalized Visualization\nRecommendation. arXiv, 2021.\n[187] X. Qin, Y. Luo, N. Tang, and G. Li. Making data visualization more\nefﬁcient and effective: a survey. VLDB J., 29(1):93–117, 2020.\n[188] A. Quamar, C. Lei, D. Miller, and et al. An Ontology-Based Conversation\nSystem for Knowledge Bases. In Proc. SIGMOD’20. ACM, 2020.\n[189] S. Reinders, M. Butler, and K. Marriott. ”Hey Model!” - Natural User\nInteractions and Agency in Accessible Interactive 3D Models. In Proc.\nCHI’20. ACM, 2020.\n[190] D. Ren, M. Brehmer, Bongshin Lee, and et al. ChartAccent: Annotation\nfor data-driven storytelling. In Proc. PaciﬁcVis’17. IEEE, 2017.\n[191] A. Rind, W. Aigner, M. Wagner, and et al.\nTask Cube: A three-\ndimensional conceptual space of user tasks in visualization design and\nevaluation. Inf. Vis., 15(4):288–300, 2016.\n[192] S. F. Roth, J. Kolojejchick, J. Mattis, and J. Goldstein. Interactive graphic\ndesign using automatic presentation knowledge. In Proc. CHI’94. ACM.\n[193] D. Saha, A. Floratou, K. Sankaranarayanan, and et al. ATHENA: An\nontologydriven system for natural language querying over relational data\nstores. Proc. VLDB Endow., 9(12):1209–1220, 2016.\n[194] B. Saket, A. Endert, and C. Demiralp. Task-Based Effectiveness of Basic\nVisualizations. IEEE Trans. Vis. Comput. Graph., 25(7), 2019.\n[195] A. Saktheeswaran, A. Srinivasan, and J. Stasko. Touch? Speech? or\nTouch and Speech? Investigating Multimodal Interaction for Visual\nNetwork Exploration and Analysis. IEEE Trans. Vis. Comput. Graph.,\n26(6):2168–2179, 2020.\n[196] A. Sarikaya and M. Gleicher. Scatterplots: Tasks, Data, and Designs.\nIEEE Trans. Vis. Comput. Graph., 24(1):402–412, 2018.\n[197] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer. Vega-Lite:\nA Grammar of Interactive Graphics. IEEE Trans. Vis. Comput. Graph.,\n23(1):341–350, 2017.\n[198] A. Satyanarayan, R. Russell, J. Hoffswell, and J. Heer.\nReactive\nVega: A Streaming Dataﬂow Architecture for Declarative Interactive\nVisualization. IEEE Trans. Vis. Comput. Graph., 22(1):659–668, 2016.\n[199] M. Savva, N. Kong, A. Chhajta, and et al.\nReVision: Automated\nclassiﬁcation, analysis and redesign of chart images. In Proc. UIST’11.\nACM, 2011.\n[200] E. Segel and J. Heer. Narrative visualization: Telling stories with data.\nIEEE Trans. Vis. Comput. Graph., 16(6):1139–1148, 2010.\n[201] P. Seipel, A. Stock, S. Santhanam, and et al. Speak to your Software\nVisualization—Exploring Component-Based Software Architectures\nin Augmented Reality with a Conversational Interface.\nIn Proc.\nVISSOFT’19. IEEE, 2019.\n[202] J. Sen, F. Ozcan, and et al. Natural Language Querying of Complex\nBusiness Intelligence Queries. In Proc. SIGMOD’19. ACM, 2019.\n[203] J. Seo and B. Shneiderman.\nA Rank-by-Feature Framework for\nInteractive Exploration of Multidimensional Data. Inf. Vis., 2005.\n[204] V. Setlur, S. Battersby, and T. Wong.\nGeoSneakPique : Visual\nAutocompletion for Geospatial Queries. In Proc. VIS’21. IEEE, 2021.\n[205] V. Setlur, S. E. Battersby, M. Tory, and et al. Eviza: A natural language\ninterface for visual analysis. In Proc. UIST’16. ACM, 2016.\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 18\n[144] C.Liu,L.Xie,Y.Han,andetal.AutoCaption:AnApproachtoGenerate [175] N.Nihalani,S.Silakari,andM.Motwani. NaturallanguageInterface\nNaturalLanguageDescriptionfromVisualizationAutomatically.InProc. forDatabase:ABriefreview. Int.J.Comput.Sci.,8(2):600–608,2011.\nPacificVis’20.IEEE,2020. [176] L.G.NonatoandM.Aupetit. MultidimensionalProjectionforVisual\n[145] Q. Liu, H. Jiang, Z.-H. Ling, and et al. Commonsense Knowledge Analytics: Linking Techniques with Distortions, Tasks, and Layout\nEnhancedEmbeddingsforSolvingPronounDisambiguationProblems Enrichment. IEEETrans.Vis.Comput.Graph.,25(8):2650–2673,2019.\ninWinogradSchemaChallenge. arXiv,2016. [177] J.ObeidandE.Hoque. Chart-to-Text:GeneratingNaturalLanguage\n[146] T.Liu,X.Li,C.Bao,andetal.Data-DrivenMarkOrientationforTrend DescriptionsforChartsbyAdaptingtheTransformerModel. InProc.\nEstimationinScatterplots. InProc.CHI’21.ACM,2021. INLG’20.ACL,2020.\n[147] G.Lo´pez,L.Quesada,andL.A.Guerrero. Alexavs.Sirivs.Cortana [178] M.Oppermann,R.Kincaid,andT.Munzner. VizCommender:Com-\nvs. Google Assistant: A Comparison of Speech-Based Natural User putingText-BasedSimilarityinVisualizationRepositoriesforContent-\nInterfaces. InProc.AHFE’17.Springer,2017. BasedRecommendations. IEEETrans.Vis.Comput.Graph.,2021.\n[148] Y. Luo, X. Qin, N. Tang, and et al. DeepEye: Creating good data [179] M.Peters,M.Neumann,M.Iyyer,andetal. DeepContextualizedWord\nvisualizationsbykeywordsearch. InProc.SIGMOD’18.ACM,2018. Representations. InProc.NAACL’18.ACL,2018.\n[149] Y.Luo,X.Qin,N.Tang,andG.Li. Deepeye:towardsautomaticdata [180] J.PocoandJ.Heer. Reverse-EngineeringVisualizations:Recovering\nvisualization. InProc.ICDE’18.IEEE,2018. VisualEncodingsfromChartImages. Comput.Graph.Forum,2017.\n[150] Y.Luo,N.Tang,G.Li,andetal. NaturalLanguagetoVisualizationby [181] S.Pradhan,A.Moschitti,N.Xue,andetal. CoNLL-2012SharedTask:\nNeuralMachineTranslation. InProc.VIS’21.IEEE,2021. ModelingMultilingualUnrestrictedCoreferenceinOntoNotes. InProc.\n[151] Y.Luo,N.Tang,G.Li,andetal. SynthesizingNaturalLanguageto EMNLP’12.ACL,2012.\nVisualization(NL2VIS)BenchmarksfromNL2SQLBenchmarks. In [182] P.Qi,Y.Zhang,Y.Zhang,andetal.Stanza:APythonNaturalLanguage\nProc.SIGMOD’21.ACM,2021. ProcessingToolkitforManyHumanLanguages. InProc.ACL’20.ACL.\n[152] Y.Ma,A.K.H.Tung,W.Wang,andetal.ScatterNet:ADeepSubjective [183] C.Qian,S.Sun,W.Cui,andetal.Retrieve-Then-Adapt:Example-based\nSimilarityModelforVisualAnalysisofScatterplots. IEEETrans.Vis. AutomaticGenerationforProportion-relatedInfographics. IEEETrans.\nComput.Graph.,26(3):1562–1576,2020. Vis.Comput.Graph.,27(2):443–452,2021.\n[153] J. Mackinlay. Automating the design of graphical presentations of [184] X.Qian,E.Koh,F.Du,andetal. GeneratingAccurateCaptionUnits\nrelationalinformation. ACMTrans.Graph.,5(2):110–141,1986. forFigureCaptioning. InProc.WWW’21.ACM,2021.\n[154] J. Mackinlay, P. Hanrahan, and C. Stolte. Show Me: Automatic [185] X. Qian, R. A. Rossi, F. Du, and et al. Learning to Recommend\nPresentation for Visual Analysis. IEEE Trans. Vis. Comput. Graph., VisualizationsfromData. InProc.KDD’21.ACM,2021.\n13(6):1137–1144,2007. [186] X. Qian, R. A. Rossi, F. Du, and et al. Personalized Visualization\n[155] S.Madan,Z.Bylinskii,M.Tancik,andetal. SyntheticallyTrainedIcon Recommendation. arXiv,2021.\nProposalsforParsingandSummarizingInfographics. arXiv,2018. [187] X.Qin,Y.Luo,N.Tang,andG.Li. Makingdatavisualizationmore\n[156] S.Maji,S.S.Rout,andS.Choudhary. DCoM:ADeepColumnMapper efficientandeffective:asurvey. VLDBJ.,29(1):93–117,2020.\nforSemanticDataTypeDetection. arXiv,2021. [188] A.Quamar,C.Lei,D.Miller,andetal.AnOntology-BasedConversation\n[157] C.Manning,M.Surdeanu,J.Bauer,andetal. TheStanfordCoreNLP SystemforKnowledgeBases. InProc.SIGMOD’20.ACM,2020.\nNaturalLanguageProcessingToolkit. InProc.ACL’14.ACL,2014. [189] S.Reinders,M.Butler,andK.Marriott. ”HeyModel!”-NaturalUser\n[158] M.Mathew,V.Bagal,andetal. InfographicVQA. arXiv,2021. InteractionsandAgencyinAccessibleInteractive3DModels. InProc.\n[159] M. Matsushita, E. Maeda, and T. Kato. An interactive visualization CHI’20.ACM,2020.\nmethodofnumericaldatabasedonnaturallanguagerequirements.Int.J. [190] D.Ren,M.Brehmer,BongshinLee,andetal. ChartAccent:Annotation\nHum.Comput.Stud.,60(4):469–488,2004. fordata-drivenstorytelling. InProc.PacificVis’17.IEEE,2017.\n[160] L.McNabbandR.S.Laramee. SurveyofSurveys(SoS)-Mapping [191] A. Rind, W. Aigner, M. Wagner, and et al. Task Cube: A three-\nTheLandscapeofSurveyPapersinInformationVisualization. Comput. dimensionalconceptualspaceofusertasksinvisualizationdesignand\nGraph.Forum,36(3):589–617,2017. evaluation. Inf.Vis.,15(4):288–300,2016.\n[161] R.Metoyer,B.Lee,N.HenryRiche,andM.Czerwinski.Understanding [192] S.F.Roth,J.Kolojejchick,J.Mattis,andJ.Goldstein.Interactivegraphic\nthe verbal language and structure of end-user descriptions of data designusingautomaticpresentationknowledge. InProc.CHI’94.ACM.\nvisualizations. InProc.CHI’12.ACM,2012. [193] D.Saha,A.Floratou,K.Sankaranarayanan,andetal. ATHENA:An\n[162] R. Metoyer, Q. Zhi, B. Janczuk, and W. Scheirer. Coupling story ontologydrivensystemfornaturallanguagequeryingoverrelationaldata\ntovisualization:Usingtextualanalysisasabridgebetweendataand stores. Proc.VLDBEndow.,9(12):1209–1220,2016.\ninterpretation. InProc.IUI’18.ACM,2018. [194] B.Saket,A.Endert,andC.Demiralp.Task-BasedEffectivenessofBasic\n[163] T.Mikolov,K.Chen,G.Corrado,andJ.Dean. Efficientestimationof Visualizations. IEEETrans.Vis.Comput.Graph.,25(7),2019.\nwordrepresentationsinvectorspace. InProc.ICLR’13,2013. [195] A.Saktheeswaran,A.Srinivasan,andJ.Stasko. Touch?Speech?or\n[164] G.A.Miller.WordNet:ALexicalDatabaseforEnglish.Commun.ACM, Touch and Speech? Investigating Multimodal Interaction for Visual\n38(11):39–41,1995. NetworkExplorationandAnalysis. IEEETrans.Vis.Comput.Graph.,\n[165] M. Mitri. Story Analysis Using Natural Language Processing and 26(6):2168–2179,2020.\nInteractiveDashboards. J.Comput.Inf.Syst.,pages1–11,2020. [196] A.SarikayaandM.Gleicher. Scatterplots:Tasks,Data,andDesigns.\n[166] V.O.Mittal,J.D.Moore,G.Carenini,andS.Roth.DescribingComplex IEEETrans.Vis.Comput.Graph.,24(1):402–412,2018.\nChartsinNaturalLanguage:ACaptionGenerationSystem. Comput. [197] A.Satyanarayan,D.Moritz,K.Wongsuphasawat,andJ.Heer.Vega-Lite:\nLinguist.,24(3):431–467,1998. AGrammarofInteractiveGraphics. IEEETrans.Vis.Comput.Graph.,\n[167] P.Moraes,G.Sina,K.McCoy,andS.Carberry. GeneratingSummaries 23(1):341–350,2017.\nofLineGraphs. InProc.INLG’14.ACL,2014. [198] A. Satyanarayan, R. Russell, J. Hoffswell, and J. Heer. Reactive\n[168] D.Moritz,C.Wang,G.L.Nelson,andetal. FormalizingVisualization Vega:AStreamingDataflowArchitectureforDeclarativeInteractive\nDesignKnowledgeasConstraints:ActionableandExtensibleModelsin Visualization. IEEETrans.Vis.Comput.Graph.,22(1):659–668,2016.\nDraco. IEEETrans.Vis.Comput.Graph.,25(1):438–448,2019. [199] M. Savva, N. Kong, A. Chhajta, and et al. ReVision: Automated\n[169] T.Murillo-MoralesandK.Miesenberger. AUDiaL:ANaturalLanguage classification,analysisandredesignofchartimages. InProc.UIST’11.\nInterfacetoMakeStatisticalChartsAccessibletoBlindPersons.InLect. ACM,2011.\nNotesComput.Sci.,pages373–384.Springer,2020. [200] E.SegelandJ.Heer. Narrativevisualization:Tellingstorieswithdata.\n[170] B.Mutlu,E.Veas,andC.Trattner.VizRec:Recommendingpersonalized IEEETrans.Vis.Comput.Graph.,16(6):1139–1148,2010.\nvisualizations. ACMTrans.Interact.Intell.Syst.,6(4):1–39,2016. [201] P.Seipel,A.Stock,S.Santhanam,andetal. SpeaktoyourSoftware\n[171] M. Nafari and C. Weaver. Augmenting Visualization with Natural Visualization—Exploring Component-Based Software Architectures\nLanguageTranslationofInteraction:AUsabilityStudy. Comput.Graph. in Augmented Reality with a Conversational Interface. In Proc.\nForum,32(3):391–400,2013. VISSOFT’19.IEEE,2019.\n[172] M.NafariandC.Weaver. Query2Question:Translatingvisualization [202] J.Sen,F.Ozcan,andetal. NaturalLanguageQueryingofComplex\ninteractionintonaturallanguage. IEEETrans.Vis.Comput.Graph., BusinessIntelligenceQueries. InProc.SIGMOD’19.ACM,2019.\n21(6):756–769,2015. [203] J. Seo and B. Shneiderman. A Rank-by-Feature Framework for\n[173] A.Narechania,A.Fourney,andetal. DIY:AssessingtheCorrectnessof InteractiveExplorationofMultidimensionalData. Inf.Vis.,2005.\nNaturalLanguagetoSQLSystems. InProc.IUI’21.ACM,2021. [204] V. Setlur, S. Battersby, and T. Wong. GeoSneakPique : Visual\n[174] A.Narechania,A.Srinivasan,andJ.Stasko. NL4DV:AToolkitfor AutocompletionforGeospatialQueries. InProc.VIS’21.IEEE,2021.\nGeneratingAnalyticSpecificationsforDataVisualizationfromNatural [205] V.Setlur,S.E.Battersby,M.Tory,andetal. Eviza:Anaturallanguage\nLanguageQueries. IEEETrans.Vis.Comput.Graph.,27(2),2021. interfaceforvisualanalysis. InProc.UIST’16.ACM,2016.",
    "tables_extraction_text_csv": "0,1\n\"IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\",18\n\"[144] C. Liu, L. Xie, Y. Han, and et al. AutoCaption: An Approach to Generate\",\"language Interface\n[175] N. Nihalani, S. Silakari, and M. Motwani. Natural\"\nNatural Language Description from Visualization Automatically. In Proc.,\"for Database: A Brief review.\nInt. J. Comput. Sci., 8(2):600–608, 2011.\"\n\"PaciﬁcVis’20. IEEE, 2020.\",[176] L. G. Nonato and M. Aupetit. Multidimensional Projection for Visual\n\"[145] Q. Liu, H.\nJiang, Z.-H. Ling, and et al.\nCommonsense Knowledge\",\"Analytics: Linking Techniques with Distortions, Tasks,\nand Layout\"\nEnhanced Embeddings for Solving Pronoun Disambiguation Problems,\"Enrichment.\nIEEE Trans. Vis. Comput. Graph., 25(8):2650–2673, 2019.\"\n\"in Winograd Schema Challenge. arXiv, 2016.\",\"[177]\nJ. Obeid and E. Hoque. Chart-to-Text: Generating Natural Language\"\n\"[146] T. Liu, X. Li, C. Bao, and et al. Data-Driven Mark Orientation for Trend\",\"Descriptions for Charts by Adapting the Transformer Model.\nIn Proc.\"\n\"Estimation in Scatterplots.\nIn Proc. CHI’21. ACM, 2021.\",\"INLG’20. ACL, 2020.\"\n\"[147] G. L´opez, L. Quesada, and L. A. Guerrero. Alexa vs. Siri vs. Cortana\",\"[178] M. Oppermann, R. Kincaid, and T. Munzner. VizCommender: Com-\"\nvs. Google Assistant: A Comparison of Speech-Based Natural User,puting Text-Based Similarity in Visualization Repositories for Content-\n\"Interfaces.\nIn Proc. AHFE’17. Springer, 2017.\",\"Based Recommendations.\nIEEE Trans. Vis. Comput. Graph., 2021.\"\n\"[148] Y. Luo, X. Qin, N. Tang, and et al.\nDeepEye: Creating good data\",\"[179] M. Peters, M. Neumann, M. Iyyer, and et al. Deep Contextualized Word\"\n\"visualizations by keyword search.\nIn Proc. SIGMOD’18. ACM, 2018.\",\"Representations.\nIn Proc. NAACL’18. ACL, 2018.\"\n\"[149] Y. Luo, X. Qin, N. Tang, and G. Li. Deepeye:\ntowards automatic data\",\"[180]\nJ. Poco and J. Heer. Reverse-Engineering Visualizations: Recovering\"\n\"visualization.\nIn Proc. ICDE’18. IEEE, 2018.\",\"Visual Encodings from Chart Images. Comput. Graph. Forum, 2017.\"\n\"[150] Y. Luo, N. Tang, G. Li, and et al. Natural Language to Visualization by\",\"[181]\nS. Pradhan, A. Moschitti, N. Xue, and et al. CoNLL-2012 Shared Task:\"\n\"Neural Machine Translation.\nIn Proc. VIS’21. IEEE, 2021.\",\"Modeling Multilingual Unrestricted Coreference in OntoNotes.\nIn Proc.\"\n\"[151] Y. Luo, N. Tang, G. Li, and et al. Synthesizing Natural Language to\",\"EMNLP’12. ACL, 2012.\"\n\"Visualization (NL2VIS) Benchmarks from NL2SQL Benchmarks.\nIn\",\"[182]\nP. Qi, Y. Zhang, Y. Zhang, and et al. Stanza: A Python Natural Language\"\n\"Proc. SIGMOD’21. ACM, 2021.\",\"Processing Toolkit for Many Human Languages.\nIn Proc. ACL’20. ACL.\"\n\"[152] Y. Ma, A. K. H. Tung, W. Wang, and et al. ScatterNet: A Deep Subjective\",\"[183] C. Qian, S. Sun, W. Cui, and et al. Retrieve-Then-Adapt: Example-based\"\n\"IEEE Trans. Vis.\nSimilarity Model for Visual Analysis of Scatterplots.\",\"IEEE Trans.\nAutomatic Generation for Proportion-related Infographics.\"\n\"Comput. Graph., 26(3):1562–1576, 2020.\",\"Vis. Comput. Graph., 27(2):443–452, 2021.\"\n\"[153]\nJ. Mackinlay.\nAutomating the design of graphical presentations of\",\"[184] X. Qian, E. Koh, F. Du, and et al. Generating Accurate Caption Units\"\n\"relational\ninformation. ACM Trans. Graph., 5(2):110–141, 1986.\",\"for Figure Captioning.\nIn Proc. WWW’21. ACM, 2021.\"\n\"[154]\nJ. Mackinlay, P. Hanrahan,\nand C. Stolte.\nShow Me: Automatic\",\"[185] X. Qian, R. A. Rossi, F. Du,\nand et\nal.\nLearning to Recommend\"\n\"Presentation for Visual Analysis.\nIEEE Trans. Vis. Comput. Graph.,\",\"Visualizations from Data.\nIn Proc. KDD’21. ACM, 2021.\"\n\"13(6):1137–1144, 2007.\",\"[186] X. Qian, R. A. Rossi, F. Du, and et al.\nPersonalized Visualization\"\n\"[155]\nS. Madan, Z. Bylinskii, M. Tancik, and et al. Synthetically Trained Icon\",\"Recommendation. arXiv, 2021.\"\n\"Proposals for Parsing and Summarizing Infographics. arXiv, 2018.\",\"[187] X. Qin, Y. Luo, N. Tang, and G. Li. Making data visualization more\"\n\"[156]\nS. Maji, S. S. Rout, and S. Choudhary. DCoM: A Deep Column Mapper\",\"efﬁcient and effective: a survey. VLDB J., 29(1):93–117, 2020.\"\n\"for Semantic Data Type Detection. arXiv, 2021.\",\"[188] A. Quamar, C. Lei, D. Miller, and et al. An Ontology-Based Conversation\"\n\"[157] C. Manning, M. Surdeanu, J. Bauer, and et al. The Stanford CoreNLP\",\"System for Knowledge Bases.\nIn Proc. SIGMOD’20. ACM, 2020.\"\n\"Natural Language Processing Toolkit.\nIn Proc. ACL’14. ACL, 2014.\",\"[189]\nS. Reinders, M. Butler, and K. Marriott. ”Hey Model!” - Natural User\"\n\"[158] M. Mathew, V. Bagal, and et al.\nInfographicVQA. arXiv, 2021.\",\"Interactions and Agency in Accessible Interactive 3D Models.\nIn Proc.\"\n\"[159] M. Matsushita, E. Maeda, and T. Kato. An interactive visualization\",\"CHI’20. ACM, 2020.\"\n\"Int. J.\nmethod of numerical data based on natural language requirements.\",\"[190] D. Ren, M. Brehmer, Bongshin Lee, and et al. ChartAccent: Annotation\"\n\"Hum. Comput. Stud., 60(4):469–488, 2004.\",\"for data-driven storytelling.\nIn Proc. PaciﬁcVis’17. IEEE, 2017.\"\n[160] L. McNabb and R. S. Laramee. Survey of Surveys (SoS) - Mapping,\"[191] A. Rind, W. Aigner, M. Wagner,\nand et\nal.\nTask Cube: A three-\"\nThe Landscape of Survey Papers in Information Visualization. Comput.,dimensional conceptual space of user tasks in visualization design and\n\"Graph. Forum, 36(3):589–617, 2017.\",\"evaluation.\nInf. Vis., 15(4):288–300, 2016.\"\n\"[161] R. Metoyer, B. Lee, N. Henry Riche, and M. Czerwinski. Understanding\",\"[192]\nS. F. Roth, J. Kolojejchick, J. Mattis, and J. Goldstein. Interactive graphic\"\n\"the verbal\nlanguage\nand structure of\nend-user descriptions of data\",\"design using automatic presentation knowledge.\nIn Proc. CHI’94. ACM.\"\n\"visualizations.\nIn Proc. CHI’12. ACM, 2012.\",\"[193] D. Saha, A. Floratou, K. Sankaranarayanan, and et al. ATHENA: An\"\n\"[162] R. Metoyer, Q. Zhi, B.\nJanczuk,\nand W. Scheirer.\nCoupling story\",ontologydriven system for natural language querying over relational data\nto visualization: Using textual analysis as a bridge between data and,\"stores. Proc. VLDB Endow., 9(12):1209–1220, 2016.\"\n\"interpretation.\nIn Proc. IUI’18. ACM, 2018.\",\"[194] B. Saket, A. Endert, and C. Demiralp. Task-Based Effectiveness of Basic\"\n\"[163] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient estimation of\",\"Visualizations.\nIEEE Trans. Vis. Comput. Graph., 25(7), 2019.\"\n\"word representations in vector space.\nIn Proc. ICLR’13, 2013.\",\"[195] A. Saktheeswaran, A. Srinivasan, and J. Stasko.\nTouch? Speech? or\"\n\"[164] G. A. Miller. WordNet: A Lexical Database for English. Commun. ACM,\",\"Touch and Speech? Investigating Multimodal\nInteraction for Visual\"\n\"38(11):39–41, 1995.\",\"Network Exploration and Analysis.\nIEEE Trans. Vis. Comput. Graph.,\"\n\"[165] M. Mitri.\nStory Analysis Using Natural Language Processing and\",\"26(6):2168–2179, 2020.\"\n\"Interactive Dashboards. J. Comput. Inf. Syst., pages 1–11, 2020.\",\"[196] A. Sarikaya and M. Gleicher. Scatterplots: Tasks, Data, and Designs.\"\n\"[166] V. O. Mittal, J. D. Moore, G. Carenini, and S. Roth. Describing Complex\",\"IEEE Trans. Vis. Comput. Graph., 24(1):402–412, 2018.\"\nCharts in Natural Language: A Caption Generation System. Comput.,\"[197] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer. Vega-Lite:\"\n\"Linguist., 24(3):431–467, 1998.\",\"A Grammar of Interactive Graphics.\nIEEE Trans. Vis. Comput. Graph.,\"\n\"[167]\nP. Moraes, G. Sina, K. McCoy, and S. Carberry. Generating Summaries\",\"23(1):341–350, 2017.\"\n\"of Line Graphs.\nIn Proc. INLG’14. ACL, 2014.\",\"[198] A. Satyanarayan, R. Russell,\nJ. Hoffswell,\nand J. Heer.\nReactive\"\n\"[168] D. Moritz, C. Wang, G. L. Nelson, and et al. Formalizing Visualization\",Vega: A Streaming Dataﬂow Architecture for Declarative Interactive\nDesign Knowledge as Constraints: Actionable and Extensible Models in,\"Visualization.\nIEEE Trans. Vis. Comput. Graph., 22(1):659–668, 2016.\"\n\"Draco.\nIEEE Trans. Vis. Comput. Graph., 25(1):438–448, 2019.\",\"[199] M. Savva, N. Kong, A. Chhajta,\nand et\nal.\nReVision: Automated\"\n[169] T. Murillo-Morales and K. Miesenberger. AUDiaL: A Natural Language,\"classiﬁcation, analysis and redesign of chart\nimages.\nIn Proc. UIST’11.\"\n\"Interface to Make Statistical Charts Accessible to Blind Persons.\nIn Lect.\",\"ACM, 2011.\"\n\"Notes Comput. Sci., pages 373–384. Springer, 2020.\",[200] E. Segel and J. Heer. Narrative visualization: Telling stories with data.\n\"[170] B. Mutlu, E. Veas, and C. Trattner. VizRec: Recommending personalized\",\"IEEE Trans. Vis. Comput. Graph., 16(6):1139–1148, 2010.\"\n\"visualizations. ACM Trans. Interact. Intell. Syst., 6(4):1–39, 2016.\",\"[201]\nP. Seipel, A. Stock, S. Santhanam, and et al. Speak to your Software\"\n\"[171] M. Nafari and C. Weaver.\nAugmenting Visualization with Natural\",Visualization—Exploring Component-Based Software Architectures\nLanguage Translation of Interaction: A Usability Study. Comput. Graph.,\"in Augmented Reality with\na Conversational\nInterface.\nIn Proc.\"\n\"Forum, 32(3):391–400, 2013.\",\"VISSOFT’19. IEEE, 2019.\"\n[172] M. Nafari and C. Weaver. Query2Question: Translating visualization,\"[202]\nJ. Sen, F. Ozcan, and et al. Natural Language Querying of Complex\"\n\"interaction into natural\nlanguage.\nIEEE Trans. Vis. Comput. Graph.,\",\"Business Intelligence Queries.\nIn Proc. SIGMOD’19. ACM, 2019.\"\n\"21(6):756–769, 2015.\",\"[203]\nJ. Seo\nand B. Shneiderman.\nA Rank-by-Feature Framework\nfor\"\n\"[173] A. Narechania, A. Fourney, and et al. DIY: Assessing the Correctness of\",\"Interactive Exploration of Multidimensional Data.\nInf. Vis., 2005.\"\n\"Natural Language to SQL Systems.\nIn Proc. IUI’21. ACM, 2021.\",\"[204] V.\nSetlur,\nS. Battersby,\nand T. Wong.\nGeoSneakPique\n: Visual\"\n\"[174] A. Narechania, A. Srinivasan, and J. Stasko. NL4DV: A Toolkit\nfor\",\"Autocompletion for Geospatial Queries.\nIn Proc. VIS’21. IEEE, 2021.\"\nGenerating Analytic Speciﬁcations for Data Visualization from Natural,\"[205] V. Setlur, S. E. Battersby, M. Tory, and et al. Eviza: A natural\nlanguage\"\n\"Language Queries.\nIEEE Trans. Vis. Comput. Graph., 27(2), 2021.\",\"interface for visual analysis.\nIn Proc. UIST’16. ACM, 2016.\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 19,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n19\n[206] V. Setlur, E. Hoque, D. H. Kim, and A. X. Chang.\nSneak pique:\nExploring autocompletion as a data discovery scaffold for supporting\nvisual analysis. In Proc. UIST’20. ACM, 2020.\n[207] V. Setlur and A. Kumar. Sentiﬁers: Interpreting Vague Intent Modiﬁers\nin Visual Analysis using Word Co-occurrence and Sentiment Analysis.\nIn Proc. VIS’20. IEEE, 2020.\n[208] V. Setlur, M. Tory, and A. Djalali. Inferencing underspeciﬁed natural\nlanguage utterances in visual analysis. In Proc. IUI’19. ACM, 2019.\n[209] R. Sevastjanova, F. Beck, B. Ell, and et al. Going beyond Visualization:\nVerbalization as Complementary Medium to Explain Machine Learning\nModels. In Proc. VISxAI’18. IEEE, 2018.\n[210] S. Shekarpour, E. Marx, A.-C. Ngonga Ngomo, and S. Auer. SINA:\nSemantic interpretation of user queries for question answering on\ninterlinked data. J. Web Semant., 30:39–51, 2015.\n[211] L. Shen, E. Shen, Z. Tai, and et al. TaskVis : Task-oriented Visualization\nRecommendation. In Proc. EuroVis’21. Eurographics, 2021.\n[212] D. Shi, Y. Shi, X. Xu, and et al. Task-Oriented Optimal Sequencing of\nVisualization Charts. In Proc. VDS’19. IEEE, 2019.\n[213] D. Shi, X. Xu, F. Sun, and et al. Calliope: Automatic Visual Data\nStory Generation from a Spreadsheet. IEEE Trans. Vis. Comput. Graph.,\n27(2):453–463, 2021.\n[214] Y. Shi, C. Bryan, S. Bhamidipati, and et al. MeetingVis: Visual Narratives\nto Assist in Recalling Meeting Context and Content. IEEE Trans. Vis.\nComput. Graph., 24(6):1918–1929, 2018.\n[215] X. Shu, A. Wu, J. Tang, and et al. What Makes a Data-GIF Understand-\nable? IEEE Trans. Vis. Comput. Graph., 27(2):1492–1502, 2021.\n[216] N. Siddiqui and E. Hoque. ConVisQA: A Natural Language Interface for\nVisually Exploring Online Conversations. In Proc. IV’20. IEEE, 2020.\n[217] T. Siddiqui, P. Luh, Z. Wang, and et al.\nShapeSearch: Flexible\npatternbased querying of trend line visualizations. Proc. VLDB Endow.,\n11(12):1962–1965, 2018.\n[218] T. Siddiqui, P. Luh, Z. Wang, and et al. From Sketching to Natural Lan-\nguage: Expressive Visual Querying for Accelerating Insight. SIGMOD\nRec., 50(1):51–58, 2021.\n[219] S. Sigtia, E. Marchi, S. Kajarekar, and et al. Multi-Task Learning for\nSpeaker Veriﬁcation and Voice Trigger Detection. In Proc. ICASSP’20.\nIEEE, 2020.\n[220] A. Simitsis, G. Koutrika, and Y. Ioannidis. Pr´ecis: from unstructured\nkeywords as queries to structured databases as answers.\nVLDB J.,\n17(1):117–149, 2007.\n[221] S. Sinclair, R. Miikkulainen, and S. Sinclair.\nSubsymbolic Natural\nLanguage Processing: An Integrated Model of Scripts, Lexicon, and\nMemory, volume 73. MIT Press, 1997.\n[222] H. Singh and S. Shekhar. STL-CQA: Structure-based Transformers\nwith Localization and Encoding for Chart Question Answering. In Proc.\nEMNLP’20. ACL, 2020.\n[223] A. Spreaﬁco and G. Carenini. Neural Data-Driven Captioning of Time-\nSeries Line Charts. In Proc. AVI’20. ACM, 2020.\n[224] A. Srinivasan, M. Dontcheva, E. Adar, and S. Walker. Discovering\nnatural language commands in multimodal interfaces. In Proc. IUI’19.\nACM, 2019.\n[225] A. Srinivasan, S. M. Drucker, A. Endert, and J. Stasko. Augmenting\nvisualizations with interactive data facts to facilitate interpretation and\ncommunication. IEEE Trans. Vis. Comput. Graph., 25(1):672–681, 2019.\n[226] A. Srinivasan, B. Lee, N. Henry Riche, and et al. InChorus: Designing\nConsistent Multimodal Interactions for Data Visualization on Tablet\nDevices. In Proc. CHI’20. ACM, 2020.\n[227] A. Srinivasan, B. Lee, and J. T. Stasko.\nInterweaving Multimodal\nInteraction with Flexible Unit Visualizations for Data Exploration. IEEE\nTrans. Vis. Comput. Graph., 14(8):1–15, 2020.\n[228] A. Srinivasan, N. Nyapathy, B. Lee, and et al. Collecting and Character-\nizing Natural Language Utterances for Specifying Data Visualizations.\nIn Proc. CHI’21, 2021.\n[229] A. Srinivasan and V. Setlur.\nSnowy:Recommending Utterances for\nConversational Visual Analysis. In Proc. UIST’21. ACM, 2021.\n[230] A. Srinivasan and J. Stasko. Natural Language Interfaces for Data\nAnalysis with Visualization: Considering What Has and Could Be Asked.\nIn Proc. EuroVis’17. Eurographics, 2017.\n[231] A. Srinivasan and J. Stasko. Orko: Facilitating Multimodal Interaction\nfor Visual Exploration and Analysis of Networks. IEEE Trans. Vis.\nComput. Graph., 24(1):511–521, 2018.\n[232] A. Srinivasan and J. Stasko. How to ask what to say?: Strategies for\nevaluating natural language interfaces for data visualization.\nIEEE\nComput. Graph. Appl., 40(4):96–103, 2020.\n[233] B. Steichen, G. Carenini, and C. Conati. User-adaptive information\nvisualization - Using eye gaze data to infer visualization tasks and user\ncognitive abilities. In Proc. IUI’13. ACM, 2013.\n[234] C. Stolte, D. Tang, and P. Hanrahan. Polaris: a system for query, analysis,\nand visualization of multidimensional relational databases. IEEE Trans.\nVis. Comput. Graph., 8(1):52–65, 2002.\n[235] D. Streeb, Y. Metz, U. Schlegel, and et al. Task-based Visual Interactive\nModeling: Decision Trees and Rule-based Classiﬁers. IEEE Trans. Vis.\nComput. Graph., XXX(XXX):1–18, 2021.\n[236] N. Stylianou and I. Vlahavas. A neural Entity Coreference Resolution\nreview. Expert Syst. Appl., 168(114466):1–20, 2021.\n[237] W. Su, X. Zhu, Y. Cao, and et al. VL-BERT: Pre-training of Generic\nVisual-Linguistic Representations. arXiv, 2019.\n[238] Y. Suhara, J. Li, Y. Li, and et al. Annotating Columns with Pre-trained\nLanguage Models. arXiv, 2021.\n[239] Y. Sun, J. Leigh, A. Johnson, and S. Lee. Articulate: A Semi-automated\nModel for Translating Natural Language Queries into Meaningful\nVisualizations. In Lect. Notes Comput. Sci. Springer, 2010.\n[240] J. R. Thompson, Z. Liu, and J. Stasko. Data Animator: Authoring\nExpressive Animated Data Graphics. In Proc. CHI’21. ACM, 2021.\n[241] C. Tong, R. Roberts, R. Borgo, and et al. Storytelling and visualization:\nAn extended survey. Inf., 9(3):1–42, 2018.\n[242] M. Tory and V. Setlur. Do What I Mean, Not What I Say! Design Con-\nsiderations for Supporting Intent and Context in Analytical Conversation.\nIn Proc. VAST’19. IEEE, 2019.\n[243] B. Tversky, J. B. Morrison, and M. Betrancourt. Animation: Can it\nfacilitate? Int. J. Hum. Comput. Stud., 57(4):247–262, 2002.\n[244] A. Van Dam. Post-WIMP User Interfaces. Commun. ACM, 40(2), 1997.\n[245] S. Van Den Elzen and J. J. Van Wijk. Small multiples, large singles: A\nnew approach for visual data exploration. Comput. Graph. Forum, 32(3\nPART2):191–200, 2013.\n[246] M. Vartak, S. Madden, A. Parameswaran, and N. Polyzotis. SEEDB:\nAutomatically generating query visualizations. Proc. VLDB Endow.,\n7(13):1581–1584, 2014.\n[247] M. Vartak, S. Rahman, S. Madden, and et al. SEEDB: Efﬁcient data-\ndriven visualization recommendations to support visual analytics. Proc.\nVLDB Endow., 8(13):2182–2193, 2015.\n[248] A. Vaswani, N. Shazeer, N. Parmar, and et al. Attention Is All You Need.\nIn Proc. NIPS’17. NIPS, 2017.\n[249] C. Wang, Y. Chen, Z. Xue, and et al. CogNet: Bridging Linguistic\nKnowledge, World Knowledge andCommonsense Knowledge. In Proc.\nAAAI’21. AAAI, 2020.\n[250] C. Wang, Y. Feng, R. Bodik, and et al.\nFalx: Synthesis-Powered\nVisualization Authoring. In Proc. CHI’21. ACM, 2021.\n[251] Q. Wang, Z. Chen, Y. Wang, and H. Qu. A Survey on ML4VIS: Applying\nMachine Learning Advances to Data Visualization. arXiv, 2021.\n[252] W. Wang, Y. Tian, H. Wang, and W.-S. Ku. A Natural Language Interface\nfor Database: Achieving Transfer-learnability Using Adversarial Method\nfor Question Understanding. In Proc. ICDE’20. IEEE, 2020.\n[253] Y. Wang, F. Han, L. Zhu, and et al. Line Graph or Scatter Plot? Automatic\nSelection of Methods for Visualizing Trends in Time Series. IEEE Trans.\nVis. Comput. Graph., 24(2):1141–1154, 2018.\n[254] Y. Wang, Z. Sun, H. Zhang, and et al. DataShot: Automatic Generation\nof Fact Sheets from Tabular Data. IEEE Trans. Vis. Comput. Graph.,\n26(1):895–905, 2020.\n[255] Y. Wang, W. M. White, and E. Andersen. PathViewer: Visualizing\nPathways through Student Data. In Proc. CHI’17. ACM, 2017.\n[256] Y. Wang, H. Zhang, H. Huang, and et al. InfoNice: Easy Creation of\nInformation Graphics. In Proc. CHI’18. ACM, 2018.\n[257] Z. Wang, L. Sundin, D. Murray-Rust, and B. Bach. Cheat Sheets for\nData Visualization Techniques. In Proc. CHI’20. ACM, 2020.\n[258] G. Wohlgenannt, D. Mouromtsev, D. Pavlov, and et al. A Comparative\nEvaluation of Visual and Natural Language Question Answering over\nLinked Data. In Proc.K’19. SCITEPRESS, 2019.\n[259] K. Wongsuphasawat, D. Moritz, A. Anand, and et al. Towards a general-\npurpose query language for visualization recommendation. In Proc.\nHILDA’16. ACM, 2016.\n[260] K. Wongsuphasawat, D. Moritz, A. Anand, and et al.\nVoyager:\nExploratory Analysis via Faceted Browsing of Visualization Recom-\nmendations. IEEE Trans. Vis. Comput. Graph., 22(1):649–658, 2016.\n[261] K. Wongsuphasawat, Z. Qu, D. Moritz, and et al. Voyager 2: Augmenting\nvisual analysis with partial view speciﬁcations. In Proc. CHI’17. ACM.\n[262] A. Wu, Y. Wang, X. Shu, and et al. AI4VIS: Survey on Artiﬁcial\nIntelligence Approaches for Data Visualization. IEEE Trans. Vis. Comput.\nGraph., pages 1–20, 2021.\n[263] A. Wu, Y. Wang, M. Zhou, and et al. MultiVision: Designing Analytical\nDashboards with Deep Learning Based Recommendation. IEEE Trans.\nVis. Comput. Graph., pages 1–11, 2021.\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 19\n[206] V. Setlur, E. Hoque, D. H. Kim, and A. X. Chang. Sneak pique: [234] C.Stolte,D.Tang,andP.Hanrahan.Polaris:asystemforquery,analysis,\nExploringautocompletionasadatadiscoveryscaffoldforsupporting andvisualizationofmultidimensionalrelationaldatabases. IEEETrans.\nvisualanalysis. InProc.UIST’20.ACM,2020. Vis.Comput.Graph.,8(1):52–65,2002.\n[207] V.SetlurandA.Kumar. Sentifiers:InterpretingVagueIntentModifiers [235] D.Streeb,Y.Metz,U.Schlegel,andetal. Task-basedVisualInteractive\ninVisualAnalysisusingWordCo-occurrenceandSentimentAnalysis. Modeling:DecisionTreesandRule-basedClassifiers. IEEETrans.Vis.\nInProc.VIS’20.IEEE,2020. Comput.Graph.,XXX(XXX):1–18,2021.\n[208] V.Setlur,M.Tory,andA.Djalali. Inferencingunderspecifiednatural [236] N.StylianouandI.Vlahavas. AneuralEntityCoreferenceResolution\nlanguageutterancesinvisualanalysis. InProc.IUI’19.ACM,2019. review. ExpertSyst.Appl.,168(114466):1–20,2021.\n[209] R.Sevastjanova,F.Beck,B.Ell,andetal. GoingbeyondVisualization: [237] W.Su,X.Zhu,Y.Cao,andetal. VL-BERT:Pre-trainingofGeneric\nVerbalizationasComplementaryMediumtoExplainMachineLearning Visual-LinguisticRepresentations. arXiv,2019.\nModels. InProc.VISxAI’18.IEEE,2018. [238] Y.Suhara,J.Li,Y.Li,andetal. AnnotatingColumnswithPre-trained\n[210] S.Shekarpour,E.Marx,A.-C.NgongaNgomo,andS.Auer. SINA: LanguageModels. arXiv,2021.\nSemantic interpretation of user queries for question answering on\n[239] Y.Sun,J.Leigh,A.Johnson,andS.Lee. Articulate:ASemi-automated\ninterlinkeddata. J.WebSemant.,30:39–51,2015.\nModel for Translating Natural Language Queries into Meaningful\n[211] L.Shen,E.Shen,Z.Tai,andetal.TaskVis:Task-orientedVisualization Visualizations. InLect.NotesComput.Sci.Springer,2010.\nRecommendation. InProc.EuroVis’21.Eurographics,2021.\n[240] J. R. Thompson, Z. Liu, and J. Stasko. Data Animator: Authoring\n[212] D.Shi,Y.Shi,X.Xu,andetal. Task-OrientedOptimalSequencingof\nExpressiveAnimatedDataGraphics. InProc.CHI’21.ACM,2021.\nVisualizationCharts. InProc.VDS’19.IEEE,2019.\n[241] C.Tong,R.Roberts,R.Borgo,andetal. Storytellingandvisualization:\n[213] D. Shi, X. Xu, F. Sun, and et al. Calliope: Automatic Visual Data\nAnextendedsurvey. Inf.,9(3):1–42,2018.\nStoryGenerationfromaSpreadsheet. IEEETrans.Vis.Comput.Graph.,\n[242] M.ToryandV.Setlur. DoWhatIMean,NotWhatISay!DesignCon-\n27(2):453–463,2021.\nsiderationsforSupportingIntentandContextinAnalyticalConversation.\n[214] Y.Shi,C.Bryan,S.Bhamidipati,andetal.MeetingVis:VisualNarratives\nInProc.VAST’19.IEEE,2019.\ntoAssistinRecallingMeetingContextandContent. IEEETrans.Vis.\n[243] B. Tversky, J. B. Morrison, and M. Betrancourt. Animation: Can it\nComput.Graph.,24(6):1918–1929,2018.\nfacilitate? Int.J.Hum.Comput.Stud.,57(4):247–262,2002.\n[215] X.Shu,A.Wu,J.Tang,andetal. WhatMakesaData-GIFUnderstand-\nable? IEEETrans.Vis.Comput.Graph.,27(2):1492–1502,2021. [244] A.VanDam. Post-WIMPUserInterfaces. Commun.ACM,40(2),1997.\n[216] N.SiddiquiandE.Hoque.ConVisQA:ANaturalLanguageInterfacefor [245] S.VanDenElzenandJ.J.VanWijk. Smallmultiples,largesingles:A\nVisuallyExploringOnlineConversations. InProc.IV’20.IEEE,2020. newapproachforvisualdataexploration. Comput.Graph.Forum,32(3\n[217] T. Siddiqui, P. Luh, Z. Wang, and et al. ShapeSearch: Flexible PART2):191–200,2013.\npatternbasedqueryingoftrendlinevisualizations. Proc.VLDBEndow., [246] M.Vartak,S.Madden,A.Parameswaran,andN.Polyzotis. SEEDB:\n11(12):1962–1965,2018. Automatically generating query visualizations. Proc. VLDB Endow.,\n[218] T.Siddiqui,P.Luh,Z.Wang,andetal. FromSketchingtoNaturalLan- 7(13):1581–1584,2014.\nguage:ExpressiveVisualQueryingforAcceleratingInsight. SIGMOD [247] M.Vartak,S.Rahman,S.Madden,andetal. SEEDB:Efficientdata-\nRec.,50(1):51–58,2021. drivenvisualizationrecommendationstosupportvisualanalytics. Proc.\n[219] S.Sigtia,E.Marchi,S.Kajarekar,andetal. Multi-TaskLearningfor VLDBEndow.,8(13):2182–2193,2015.\nSpeakerVerificationandVoiceTriggerDetection. InProc.ICASSP’20. [248] A.Vaswani,N.Shazeer,N.Parmar,andetal.AttentionIsAllYouNeed.\nIEEE,2020. InProc.NIPS’17.NIPS,2017.\n[220] A.Simitsis,G.Koutrika,andY.Ioannidis. Pre´cis:fromunstructured [249] C. Wang, Y. Chen, Z. Xue, and et al. CogNet: Bridging Linguistic\nkeywords as queries to structured databases as answers. VLDB J., Knowledge,WorldKnowledgeandCommonsenseKnowledge. InProc.\n17(1):117–149,2007. AAAI’21.AAAI,2020.\n[221] S. Sinclair, R. Miikkulainen, and S. Sinclair. Subsymbolic Natural [250] C. Wang, Y. Feng, R. Bodik, and et al. Falx: Synthesis-Powered\nLanguage Processing: An Integrated Model of Scripts, Lexicon, and VisualizationAuthoring. InProc.CHI’21.ACM,2021.\nMemory,volume73. MITPress,1997. [251] Q.Wang,Z.Chen,Y.Wang,andH.Qu.ASurveyonML4VIS:Applying\n[222] H. Singh and S. Shekhar. STL-CQA: Structure-based Transformers MachineLearningAdvancestoDataVisualization. arXiv,2021.\nwithLocalizationandEncodingforChartQuestionAnswering. InProc. [252] W.Wang,Y.Tian,H.Wang,andW.-S.Ku.ANaturalLanguageInterface\nEMNLP’20.ACL,2020. forDatabase:AchievingTransfer-learnabilityUsingAdversarialMethod\n[223] A.SpreaficoandG.Carenini. NeuralData-DrivenCaptioningofTime- forQuestionUnderstanding. InProc.ICDE’20.IEEE,2020.\nSeriesLineCharts. InProc.AVI’20.ACM,2020.\n[253] Y.Wang,F.Han,L.Zhu,andetal.LineGraphorScatterPlot?Automatic\n[224] A. Srinivasan, M. Dontcheva, E. Adar, and S. Walker. Discovering SelectionofMethodsforVisualizingTrendsinTimeSeries.IEEETrans.\nnaturallanguagecommandsinmultimodalinterfaces. InProc.IUI’19. Vis.Comput.Graph.,24(2):1141–1154,2018.\nACM,2019.\n[254] Y.Wang,Z.Sun,H.Zhang,andetal. DataShot:AutomaticGeneration\n[225] A.Srinivasan,S.M.Drucker,A.Endert,andJ.Stasko. Augmenting\nofFactSheetsfromTabularData. IEEETrans.Vis.Comput.Graph.,\nvisualizationswithinteractivedatafactstofacilitateinterpretationand\n26(1):895–905,2020.\ncommunication.IEEETrans.Vis.Comput.Graph.,25(1):672–681,2019.\n[255] Y. Wang, W. M. White, and E. Andersen. PathViewer: Visualizing\n[226] A.Srinivasan,B.Lee,N.HenryRiche,andetal. InChorus:Designing\nPathwaysthroughStudentData. InProc.CHI’17.ACM,2017.\nConsistent Multimodal Interactions for Data Visualization on Tablet\n[256] Y.Wang,H.Zhang,H.Huang,andetal. InfoNice:EasyCreationof\nDevices. InProc.CHI’20.ACM,2020.\nInformationGraphics. InProc.CHI’18.ACM,2018.\n[227] A. Srinivasan, B. Lee, and J. T. Stasko. Interweaving Multimodal\n[257] Z.Wang,L.Sundin,D.Murray-Rust,andB.Bach. CheatSheetsfor\nInteractionwithFlexibleUnitVisualizationsforDataExploration.IEEE\nDataVisualizationTechniques. InProc.CHI’20.ACM,2020.\nTrans.Vis.Comput.Graph.,14(8):1–15,2020.\n[228] A.Srinivasan,N.Nyapathy,B.Lee,andetal. CollectingandCharacter- [258] G.Wohlgenannt,D.Mouromtsev,D.Pavlov,andetal. AComparative\nizingNaturalLanguageUtterancesforSpecifyingDataVisualizations. EvaluationofVisualandNaturalLanguageQuestionAnsweringover\nInProc.CHI’21,2021. LinkedData. InProc.K’19.SCITEPRESS,2019.\n[229] A. Srinivasan and V. Setlur. Snowy:Recommending Utterances for [259] K.Wongsuphasawat,D.Moritz,A.Anand,andetal. Towardsageneral-\nConversationalVisualAnalysis. InProc.UIST’21.ACM,2021. purpose query language for visualization recommendation. In Proc.\n[230] A. Srinivasan and J. Stasko. Natural Language Interfaces for Data HILDA’16.ACM,2016.\nAnalysiswithVisualization:ConsideringWhatHasandCouldBeAsked. [260] K. Wongsuphasawat, D. Moritz, A. Anand, and et al. Voyager:\nInProc.EuroVis’17.Eurographics,2017. Exploratory Analysis via Faceted Browsing of Visualization Recom-\n[231] A.SrinivasanandJ.Stasko. Orko:FacilitatingMultimodalInteraction mendations. IEEETrans.Vis.Comput.Graph.,22(1):649–658,2016.\nfor Visual Exploration and Analysis of Networks. IEEE Trans. Vis. [261] K.Wongsuphasawat,Z.Qu,D.Moritz,andetal.Voyager2:Augmenting\nComput.Graph.,24(1):511–521,2018. visualanalysiswithpartialviewspecifications. InProc.CHI’17.ACM.\n[232] A.SrinivasanandJ.Stasko. Howtoaskwhattosay?:Strategiesfor [262] A. Wu, Y. Wang, X. Shu, and et al. AI4VIS: Survey on Artificial\nevaluating natural language interfaces for data visualization. IEEE IntelligenceApproachesforDataVisualization.IEEETrans.Vis.Comput.\nComput.Graph.Appl.,40(4):96–103,2020. Graph.,pages1–20,2021.\n[233] B. Steichen, G. Carenini, and C. Conati. User-adaptive information [263] A.Wu,Y.Wang,M.Zhou,andetal. MultiVision:DesigningAnalytical\nvisualization-Usingeyegazedatatoinfervisualizationtasksanduser DashboardswithDeepLearningBasedRecommendation. IEEETrans.\ncognitiveabilities. InProc.IUI’13.ACM,2013. Vis.Comput.Graph.,pages1–11,2021.",
    "tables_extraction_text_csv": "0\n\"IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n19\"\n\"[206] V. Setlur, E. Hoque, D. H. Kim,\nand A. X. Chang.\nSneak pique:\n[234] C. Stolte, D. Tang, and P. Hanrahan. Polaris: a system for query, analysis,\"\n\"IEEE Trans.\nExploring autocompletion as a data discovery scaffold for supporting\nand visualization of multidimensional relational databases.\"\n\"visual analysis.\nIn Proc. UIST’20. ACM, 2020.\nVis. Comput. Graph., 8(1):52–65, 2002.\"\n\"[207] V. Setlur and A. Kumar. Sentiﬁers: Interpreting Vague Intent Modiﬁers\n[235] D. Streeb, Y. Metz, U. Schlegel, and et al. Task-based Visual Interactive\"\n\"in Visual Analysis using Word Co-occurrence and Sentiment Analysis.\nIEEE Trans. Vis.\nModeling: Decision Trees and Rule-based Classiﬁers.\"\n\"In Proc. VIS’20. IEEE, 2020.\nComput. Graph., XXX(XXX):1–18, 2021.\"\n\"[208] V. Setlur, M. Tory, and A. Djalali.\nInferencing underspeciﬁed natural\n[236] N. Stylianou and I. Vlahavas. A neural Entity Coreference Resolution\"\n\"language utterances in visual analysis.\nIn Proc. IUI’19. ACM, 2019.\nreview. Expert Syst. Appl., 168(114466):1–20, 2021.\"\n\"[209] R. Sevastjanova, F. Beck, B. Ell, and et al. Going beyond Visualization:\n[237] W. Su, X. Zhu, Y. Cao, and et al. VL-BERT: Pre-training of Generic\"\n\"Verbalization as Complementary Medium to Explain Machine Learning\nVisual-Linguistic Representations. arXiv, 2019.\"\n\"Models.\nIn Proc. VISxAI’18. IEEE, 2018.\"\n\"[238] Y. Suhara, J. Li, Y. Li, and et al. Annotating Columns with Pre-trained\"\n\"[210]\nS. Shekarpour, E. Marx, A.-C. Ngonga Ngomo, and S. Auer.\nSINA:\"\n\"Language Models. arXiv, 2021.\"\n\"Semantic\ninterpretation\nof\nuser\nqueries\nfor\nquestion\nanswering\non\"\n\"[239] Y. Sun, J. Leigh, A. Johnson, and S. Lee. Articulate: A Semi-automated\"\n\"interlinked data. J. Web Semant., 30:39–51, 2015.\"\n\"Model\nfor Translating Natural Language Queries\ninto Meaningful\"\n\"[211] L. Shen, E. Shen, Z. Tai, and et al. TaskVis : Task-oriented Visualization\"\n\"Visualizations.\nIn Lect. Notes Comput. Sci. Springer, 2010.\"\n\"Recommendation.\nIn Proc. EuroVis’21. Eurographics, 2021.\"\n\"[240]\nJ. R. Thompson, Z. Liu, and J. Stasko.\nData Animator: Authoring\"\n\"[212] D. Shi, Y. Shi, X. Xu, and et al. Task-Oriented Optimal Sequencing of\"\n\"Expressive Animated Data Graphics.\nIn Proc. CHI’21. ACM, 2021.\"\n\"Visualization Charts.\nIn Proc. VDS’19. IEEE, 2019.\"\n\"[241] C. Tong, R. Roberts, R. Borgo, and et al. Storytelling and visualization:\"\n\"Calliope: Automatic Visual Data\n[213] D. Shi, X. Xu, F. Sun, and et al.\"\n\"An extended survey.\nInf., 9(3):1–42, 2018.\"\n\"Story Generation from a Spreadsheet.\nIEEE Trans. Vis. Comput. Graph.,\"\n\"[242] M. Tory and V. Setlur. Do What I Mean, Not What I Say! Design Con-\"\n\"27(2):453–463, 2021.\"\nsiderations for Supporting Intent and Context in Analytical Conversation.\n\"[214] Y. Shi, C. Bryan, S. Bhamidipati, and et al. MeetingVis: Visual Narratives\"\n\"In Proc. VAST’19. IEEE, 2019.\"\n\"IEEE Trans. Vis.\nto Assist\nin Recalling Meeting Context and Content.\"\n\"[243] B. Tversky, J. B. Morrison, and M. Betrancourt. Animation: Can it\"\n\"Comput. Graph., 24(6):1918–1929, 2018.\"\n\"facilitate? Int. J. Hum. Comput. Stud., 57(4):247–262, 2002.\"\n\"[215] X. Shu, A. Wu, J. Tang, and et al. What Makes a Data-GIF Understand-\"\n\"[244] A. Van Dam. Post-WIMP User Interfaces. Commun. ACM, 40(2), 1997.\"\n\"able? IEEE Trans. Vis. Comput. Graph., 27(2):1492–1502, 2021.\"\n\"[245]\nS. Van Den Elzen and J. J. Van Wijk. Small multiples,\nlarge singles: A\"\n[216] N. Siddiqui and E. Hoque. ConVisQA: A Natural Language Interface for\n\"new approach for visual data exploration. Comput. Graph. Forum, 32(3\"\n\"Visually Exploring Online Conversations.\nIn Proc. IV’20. IEEE, 2020.\"\n\"PART2):191–200, 2013.\"\n\"[217] T. Siddiqui, P. Luh, Z. Wang,\nand\net\nal.\nShapeSearch: Flexible\"\n\"[246] M. Vartak, S. Madden, A. Parameswaran, and N. Polyzotis. SEEDB:\npatternbased querying of trend line visualizations. Proc. VLDB Endow.,\"\n\"Automatically generating query visualizations.\nProc. VLDB Endow.,\n11(12):1962–1965, 2018.\"\n\"7(13):1581–1584, 2014.\n[218] T. Siddiqui, P. Luh, Z. Wang, and et al. From Sketching to Natural Lan-\"\n\"[247] M. Vartak, S. Rahman, S. Madden, and et al. SEEDB: Efﬁcient data-\nguage: Expressive Visual Querying for Accelerating Insight. SIGMOD\"\n\"driven visualization recommendations to support visual analytics. Proc.\nRec., 50(1):51–58, 2021.\"\n\"VLDB Endow., 8(13):2182–2193, 2015.\n[219]\nS. Sigtia, E. Marchi, S. Kajarekar, and et al. Multi-Task Learning for\"\n\"[248] A. Vaswani, N. Shazeer, N. Parmar, and et al. Attention Is All You Need.\nSpeaker Veriﬁcation and Voice Trigger Detection.\nIn Proc. ICASSP’20.\"\n\"In Proc. NIPS’17. NIPS, 2017.\nIEEE, 2020.\"\n\"[220] A. Simitsis, G. Koutrika, and Y. Ioannidis. Pr´ecis: from unstructured\nCogNet: Bridging Linguistic\n[249] C. Wang, Y. Chen, Z. Xue, and et al.\"\n\"keywords as queries\nto structured databases as answers.\nVLDB J.,\nKnowledge, World Knowledge andCommonsense Knowledge.\nIn Proc.\"\n\"17(1):117–149, 2007.\nAAAI’21. AAAI, 2020.\"\n\"Subsymbolic Natural\n[221]\nS. Sinclair, R. Miikkulainen, and S. Sinclair.\n[250] C. Wang, Y. Feng, R. Bodik,\nand et\nal.\nFalx: Synthesis-Powered\"\n\"Language Processing: An Integrated Model of Scripts, Lexicon, and\nVisualization Authoring.\nIn Proc. CHI’21. ACM, 2021.\"\n\"Memory, volume 73. MIT Press, 1997.\n[251] Q. Wang, Z. Chen, Y. Wang, and H. Qu. A Survey on ML4VIS: Applying\"\n\"[222] H. Singh and S. Shekhar.\nSTL-CQA: Structure-based Transformers\nMachine Learning Advances to Data Visualization. arXiv, 2021.\"\n\"with Localization and Encoding for Chart Question Answering.\nIn Proc.\"\n\"[252] W. Wang, Y. Tian, H. Wang, and W.-S. Ku. A Natural Language Interface\"\n\"EMNLP’20. ACL, 2020.\"\nfor Database: Achieving Transfer-learnability Using Adversarial Method\n[223] A. Spreaﬁco and G. Carenini. Neural Data-Driven Captioning of Time-\n\"for Question Understanding.\nIn Proc. ICDE’20. IEEE, 2020.\"\n\"Series Line Charts.\nIn Proc. AVI’20. ACM, 2020.\"\n\"[253] Y. Wang, F. Han, L. Zhu, and et al. Line Graph or Scatter Plot? Automatic\"\n\"[224] A. Srinivasan, M. Dontcheva, E. Adar, and S. Walker. Discovering\"\n\"IEEE Trans.\nSelection of Methods for Visualizing Trends in Time Series.\"\n\"natural language commands in multimodal interfaces.\nIn Proc. IUI’19.\"\n\"Vis. Comput. Graph., 24(2):1141–1154, 2018.\"\n\"ACM, 2019.\"\n\"[254] Y. Wang, Z. Sun, H. Zhang, and et al. DataShot: Automatic Generation\"\n\"[225] A. Srinivasan, S. M. Drucker, A. Endert, and J. Stasko. Augmenting\"\n\"of Fact Sheets from Tabular Data.\nIEEE Trans. Vis. Comput. Graph.,\"\nvisualizations with interactive data facts to facilitate interpretation and\n\"26(1):895–905, 2020.\"\n\"communication.\nIEEE Trans. Vis. Comput. Graph., 25(1):672–681, 2019.\"\n\"[255] Y. Wang, W. M. White, and E. Andersen.\nPathViewer: Visualizing\"\n\"[226] A. Srinivasan, B. Lee, N. Henry Riche, and et al.\nInChorus: Designing\"\n\"Pathways through Student Data.\nIn Proc. CHI’17. ACM, 2017.\"\n\"Consistent Multimodal\nInteractions for Data Visualization on Tablet\"\n\"[256] Y. Wang, H. Zhang, H. Huang, and et al.\nInfoNice: Easy Creation of\"\n\"Devices.\nIn Proc. CHI’20. ACM, 2020.\"\n\"Information Graphics.\nIn Proc. CHI’18. ACM, 2018.\"\n\"[227] A. Srinivasan, B. Lee, and J. T. Stasko.\nInterweaving Multimodal\"\n\"[257] Z. Wang, L. Sundin, D. Murray-Rust, and B. Bach. Cheat Sheets for\"\n\"IEEE\nInteraction with Flexible Unit Visualizations for Data Exploration.\"\n\"Data Visualization Techniques.\nIn Proc. CHI’20. ACM, 2020.\"\n\"Trans. Vis. Comput. Graph., 14(8):1–15, 2020.\"\n\"[258] G. Wohlgenannt, D. Mouromtsev, D. Pavlov, and et al. A Comparative\"\n\"[228] A. Srinivasan, N. Nyapathy, B. Lee, and et al. Collecting and Character-\"\nEvaluation of Visual and Natural Language Question Answering over\nizing Natural Language Utterances for Specifying Data Visualizations.\n\"Linked Data.\nIn Proc.K’19. SCITEPRESS, 2019.\"\n\"In Proc. CHI’21, 2021.\"\n\"[259] K. Wongsuphasawat, D. Moritz, A. Anand, and et al. Towards a general-\"\n\"[229] A. Srinivasan and V. Setlur.\nSnowy:Recommending Utterances\nfor\"\n\"purpose query language for visualization recommendation.\nIn Proc.\"\n\"Conversational Visual Analysis.\nIn Proc. UIST’21. ACM, 2021.\"\n\"HILDA’16. ACM, 2016.\"\n\"Natural Language Interfaces\nfor Data\n[230] A. Srinivasan and J. Stasko.\"\n\"[260] K. Wongsuphasawat, D. Moritz, A. Anand,\nand\net\nal.\nVoyager:\nAnalysis with Visualization: Considering What Has and Could Be Asked.\"\n\"Exploratory Analysis via Faceted Browsing of Visualization Recom-\nIn Proc. EuroVis’17. Eurographics, 2017.\"\n\"mendations.\nIEEE Trans. Vis. Comput. Graph., 22(1):649–658, 2016.\"\n[231] A. Srinivasan and J. Stasko. Orko: Facilitating Multimodal Interaction\n\"[261] K. Wongsuphasawat, Z. Qu, D. Moritz, and et al. Voyager 2: Augmenting\nIEEE Trans. Vis.\nfor Visual Exploration and Analysis of Networks.\"\n\"visual analysis with partial view speciﬁcations.\nIn Proc. CHI’17. ACM.\nComput. Graph., 24(1):511–521, 2018.\"\n\"[262] A. Wu, Y. Wang, X. Shu, and et al.\nAI4VIS: Survey on Artiﬁcial\nto say?: Strategies for\n[232] A. Srinivasan and J. Stasko. How to ask what\"\n\"Intelligence Approaches for Data Visualization. IEEE Trans. Vis. Comput.\nIEEE\nevaluating natural\nlanguage interfaces\nfor data visualization.\"\n\"Graph., pages 1–20, 2021.\nComput. Graph. Appl., 40(4):96–103, 2020.\"\n\"[233] B. Steichen, G. Carenini, and C. Conati. User-adaptive information\n[263] A. Wu, Y. Wang, M. Zhou, and et al. MultiVision: Designing Analytical\"\n\"IEEE Trans.\nvisualization - Using eye gaze data to infer visualization tasks and user\nDashboards with Deep Learning Based Recommendation.\"\n\"cognitive abilities.\nIn Proc. IUI’13. ACM, 2013.\nVis. Comput. Graph., pages 1–11, 2021.\"\n",
    "ocr_extraction_text": ""
  },
  {
    "pdf_filename": "LiteratureReview.pdf",
    "page_number": 20,
    "contains_tables": true,
    "contains_images_of_text": false,
    "contains_images_of_non_text": false,
    "pymupdf_extraction_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n20\n[264] A. Wu, L. Xie, B. Lee, and et al.\nLearning to Automate Chart\nLayout Conﬁgurations Using Crowdsourced Paired Comparison. In\nProc. CHI’21. ACM, 2021.\n[265] H. Xia. Crosspower: Bridging graphics and linguistics. In Proc. UIST’20.\nACM, 2020.\n[266] H. Xia, J. Jacobs, and M. Agrawala. Crosscast: Adding Visuals to Audio\nTravel Podcasts. In Proc. UIST’20. ACM, 2020.\n[267] Y. Xian, H. Zhao, T. Y. Lee, and et al. EXACTA: Explainable Column\nAnnotation. In Proc. KDD’21. ACM, 2021.\n[268] X. Xu, C. Liu, and D. Song. SQLNet: Generating Structured Queries\nFrom Natural Language Without Reinforcement Learning. arXiv, 2017.\n[269] S. Yagcioglu, A. Erdem, E. Erdem, and N. Ikizler-Cinbis. RecipeQA: A\nChallenge Dataset for Multimodal Comprehension of Cooking Recipes.\nIn Proc. EMNLP’18. ACL, 2018.\n[270] N. Yaghmazadeh, Y. Wang, I. Dillig, and T. Dillig. SQLizer: query\nsynthesis from natural language. Proc. ACM Program. Lang., 2017.\n[271] T. Young, D. Hazarika, S. Poria, and E. Cambria. Recent Trends in Deep\nLearning Based Natural Language Processing. IEEE Comput. Intell.\nMag., 13(3):55–75, 2018.\n[272] B. Yu and C. T. Silva. VisFlow - Web-based Visualization Framework\nfor Tabular Data with a Subset Flow Model. IEEE Trans. Vis. Comput.\nGraph., 23(1):251–260, 2017.\n[273] B. Yu and C. T. Silva. FlowSense: A Natural Language Interface for\nVisual Data Exploration within a Dataﬂow System. IEEE Trans. Vis.\nComput. Graph., 26(1):1–11, 2020.\n[274] T. Yu, R. Zhang, K. Yang, and et al. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic parsing and text-to-SQL\ntask. In Proc. EMNLP’18. ACL, 2018.\n[275] L. Yuan, Z. Zhou, J. Zhao, and et al.\nInfoColorizer: Interactive\nRecommendation of Color Palettes for Infographics. IEEE Trans. Vis.\nComput. Graph., 2626:1–15, 2021.\n[276] R. Zehrung, A. Singhal, M. Correll, and L. Battle. Vis Ex Machina:\nAn Analysis of Trust in Human versus Algorithmically Generated\nVisualization Recommendations. In Proc. CHI’21. ACM, 2021.\n[277] Z. Zeng, P. Moh, F. Du, and et al. An Evaluation-Focused Framework for\nVisualization Recommendation Algorithms. IEEE Trans. Vis. Comput.\nGraph., pages 1–11, 2021.\n[278] D. Zhang, Y. Suhara, J. Li, and et al. Sato: Contextual semantic type\ndetection in tables. Proc. VLDB Endow., 13(11):1835–1848, 2020.\n[279] H. Zhang, Y. Song, Y. Song, and D. Yu. Knowledge-aware Pronoun\nCoreference Resolution. In Proc. ACL’19. ACL, 2019.\n[280] H. Zhang, X. Zhao, and Y. Song. A Brief Survey and Comparative Study\nof Recent Development of Pronoun Coreference Resolution. arXiv, 2020.\n[281] S. Zhang, L. Yao, A. Sun, and Y. Tay.\nDeep Learning Based\nRecommender System. ACM Comput. Surv., 52(1):1–38, 2019.\n[282] Y. Zhang, P. Pasupat, and P. Liang. Macro Grammars and Holistic\nTriggering for Efﬁcient Semantic Parsing. In Proc. EMNLP’17. ACL.\n[283] Z. Zhang, Y. Gu, X. Han, and et al. CPM-2: Large-scale Cost-effective\nPre-trained Language Models. arXiv, 2021.\n[284] Zhen Wen, M. Zhou, and V. Aggarwal. An optimization-based approach\nto dynamic visual context management. In Proc. InfoVis’05. IEEE, 2005.\n[285] W. Zheng, H. Cheng, L. Zou, and et al.\nNatural Language Ques-\ntion/Answering. In Proc. CIKM’17. ACM, 2017.\n[286] X. Zheng, X. Qiao, Y. Cao, and R. W. H. Lau. Content-aware generative\nmodeling of graphic design layouts. ACM Trans. Graph., 2019.\n[287] V. Zhong and et al. Seq2SQL: Generating Structured Queries from\nNatural Language using Reinforcement Learning. arXiv, 2017.\n[288] M. Zhou, Q. Li, X. He, and et al. Table2Charts: Recommending Charts\nby Learning Shared Table Representations. In Proc. KDD’21. ACM.\n[289] S. Zhu, G. Sun, Q. Jiang, and et al. A survey on automatic infographics\nand visualization recommendations. Vis. Informatics, 4(3):24–40, 2020.\n[290] F. ˝Ozcan, A. Quamar, J. Sen, and et al. State of the Art and Open\nChallenges in Natural Language Interfaces to Data. In Proc. SIGMOD’20.\nACM, 2020.\nLeixian Shen received his bachelor’s degree in\nSoftware Engineering from the Nanjing University\nof Posts and Telecommunications in 2020. He\nis currently a master student in the School of\nSoftware, Tsinghua University, Beijing, China. His\nresearch interests include data visualization and\nhuman computer interaction.\nEnya Shen received the BSc degree from Nan-\njing University of Aeronautics and Astronautics,\nin 2008, and MSc and PhD degree from National\nUniversity of Defense Technology, in 2010 and\n2014, respectively. He works as a research assis-\ntant in Tsinghua University. His research interests\ninclude data visualization, human computer inter-\naction, augmented reality, and geometry model-\ning.\nYuyu Luo received his bachelor’s degree in\nSoftware Engineering from the University of Elec-\ntronic Science and Technology of China in 2018.\nHe is currently a PhD student in the Department\nof Computer Science, Tsinghua University, Bei-\njing, China. His research interests include data\nvisualization and data cleaning.\nXiaocong Yang is a fourth-year undergraduate\nworking towards a BS degree at Tsinghua Uni-\nversity, and plan to apply for a PhD position in\nComputer Science. His research interests include\nnatural language processing and machine learn-\ning theory.\nXuming Hu received the BE degree in Computer\nScience and Technology, Dalian University of\nTechnology. He is working toward the PhD degree\nat Tsinghua University. His research interests\ninclude natural language processing and infor-\nmation extraction.\nXiongshuai Zhang received Bachelor’s Degree\nfrom South China University of Techinology,\nChina, in 2019. He is now a master student\nwith the School of Software, Tsinghua Univer-\nsity, China. His research interests include data\nvisualization and computer graphics.\nZhiwei Tai received his BS degree from the\nSchool of Software, Tsinghua University in 2020.\nHe is currently working toward the Master’s\ndegree in Software Engineering at Tsinghua\nUniversity. His research interests include data\nvisualization, digital twin, and augmented reality.\nJianmin Wang received his bachelor’s degree\nfrom Peking University, China, in 1990, and the\nME and PhD degrees in computer software from\nTsinghua University, China, in 1992 and 1995,\nrespectively. He is a full professor with the School\nof Software, Tsinghua University. His research\ninterests include big data management and large-\nscale data analytics. He is leading to develop\na big data management system in the National\nEngineering Lab for Big Data Software.\n",
    "pdfplumber_extraction_text": "IEEETRANSACTIONSONVISUALIZATIONANDCOMPUTERGRAPHICS,VOL.XX,NO.X,XX2022 20\n[264] A. Wu, L. Xie, B. Lee, and et al. Learning to Automate Chart EnyaShenreceivedtheBScdegreefromNan-\nLayout Configurations Using Crowdsourced Paired Comparison. In jingUniversityofAeronauticsandAstronautics,\nProc.CHI’21.ACM,2021. in2008,andMScandPhDdegreefromNational\n[265] H.Xia.Crosspower:Bridginggraphicsandlinguistics.InProc.UIST’20. UniversityofDefenseTechnology,in2010and\nACM,2020. 2014,respectively.Heworksasaresearchassis-\n[266] H.Xia,J.Jacobs,andM.Agrawala.Crosscast:AddingVisualstoAudio tantinTsinghuaUniversity.Hisresearchinterests\nTravelPodcasts. InProc.UIST’20.ACM,2020. includedatavisualization,humancomputerinter-\n[267] Y.Xian,H.Zhao,T.Y.Lee,andetal. EXACTA:ExplainableColumn action,augmentedreality,andgeometrymodel-\nAnnotation. InProc.KDD’21.ACM,2021. ing.\n[268] X.Xu,C.Liu,andD.Song. SQLNet:GeneratingStructuredQueries\nFromNaturalLanguageWithoutReinforcementLearning. arXiv,2017.\n[269] S.Yagcioglu,A.Erdem,E.Erdem,andN.Ikizler-Cinbis. RecipeQA:A Yuyu Luo received his bachelor’s degree in\nChallengeDatasetforMultimodalComprehensionofCookingRecipes. SoftwareEngineeringfromtheUniversityofElec-\nInProc.EMNLP’18.ACL,2018. tronicScienceandTechnologyofChinain2018.\n[270] N. Yaghmazadeh, Y. Wang, I. Dillig, and T. Dillig. SQLizer: query HeiscurrentlyaPhDstudentintheDepartment\nsynthesisfromnaturallanguage. Proc.ACMProgram.Lang.,2017. ofComputerScience,TsinghuaUniversity,Bei-\n[271] T.Young,D.Hazarika,S.Poria,andE.Cambria.RecentTrendsinDeep jing,China.Hisresearchinterestsincludedata\nLearningBasedNaturalLanguageProcessing. IEEEComput.Intell. visualizationanddatacleaning.\nMag.,13(3):55–75,2018.\n[272] B.YuandC.T.Silva. VisFlow-Web-basedVisualizationFramework\nforTabularDatawithaSubsetFlowModel. IEEETrans.Vis.Comput.\nGraph.,23(1):251–260,2017.\n[273] B.YuandC.T.Silva. FlowSense:ANaturalLanguageInterfacefor XiaocongYangisafourth-yearundergraduate\nVisualDataExplorationwithinaDataflowSystem. IEEETrans.Vis. workingtowardsaBSdegreeatTsinghuaUni-\nComput.Graph.,26(1):1–11,2020. versity,andplantoapplyforaPhDpositionin\n[274] T.Yu,R.Zhang,K.Yang,andetal.Spider:Alarge-scalehuman-labeled ComputerScience.Hisresearchinterestsinclude\ndatasetforcomplexandcross-domainsemanticparsingandtext-to-SQL naturallanguageprocessingandmachinelearn-\ntask. InProc.EMNLP’18.ACL,2018. ingtheory.\n[275] L. Yuan, Z. Zhou, J. Zhao, and et al. InfoColorizer: Interactive\nRecommendationofColorPalettesforInfographics. IEEETrans.Vis.\nComput.Graph.,2626:1–15,2021.\n[276] R.Zehrung,A.Singhal,M.Correll,andL.Battle. VisExMachina:\nAn Analysis of Trust in Human versus Algorithmically Generated\nVisualizationRecommendations. InProc.CHI’21.ACM,2021. XumingHureceivedtheBEdegreeinComputer\n[277] Z.Zeng,P.Moh,F.Du,andetal.AnEvaluation-FocusedFrameworkfor Science and Technology, Dalian University of\nVisualizationRecommendationAlgorithms. IEEETrans.Vis.Comput. Technology.HeisworkingtowardthePhDdegree\nGraph.,pages1–11,2021. at Tsinghua University. His research interests\n[278] D.Zhang,Y.Suhara,J.Li,andetal. Sato:Contextualsemantictype include natural language processing and infor-\ndetectionintables. Proc.VLDBEndow.,13(11):1835–1848,2020. mationextraction.\n[279] H.Zhang,Y.Song,Y.Song,andD.Yu. Knowledge-awarePronoun\nCoreferenceResolution. InProc.ACL’19.ACL,2019.\n[280] H.Zhang,X.Zhao,andY.Song.ABriefSurveyandComparativeStudy\nofRecentDevelopmentofPronounCoreferenceResolution.arXiv,2020.\n[281] S. Zhang, L. Yao, A. Sun, and Y. Tay. Deep Learning Based\nRecommenderSystem. ACMComput.Surv.,52(1):1–38,2019. XiongshuaiZhangreceivedBachelor’sDegree\n[282] Y. Zhang, P. Pasupat, and P. Liang. Macro Grammars and Holistic from South China University of Techinology,\nTriggeringforEfficientSemanticParsing. InProc.EMNLP’17.ACL. China, in 2019. He is now a master student\n[283] Z.Zhang,Y.Gu,X.Han,andetal. CPM-2:Large-scaleCost-effective with the School of Software, Tsinghua Univer-\nPre-trainedLanguageModels. arXiv,2021. sity,China.Hisresearchinterestsincludedata\n[284] ZhenWen,M.Zhou,andV.Aggarwal. Anoptimization-basedapproach visualizationandcomputergraphics.\ntodynamicvisualcontextmanagement.InProc.InfoVis’05.IEEE,2005.\n[285] W. Zheng, H. Cheng, L. Zou, and et al. Natural Language Ques-\ntion/Answering. InProc.CIKM’17.ACM,2017.\n[286] X.Zheng,X.Qiao,Y.Cao,andR.W.H.Lau. Content-awaregenerative\nmodelingofgraphicdesignlayouts. ACMTrans.Graph.,2019.\n[287] V. Zhong and et al. Seq2SQL: Generating Structured Queries from Zhiwei Tai received his BS degree from the\nNaturalLanguageusingReinforcementLearning. arXiv,2017. SchoolofSoftware,TsinghuaUniversityin2020.\n[288] M.Zhou,Q.Li,X.He,andetal. Table2Charts:RecommendingCharts He is currently working toward the Master’s\nbyLearningSharedTableRepresentations. InProc.KDD’21.ACM. degree in Software Engineering at Tsinghua\n[289] S.Zhu,G.Sun,Q.Jiang,andetal. Asurveyonautomaticinfographics University. His research interests include data\nandvisualizationrecommendations. Vis.Informatics,4(3):24–40,2020. visualization,digitaltwin,andaugmentedreality.\n[290] F. O˝zcan, A. Quamar, J. Sen, and et al. State of the Art and Open\nChallengesinNaturalLanguageInterfacestoData.InProc.SIGMOD’20.\nACM,2020.\nLeixianShenreceivedhisbachelor’sdegreein Jianmin Wang received his bachelor’s degree\nSoftwareEngineeringfromtheNanjingUniversity fromPekingUniversity,China,in1990,andthe\nof Posts and Telecommunications in 2020. He MEandPhDdegreesincomputersoftwarefrom\nis currently a master student in the School of Tsinghua University, China, in 1992 and 1995,\nSoftware,TsinghuaUniversity,Beijing,China.His respectively.HeisafullprofessorwiththeSchool\nresearchinterestsincludedatavisualizationand ofSoftware,TsinghuaUniversity.Hisresearch\nhumancomputerinteraction. interestsincludebigdatamanagementandlarge-\nscale data analytics. He is leading to develop\nabigdatamanagementsystemintheNational\nEngineeringLabforBigDataSoftware.",
    "tables_extraction_text_csv": "0\n\"\"\n",
    "ocr_extraction_text": ""
  }
]