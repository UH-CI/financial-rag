[
  {
    "chunk_id": 0,
    "chunk_text": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n1\nTowards Natural Language Interfaces for Data\nVisualization: A Survey\nLeixian Shen, Enya Shen, Yuyu Luo, Xiaocong Yang, Xuming Hu,\nXiongshuai Zhang, Zhiwei Tai, and Jianmin Wang\nAbstract—Utilizing Visualization-oriented Natural Language Interfaces (V-NLI) as a complementary input modality to direct manipulation\nfor visual analytics can provide an engaging user experience. It enables users to focus on their tasks rather than having to worry about\nhow to operate visualization tools on the interface. In the past two decades, leveraging advanced natural language processing\ntechnologies, numerous V-NLI systems have been developed in academic research and commercial software, especially in recent years.\nIn this article, we conduct a comprehensive review of the existing V-NLIs. In order to classify each paper, we develop categorical\ndimensions based on a classic information visualization pipeline with the exten",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 1,
    "chunk_text": " develop categorical\ndimensions based on a classic information visualization pipeline with the extension of a V-NLI layer. The following seven stages are used:\nquery interpretation, data transformation, visual mapping, view transformation, human interaction, dialogue management, and\npresentation. Finally, we also shed light on several promising directions for future work in the V-NLI community.\nIndex Terms—Data Visualization, Natural Language Interfaces, Survey.\n!\n1\nINTRODUCTION\nT\nHE use of interactive visualization is becoming increasingly\npopular in data analytics [17]. As a common part of analytics\nsuites, Windows, Icons, Menus, and Pointer (WIMP) interfaces\nhave been widely employed to facilitate interactive visual analysis\nin current practices. However, this interaction paradigm presents a\nsteep learning curve in visualization tools since it requires users to\ntranslate their analysis intents into tool-speciﬁc operations [126],\nas shown in the upper part of Figure 1.\nOver the years",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 2,
    "chunk_text": "s intents into tool-speciﬁc operations [126],\nas shown in the upper part of Figure 1.\nOver the years, the rapid development of Natural Language\nProcessing (NLP) technology has provided great opportunities to\nexplore a natural-language-based interaction paradigm for data\nvisualization [18], [271]. With the help of advanced NLP toolkits\n[1], [3], [21], [81], [157], a surge of Visualization-oriented Natural\nLanguage Interfaces (V-NLI) emerged recently as a complementary\ninput modality to traditional WIMP interaction. V-NLIs accept the\nuser’s natural language queries (e.g., “visualize the distribution\nof budget as a histogram”) as input and output appropriate visu-\nalizations (e.g., correspondingly, a histogram with the production\nbudget, a data attribute of cars dataset). The emergence of V-NLI\ncan greatly enhance the usability of visualization tools in terms of:\n(a) Convenience and novice-friendliness. Natural language is a skill\nthat is mastered by the public. By using natural language ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 3,
    "chunk_text": "friendliness. Natural language is a skill\nthat is mastered by the public. By using natural language to interact\nwith computers, V-NLI closes the tool-speciﬁc manipulations to\nusers, as shown in Figure 1, facilitating the analysis ﬂow for\nnovices. (b) Intuitiveness and effectiveness. It is a consensus that\nvisual analysis is most effective when users can focus on their data\nrather than manipulations on the interface of analysis tools [83].\nWith the help of V-NLI, users can express their analytic tasks in\ntheir terms. (c) Humanistic care. A sizeable amount of information\nwe access nowadays is supported by visual means. V-NLI can be\nan innovative means for non-visual access, which promotes the\ninclusion of blind and low vision people.\n•\nAll authors are from Tsinghua University, Beijing, China. E-mail:\n{slx20,luoyy18,yangxc18,hxm19,zxs21,tzw20}@mails.tsinghua.edu.cn,\n{shenenya, jimwang}@tsinghua.edu.cn.\nManuscript received XX XX, 2021; revised XX XX, 2022.\nAnalysis\nIntents\nVisualization\nto",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 4,
    "chunk_text": "ghua.edu.cn.\nManuscript received XX XX, 2021; revised XX XX, 2022.\nAnalysis\nIntents\nVisualization\ntools\nTranslate \nanalysis intents into\ntool-specific operations \nVisualizations\nAnalysis\nIntents\nVisualization\ntools\nVisualizations\nTypical interactive visualization paradigm\nInteractive visualization with V-NLIs\nFig. 1. The traditional interaction paradigm requires users to translate\ntheir analysis intents into tool-speciﬁc operations [126]. With the help of\nV-NLI, users can express their analysis intents in their terms.\nHowever, designing and implementing V-NLIs is challenging\ndue to the ambiguous and underspeciﬁed nature of human language,\nthe complexity of maintaining context in a conversational ﬂow,\nand the lack of discoverability (communicating to users what\nthe system can do). In the past two decades, to address the\nchallenges, numerous systems have been developed in academic\nresearch and commercial software, especially in recent years. The\ntimeline of V-NLI is shown in Figure 2. Ba",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 5,
    "chunk_text": " and commercial software, especially in recent years. The\ntimeline of V-NLI is shown in Figure 2. Back in 2001, Cox et\nal. [40] presented an initial prototype of NLI for visualization,\nwhich can only accept well-structured queries. Articulate [239]\nintroduced a two-step process to create visualizations from NL\nqueries almost a decade later. It ﬁrst extracts the user’s analytic\ntask and data attributes and then automatically determines the\nappropriate visualizations based on the information. Although the\ninfancy-stage investigations were a promising start, as natural\nlanguage was not yet a prevalent interaction modality, the V-NLI\nsystems were restricted to simple prototypes. However, since Apple\nintegrated Siri [219] into the iPhone, NLIs began to attract more\nattention. Around 2013, the advent of word embeddings [163]\npromoted the advances of neural networks for NLP, rekindling\ncommercial interest in V-NLI. IBM ﬁrstly published their NL-\nbased cognitive service, Watson Analytics [4], ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 6,
    "chunk_text": "al interest in V-NLI. IBM ﬁrstly published their NL-\nbased cognitive service, Watson Analytics [4], in 2014. Microsoft\nPower BI’s Q&A [5] and Tableau’s Ask data [2] were announced\nin 2018 and 2019, respectively, offering various features like\nautocompletion and context management. DataTone [63] ﬁrst\narXiv:2109.03506v2  [cs.HC]  4 Feb 2022\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n2\nCox et al. \nArticulate\nApple Siri\nIBM \nWatson Analytics\nMicrosoft \nPower BI Q&A\nTableau \nAsk Data\nDataTone\nEviza\nOrko\nncNet, ADVISor,\nData@Hand\nQuda, NLV\nNL4DV\nSneak pique\nnvBench\nWord embeddings\nELMO, BERT\nInfancy Stage\nDevelopment Stage\nOutbreak Stage\n.   .   .   .   .   .   .   .\nFlowSense\n2001 (1)\n2010 (1)\n2011 (1)\n2021 (11)\n2013 (1)\n2014 (1)\n2015 (1)\n2016 (3)\n2018 (7)\n2019 (9)\n2020 (13)\n2012 (2)\n2017 (4)\n2002 (1)\n2005 (1)\n: Academic research \n  : Commercial software      \n        : NLP milestone \n: Dataset\nAttention\nSeq-to-Seq\nFig. 2. Timeline of V-NLI. We brieﬂy",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 7,
    "chunk_text": "e      \n        : NLP milestone \n: Dataset\nAttention\nSeq-to-Seq\nFig. 2. Timeline of V-NLI. We brieﬂy divide the timeline into the infancy, development, and outbreak stages. The number of yearly published papers is\nattached. The timeline consists of four parts: academic research, commercial software, NLP milestone, and dataset.\nTABLE 1\nRelevant venues\nField\nVenues\nVisualization\n(VIS)\nIEEE VIS (InfoVis, VAST, SciVis), EuroVis,\nPaciﬁcVis, TVCG, CGF, CGA\nHuman-Computer Interaction\n(HCI)\nCHI, UIST, IUI\nNatural Language Processing\n(NLP)\nACL, EMNLP, NAACL, COLING\nData Mining and Management\n(DMM)\nKDD, SIGMOD, ICDE, VLDB\nintroduced ambiguity widgets, which are user interface widgets\n(e.g., dropdown and slider) to help users modify system-generated\nresponses. As opposed to one-off commands, Eviza [205] enabled\nusers to have interactive conversations with their data. After\ntechnological accumulation, the past ﬁve years have seen the\noutbreak of V-NLI (see the number of yearly published papers in\n",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 8,
    "chunk_text": "n, the past ﬁve years have seen the\noutbreak of V-NLI (see the number of yearly published papers in\nFigure 2). With the development of hardware devices, synergistic\nmultimodal visualization interfaces gained notable interest. Orko\n[231] was the ﬁrst system to combine touch and speech input on\ntablet devices, and Data@Hand [115] focused on smartphones.\nSneak pique [206] explored how autocompletion can improve\nsystem discoverability while helping users formulate analytical\nquestions. The pretrained language models obtained new state-of-\nthe-art results on various NLP tasks from 2018, which provided\ngreat opportunities to improve the intelligence of V-NLI [49], [179].\nADVISor [143] and ncNet [150] were follow-up deep learning-\nbased solutions. Quda [61] and NLV [228] contributed datasets of\nNL queries for visual data analytics, and nvBench produced the\nﬁrst V-NLI benchmark [151]. Beyond data exploration, FlowSense\n[273] augmented a data ﬂow-based visualization system with V-\nNLI. The NL4D",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 9,
    "chunk_text": "a exploration, FlowSense\n[273] augmented a data ﬂow-based visualization system with V-\nNLI. The NL4DV [174] toolkit can be easily integrated into existing\nvisualization systems to provide V-NLI service.\nLiterature on V-NLI research is proliferating, covering aspects\nsuch as Visualization (VIS), Human-Computer Interaction (HCI),\nNatural Language Processing (NLP), and Data Mining and Man-\nagement (DMM). As a result, there is an increasing need to better\norganize the research landscape, categorize current work, identify\nknowledge gaps, and assist people who are new to this growing area\nto understand the challenges and subtleties in the community. For\nthis purpose, there have been several prior efforts to summarize the\nadvances in this area. For example, Srinivasan and Stasko (Short\npaper in EuroVis 2017 [230]) conducted a simple examination of\nﬁve existing V-NLI systems by comparing and contrasting them\nbased on the tasks they allow users to perform. They (Journal\npaper in CGA 2020 [232])",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 10,
    "chunk_text": "trasting them\nbased on the tasks they allow users to perform. They (Journal\npaper in CGA 2020 [232]) further highlighted critical challenges\nfor evaluating V-NLIs and discussed beneﬁts and considerations of\nthree task framing strategies. Although the two surveys can provide\nvaluable guidance for follow-up research, with the outbreak of\nV-NLI in recent years, considerable new works are to be covered\nand details to be discussed. To the best of our knowledge, this\npaper is the ﬁrst step towards comprehensively reviewing V-NLI\napproaches in a systematic manner.\nThis paper is structured as follows: First, we explain the scope\nand methodology of the survey in Section 2. What follows is the\nclassiﬁcation overview of existing works in Section 3. Then, the\ncomprehensive survey is presented in Sections 4 - 10, corresponding\nto seven stages extracted in the information visualization pipeline\n[28]. Finally, we shed light on several promising directions for\nfuture work in Section 11.\n2\nSURVEY LANDS",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 11,
    "chunk_text": "Finally, we shed light on several promising directions for\nfuture work in Section 11.\n2\nSURVEY LANDSCAPE\n2.1\nScope of the Survey\nIn order to narrow the scope of the survey within a controllable\nrange, we focused on visualization-oriented natural language\ninterfaces, which accept natural language queries as input and\noutput appropriate visualizations automatically. Users can input\nnatural language in various ways, including typing with a keyboard\n(e.g., Datatone [63] in Figure 7), speech input via a microphone\n(e.g., InChorus [226] in Figure 9), selecting text from articles (e.g.,\nMetoyer et al. [162] in Figure 13), and inputting existing textual\ndescriptions (e.g., Vis-Annotator [122] in Figure 12).\nIn addition, several ﬁelds are closely related to V-NLI. As\nshown in Table 2, the Related Stage column lists overlap stages\nwith V-NLI in Figure 3 that apply similar technologies. In order\nto make the survey more comprehensive, when introducing V-\nNLI in the following sections, we will invo",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 12,
    "chunk_text": " make the survey more comprehensive, when introducing V-\nNLI in the following sections, we will involve additional crucial\ndiscussions on the aforementioned related ﬁelds, with explanations\nof their importance and relationship with V-NLI. For example,\nVisualization Recommendation (VisRec) [187], [289] acts as the\nback-end engine of V-NLI to recommend visualizations. Natural\nLanguage Interface for Database (NLI4DB) [6] and Conversational\nInterface for Visualization (CIV) [20] share similar principles with\nV-NLI. Natural Language Generation for Visualization (NLG4Vis)\n[144], [177] and Visual Question Answering (VQA) [110], [158]\ncomplement visual data analysis with natural language as output.\nAnnotation [190], [213] and Narrative Storytelling [241] present\nfascinating visualizations by combining textual and visual elements.\n2.2\nSurvey Methodology\nTo comprehensively survey V-NLI, we performed an exhaustive\nreview of relevant journals and conferences throughout the past\ntwenty years (2000-",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 13,
    "chunk_text": "ed an exhaustive\nreview of relevant journals and conferences throughout the past\ntwenty years (2000-2021), which broadly covers VIS, HCI, NLP,\nand DMM. The relevant venues are shown in Table 1. We began\nour survey by searching keywords (“natural language” AND\n“visualization”) in Google Scholar, resulting in 1436 papers (VIS:\n455, HCI: 489, NLP: 289, and DMM: 203). We also searched for\nrepresentative related works that appeared earlier or in the cited\nreferences of related papers. During the review process, we ﬁrst\nexamined the titles of papers from these publications to identify\ncandidate papers. Next, abstracts of the candidate papers were\nbrowsed to determine whether they were related to V-NLI. The\nfull text was reviewed to make a ﬁnal decision if we could not\nobtain precise information from the title and abstract. Finally, we\ncollected 57 papers about V-NLI that accepts natural language\nas input and outputs visualizations. Details of the systems are\nlisted in Table 3, including the ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 14,
    "chunk_text": "ge\nas input and outputs visualizations. Details of the systems are\nlisted in Table 3, including the applied NLP technologies, chart\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n3\nTABLE 2\nRepresentative papers of related works to V-NLI. Related Stage column lists the overlap with V-NLI stages in Figure 3 that apply similar technologies.\nTopic\nRelated Stage\nSurvey\nRepresentative Paper\nVisualization Recommendation (VisRec)\nB/C/D/E/G\n[187], [289]\n[32], [37], [42], [46], [51], [70], [84], [86], [106], [109], [114], [130], [138],\n[142], [149], [152], [154], [168], [170], [185], [186], [203], [234], [246], [247],\n[253], [259], [260], [261], [263], [276], [277], [288]\nNatural Language Interface for DataBase (NLI4DB)\nA/B/E/F/G\n[6], [290]\n[13], [14], [15], [19], [22], [55], [72], [73], [74], [80], [87], [92], [95], [118],\n[135], [136], [137], [141], [173], [175], [188], [193], [202], [210], [220], [221],\n[252], [268], [270], [274], [285], [287]\nConversationa",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 15,
    "chunk_text": "5], [188], [193], [202], [210], [220], [221],\n[252], [268], [270], [274], [285], [287]\nConversational Interface for Visualization (CIV)\nA/B/C/D/E/F/G\n[20], [58], [77], [129], [134], [171], [172], [201], [216]\nNatural Language Generation for Visualization (NLG4Vis)\nB/E/G\n[39], [42], [44], [45], [59], [111], [144], [166], [167], [177], [184], [223]\nVisual Question Answering (VQA)\nA/B/E/F/G\n[158]\n[27], [30], [97], [98], [110], [143], [155], [222], [258], [269]\nAnnotation and Narrative Storytelling\nA/B/C/D/E/F/G\n[200], [241]\n[11], [26], [32], [33], [34], [35], [36], [41], [62], [64], [65], [82], [89], [101],\n[112], [114], [119], [122], [123], [146], [162], [165], [183], [190], [213], [240],\n[254], [256], [257], [265], [275]\nTABLE 3\nSummary of representative works in V-NLI. The ﬁrst ﬁve columns list basic information, and the left columns are characteristics of V-NLI with\ncategorical dimensions described in Section 3. Details of each column will be discussed in Sections 4 - 10.\n4\n5\n6\n7\n8\n9\n",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 16,
    "chunk_text": "ns described in Section 3. Details of each column will be discussed in Sections 4 - 10.\n4\n5\n6\n7\n8\n9\n10\nName\nPublication\nNLP Toolkit or Technology\nVisualization\nType\nRecommendation\nAlgorithm\nSemantic and Syntax Analysis\nTask Inference\nData Attributes Inference\nDefault for Underspeciﬁed Utterances\nTransformation for Data Insights\nSpatial Substrate Mapping\nGraphical Elements Mapping\nGraphical Properties Mapping\nView Transformation\nAmbiguity Widget\nAutocompletion\nMultimodal\nWIMP\nDialogue Management\nVisual Presentation\nAnnotation\nNarrative Storytelling\nNL Description Generation\nVisual Question Answering\nCox et al. [40]\nIJST’01\nSisl\nB/Ta\nInfoStill\n× ✓✓× ✓✓✓✓× × × × × ✓✓× × × ×\nKato et al. [105]\nCOLING’02 Logical form\nB/L/P/A\nTemplate\n✓✓✓× ✓✓✓✓× × × × × × ✓× × × ×\nRIA [284]\nInfoVis’05\n-\nI\nOptimization\n✓✓✓× ✓× × × × × × ✓× ✓✓× ✓× ×\nArticulate [239]\nLNCS’10\nStanford Parser\nS/R/B/L/P/Bo\nDecision tree\n✓✓✓× ✓✓✓✓× × × × × × ✓× × × ×\nContextiﬁer [89]\nCHI’13\nNLTK\nL\n-\n✓× ✓× ✓× × × × × × × × × ✓✓✓× ×\nN",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 17,
    "chunk_text": "ree\n✓✓✓× ✓✓✓✓× × × × × × ✓× × × ×\nContextiﬁer [89]\nCHI’13\nNLTK\nL\n-\n✓× ✓× ✓× × × × × × × × × ✓✓✓× ×\nNewsViews [64]\nCHI’14\nOpenCalais\nM\n-\n✓✓✓× ✓× × × × × × × × × ✓✓✓× ×\nDatatone [63]\nUIST’15\nNLTK/Stanford Parser\nS/B/L\nTemplate\n✓✓✓× ✓✓✓✓× ✓× × ✓× ✓× × × ×\nArticulate2 [120], [121]\nSIGDIAL’16 ClearNLP/Stanford Parser/OpenNLP B/L/H\nDecision tree\n✓✓✓× ✓✓✓✓✓× × ✓× ✓✓× × × ×\nEviza [205]\nUIST’16\nANTLR\nM/L/B/S\nTemplate\n✓✓✓× ✓✓✓✓× ✓✓× ✓✓✓× × × ×\nTimeLineCurator [62]\nTVCG’16\nTERNIP\nTl\nTemplate\n✓✓✓× × × × × × × × × × × ✓✓✓✓×\nAnalyza [50]\nIUI’17\nStanford Parser\nB/L\nTemplate\n✓✓✓× ✓✓✓✓× ✓✓× ✓✓✓× × × ×\nTSI [26]\nTVCG’17\nTemplate\nL/A/H\n-\n× ✓✓× ✓× × × × × × × × × ✓✓✓× ×\nAva [134]\nCIDR’17\nControlled Natural Language\nS/L/B\nTemplate\n✓✓✓× ✓✓✓✓× × × × × ✓✓× × × ✓\nDeepEye [148]\nSIGMOD’18 OpenNLP\nP/L/B/S\nTemplate\n✓× ✓× ✓✓✓✓× × × × ✓× ✓× × × ×\nEvizeon [83]\nTVCG’18\nCoreNLP\nM/L/B/S\nTemplate\n✓✓✓× ✓✓✓✓× ✓✓× ✓✓✓× × × ×\nIris [58]\nCHI’18\nDomain-speciﬁc language\nS/T\nTemplate\n✓✓✓× ✓✓✓✓× × × × × ✓✓× × × ✓\nMetoyer et al. [16",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 18,
    "chunk_text": "ris [58]\nCHI’18\nDomain-speciﬁc language\nS/T\nTemplate\n✓✓✓× ✓✓✓✓× × × × × ✓✓× × × ✓\nMetoyer et al. [162]\nIUI’18\nCoreNLP\nI\nTemplate\n✓× ✓× ✓× × × × × × × ✓× ✓× ✓× ✓\nOrko [231]\nTVCG’18\nCoreNLP/NLTK\nN\n-\n✓✓✓× ✓✓× ✓✓✓× ✓✓✓✓× × × ×\nShapeSearch [217], [218]\nVLDB’18\nStanford Parser\nL\nShapeQuery\n✓× ✓× ✓✓× ✓× ✓× × ✓× ✓× × × ×\nValletto [103]\nCHI’18\nSpacy\nB/S/M\nTemplate\n✓✓✓× ✓✓✓✓✓× × ✓✓✓✓× × × ×\nArkLang [208]\nIUI’19\nCocke-Kasami-Younger\nB/G/L/M/P/S/Tr\nTemplate\n✓✓✓✓✓✓✓✓× × × × ✓× ✓× × × ×\nAsk Data [2], [242]\nVAST’19\nProprietary\nB/G/L/M/P/S/T\nTemplate\n✓✓✓✓✓✓✓✓× ✓✓× ✓✓✓× × × ×\nData2Vis [51]\nCGA’19\nSeq2Seq\nB/A/L/S/T/St\nSeq2Seq\n× ✓× × ✓✓✓✓× × × × ✓× ✓× × × ×\nHearst et al. [78]\nVIS’19\nWordNet\n-\n-\n✓× ✓✓✓× × × × × × × × × × × × × ×\nVoder [225]\nTVCG’19\nStanford Parser\nSt/Bo/B/S/D\nTemplate\n✓✓✓× ✓✓✓✓× × × × ✓× ✓× × ✓×\nAUDiaL [169]\nLNCS’20\nCoreNLP\nB/L\nKnowledge Base\n✓✓✓× ✓✓✓✓× ✓× × ✓× ✓× × × ×\nBacci et al. [10]\nLNCS’20\nWit.ai\nP/L/B/S\nTemplate\n✓✓✓× ✓✓✓✓× × ✓× ✓× ✓× × × ×\nDataBreeze [227]\nTVCG’20\nCoreNLP/NLTK\nS/Uc",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 19,
    "chunk_text": "CS’20\nWit.ai\nP/L/B/S\nTemplate\n✓✓✓× ✓✓✓✓× × ✓× ✓× ✓× × × ×\nDataBreeze [227]\nTVCG’20\nCoreNLP/NLTK\nS/Uc\nTemplate\n✓✓✓× ✓✓✓✓✓✓× ✓✓× ✓× × × ×\nFlowSense [273]\nTVCG’20\nCoreNLP/SEMPRE\nL/S/B/M/N/Ta\nTemplate\n✓✓✓× ✓✓✓✓× × ✓× ✓× ✓× × × ×\nInChorus [226]\nCHI’20\nCoreNLP/NLTK\nL/S/B\nTemplate\n✓✓✓× ✓✓✓✓✓× × ✓✓× ✓× × × ×\nNL4DV Quda [61]\narXiv’20\nCoreNLP\nSt/B/L/A/P/S/Bo/H Template\n✓✓✓× ✓✓✓✓× ✓× ✓✓× ✓× × × ×\nSneak Pique [206]\nUIST’20\nANTLR\n-\n-\n✓✓✓× ✓× × × × × ✓× × × × × × × ×\nStory Analyzer [165]\nJCIS’20\nCoreNLP\nWc/C/T/Fg/B/M/I\nTemplate\n✓× ✓× × ✓✓✓× × × × × × ✓× ✓× ×\nText-to-Viz [41]\nTVCG’20\nStanford Parser\nI\nTemplate\n✓✓✓× ✓× × ✓× × × × × × ✓× ✓✓×\nVis-Annotator [122]\nCHI’20\nSpacy/Mask-RCNN\nB/L/P\n-\n✓× ✓× ✓× × × × × × × × × ✓✓× × ×\nSentiﬁers [207]\nVIS’20\nANTLR\n-\n-\n✓✓✓✓✓× × × × × × × ✓✓× × × × ×\nNL4DV [174]\nTVCG’21\nCoreNLP\nSt/B/L/A/P/S/Bo/H Template\n✓✓✓× ✓✓✓✓× ✓× ✓✓× ✓× × × ×\nData@Hand [115]\nCHI’21\nCompromise/Chrono\nB/L/S/Ra/Bo\nTemplate\n✓✓✓× ✓✓✓✓✓✓× ✓✓× ✓× × × ×\nRetrieve-Then-Adapt [183] TVCG’21\nTemplate\nI\nTemp",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 20,
    "chunk_text": "ono\nB/L/S/Ra/Bo\nTemplate\n✓✓✓× ✓✓✓✓✓✓× ✓✓× ✓× × × ×\nRetrieve-Then-Adapt [183] TVCG’21\nTemplate\nI\nTemplate\n✓× × × × × × × × × × × ✓× ✓✓✓✓×\nADVISor [143]\nPaciﬁcVis’21 End-to-end network\nB/L/S\nTemplate\n✓✓✓× ✓✓✓✓× × × × ✓× ✓✓× × ✓\nSeq2Vis [151]\nSIGMOD’21 Sep-to-Sep model\nB/L/S/P\nSeq2Seq\n× × ✓× ✓✓✓✓× × × × × × ✓× × × ×\nncNet [150]\nVIS’21\nTransformer-based model\nB/L/S/P\nTemplate\n× × ✓× ✓✓✓✓× × × × × × ✓× × × ×\nGeoSneakPique [204]\nVIS’21\nANTLR\n-\n-\n✓✓✓× ✓× × × × × ✓× × × × × × × ×\nSnowy [229]\nUIST’21\nCoreNLP\n-\n-\n✓✓✓× ✓× × × × × ✓× × ✓× × × × ×\nDT2VIS [93]\nCGA’21\nCoreNLP\nB/L/S/P/Bo/R\nTemplate\n✓✓✓× ✓✓✓✓× × ✓× ✓✓✓× × × ✓\n* Abbreviations for Visualization Type: Bar(B), Table(Ta), Infographic(I), Scatter(S), Radar(R), Line(L), Pie(P), Boxplot(Bo), Icon(Ic), Map(M), Heatmap(H), Timeline(Tl),\nArea(A), Network(N), Tree(Tr), Strip(St), Donut(D), Gantt(G), Word clouds(Wc), Force graph(Fg), Range(Ra), Unit column charts(Uc), and Graphics(Gr).\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL.",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 21,
    "chunk_text": "column charts(Uc), and Graphics(Gr).\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n4\nData\nTransformation\nVisual\nMapping\nView\nTransformation\nRaw data\nTransformed\nData\nVisual\nStructures\nViews\nHuman Interaction\nNL\nquery\nA\nData Space\nVisualization Space\nF\nD\nC\nB\nE\nG\nVisualization-oriented Natural Language Interface \nFig. 3. Extension of classic information visualization pipeline proposed by Card et al. [28] with V-NLI. It depicts how V-NLI works to construct\nvisualizations, which consists of the following seven stages: (A) Query Interpretation, (B) Data Transformation, (C) Visual Mapping, (D) View\nTransformation, (E) Human Interaction, (F) Dialogue Management, and (G) Presentation.\ntypes supported, visualization recommendation algorithm adopted,\nand various characteristics in V-NLI, which will be discussed in\nSections 4 - 10. On this basis, we proceeded to a comprehensive\nanalysis of the 57 papers to systematically understand the main\nresearch trends. We",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 22,
    "chunk_text": " a comprehensive\nanalysis of the 57 papers to systematically understand the main\nresearch trends. We also collected 283 papers of related works\ndescribed in Section 2.1. Table 2 lists representative papers of each\ntopic for reference, which will be selectively discussed with V-NLI\ncharacteristics in the subsequent sections. More information can be\nfound at https://v-nlis.github.io/V-NLIs-survey/.\n3\nCLASSIFICATION OVERVIEW\nThe information visualization pipeline presented by Card et al.\n[28] (see Figure 3) describes how the raw data transits into\nvisualizations and interacts with the user. We extend this pipeline\nwith an additional V-NLI layer (Green colored in Figure 3). On\nthis basis, we move forward to develop categorical dimensions\nby focusing on how V-NLI facilitates visualization generation.\nInspired by McNabb et al. [160], to facilitate the categorization\nprocess, the following stages in the pipeline are used:\n• Query Interpretation (A): Since we add the V-NLI layer in the\npipelin",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 23,
    "chunk_text": "es in the pipeline are used:\n• Query Interpretation (A): Since we add the V-NLI layer in the\npipeline, query interpretation is a foundational stage. Semantic\nand syntax analysis is generally performed ﬁrst to discover\nhierarchical structures of the NL queries so that the system can\nparse relevant information in terms of data attributes and analytic\ntasks. Due to the vague nature of natural language, dealing with\nthe underspeciﬁed utterances is another essential task in this\nstage. Details can be found in Section 4.\n• Data Transformation (B): In the original pipeline, this stage\nmainly plays a role in transforming raw data into data tables along\nwith various operations (e.g., aggregation and pivot). Since the\nmajority of raw data to analyze is already in a tabular format, we\nrename it transformed data. Data transformation is responsible for\ngenerating alternative data subsets or derivations for visualization.\nAll operations on the data plane belong to this stage. Details can\nbe found in",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 24,
    "chunk_text": "ns for visualization.\nAll operations on the data plane belong to this stage. Details can\nbe found in Section 5.\n• Visual Mapping (C): This stage focuses on mapping the\ninformation extracted from NL queries to visual structures. The\nthree elements of visual mapping for information visualization\nare spatial substrate, graphical elements, and graphical properties\n[28]. Spatial substrate is the space to create visualizations, and\nit is essential to consider the layout conﬁguration. Graphical\nelements are the visual elements appearing in the spatial substrate\n(e.g., points, lines, surfaces, and volumes). Graphical properties\ncan be implemented on the graphical elements to make them\nmore noticeable (e.g., size, orientation, color, textures, and\nshapes). Details can be found in Section 6.\n• View Transformation (D): View transformation (rendering)\ntransforms visual structures into views by specifying graphical\nproperties that turn these structures into pixels. Common forms\ninclude navigation, ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 25,
    "chunk_text": "fying graphical\nproperties that turn these structures into pixels. Common forms\ninclude navigation, animation, and visual distortion (e.g., ﬁsheye\nlens). However, in the context of our survey, this stage is rarely\ninvolved in V-NLI. Details can be found in Section 7.\n• Human Interaction (E): Human interactions with the visualiza-\ntion interface feed back into the pipeline. A user can connect\nwith a visualization manually by modifying or transforming a\nview state, or by reviewing the use, effectiveness, and knowledge\non the visualization. Dimara et al. [52] deﬁned interaction for\nvisualization as: “The interplay between a person and a data\ninterface involving a data-related intent, at least one action from\nthe person and an interface reaction that is perceived as such.”\nDescribing interaction requires all the mandatory components:\ninterplay, user, and interface. Details can be found in Section 8.\n• Dialogue Management (F): Interpreting an utterance contextu-\nally is essential for visual",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 26,
    "chunk_text": "ction 8.\n• Dialogue Management (F): Interpreting an utterance contextu-\nally is essential for visualization system intelligence, which is\nparticularly evident in V-NLI. This stage involves each step in\nthe pipeline and concentrates on facilitating a conversation with\nthe system based on the current visualization state and previous\nutterances. Details can be found in Section 9.\n• Presentation (G): We name this stage as Presentation. The\nclassic pipeline focuses on how to generate visualizations but\nignores presenting them to the user. With natural language\nintegrated into the pipeline, the vast majority of V-NLI systems\naccept natural language as input and directly display generated\nvisualizations. Furthermore, complementing visualizations with\nnatural language can provide additional surprises to the user.\nDetails can be found in Section 10.\nThe pipeline is meant to model work activities. A single\nutterance can cover multiple stages (e.g., data transformation, visual\nmapping, and human ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 27,
    "chunk_text": " A single\nutterance can cover multiple stages (e.g., data transformation, visual\nmapping, and human interaction), and users can iterate over any of\nthese stages. An utterance can also be interpreted contextually. The\nfollowing seven sections will discuss the seven stages accordingly.\n4\nQUERY INTERPRETATION\nQuery interpretation is the foundation of all subsequent stages. This\nsection will discuss how to perform semantic and syntax analysis\nof the input natural language queries, infer analytic tasks of the\nuser and data attributes to be analyzed, and make defaults for\nunderspeciﬁed utterances.\n4.1\nSemantic and Syntax Analysis\nSemantic and syntax analysis can be powerful to discover hier-\narchical structures and understand meanings in human language.\nThe semantic parser can conduct a series of NLP sub-tasks on\nthe query string to extract valuable details that can be used to\ndetect relevant phrases. The processing steps include tokenization,\nidentifying parts-of-speech (POS) tags, recogniz",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 28,
    "chunk_text": "phrases. The processing steps include tokenization,\nidentifying parts-of-speech (POS) tags, recognizing name entities,\nremoving stop words, performing stemming, creating a dependency\ntree, generating N-grams, etc. For example, Flowsense [273] is a\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n5\nTABLE 4\nComparison of commonly used NLP Toolkits\nToolkit\nCoreNLP [157]\nNLTK [21]\nOpenNLP [1]\nSpaCy [81]\nStanza [182]\nFlair [7]\nGoogleNLP [3]\nProgramming language\nJava\nPython\nJava\nPython\nPython\nPython\nPython\nTokenization\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nSentence segmentation\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nPart-of-speech tagging\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nParsing\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nLemmatization\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nNamed entities recognition\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nCoreference resolution\n✓\n✓\n✓\n-\n✓\n-\n-\nEntity linking\n✓\n✓\n-\n✓\n✓\n-\n✓\nChunker\n-\n-\n✓\n✓\n-\n-\n-\nSentiment\n✓\n-\n-\n-\n✓\n-\n✓\nText classiﬁcation\n-\n✓\n-\n✓\n-\n✓\n✓\nTrain custom model\n-\n-\n✓\n✓\n✓\n✓\n✓\nFig. 4. Semantics parsing in FlowSense [273]. The derivation of the query\nis shown as a par",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 29,
    "chunk_text": "\n✓\n✓\n✓\n✓\nFig. 4. Semantics parsing in FlowSense [273]. The derivation of the query\nis shown as a parse tree.\nnatural language interface designed for a dataﬂow visualization\nsystem [272]. It applies a semantic parser with special utterances\n(table column names, node labels, node types, and dataset names)\ntagging and placeholders. Figure 4 displays a parse tree for the\nderivation of the user’s query. The ﬁve major components of a\nquery pattern and its related parts are highlighted by a unique\ncolor, and the expanded sub-diagram is illustrated at the bottom.\nFlowsense is powered by the Stanford SEMPRE framework [282]\nand CoreNLP toolkit [157]. Advanced NLP toolkits [1], [3], [7],\n[21], [81], [157], [182] allow developers to quickly integrate NLP\nservices into their systems. As semantic and syntax analysis is a\nfundamental task for V-NLI, almost all systems support semantic\nparsing by directly using existing NLP toolkits, such as CoreNLP\n[157], NLTK [21], OpenNLP [1], SpaCy [81], Stanza [1",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 30,
    "chunk_text": "ly using existing NLP toolkits, such as CoreNLP\n[157], NLTK [21], OpenNLP [1], SpaCy [81], Stanza [182], Flair\n[7], and GoogleNLP [3], as listed in Table 3 (column NLP Toolkit\nor Technology). We also sort out commonly used characteristics of\nexisting NLP toolkits in Table 4 for reference. In addition, recently,\nthere have been some systems that do not rely on these tools but\nleverage Language Models (LMs) to directly “interpret” queries\n[143], [150]. They ﬁrst generate a rich representation of the input\nby translating them into high-dimensional vectors and then adopt\nneural networks to enable smart visualization inference.\nDiscussion: Most V-NLI systems rely on existing NLP toolkits\nfor semantic and syntax analysis. So far, these tools are a good\nchoice for query parsing and can be easily integrated into the\nsystem. However, they are trained on NLP datasets, thus lacking\nadequate consideration of visualization elements (e.g., mark, visual\nchannel, and encoding property). A promising so",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 31,
    "chunk_text": "ration of visualization elements (e.g., mark, visual\nchannel, and encoding property). A promising solution may be to\ndevelop a new NLP toolkit speciﬁcally for visualization.\n4.2\nTask Inference\n4.2.1\nTask modeling\nA growing body of literature recognizes that the user’s analytic\ntask is vital for automatic visualization creation [24], [108], [113],\n[191], [194]. As for task modeling, there have been many prior\nefforts to provide deﬁnitions of the analytic tasks. For instance,\nAmar et al. [9] proposed ten low-level analytic tasks that capture\nthe user’s activities while employing visualization tools for data\nexploration. The ten tasks are later extensively applied in numerous\nvisualization systems [93], [113], [142], [168], [194], [212], [225],\n[245], [254], as listed in Table 5. Saket et al. [194] evaluated\nthe effectiveness of ﬁve commonly used chart types across the\nten tasks [9] by a crowdsourced experiment. Furthermore, they\nderived chart type recommendations for different tasks. Kim",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 32,
    "chunk_text": "owdsourced experiment. Furthermore, they\nderived chart type recommendations for different tasks. Kim et\nal. [113] measured subject performance across task types derived\nfrom the ten tasks [9] and added a compare values task. The\nanalytic tasks are further grouped into two categories: value tasks\nthat just retrieve or compare individual values and summary tasks\nthat require identiﬁcation or comparison of aggregate properties.\nNL4DV [174] includes a Trend task in addition. AutoBrief [107]\nenhances visualization systems by introducing domain-level tasks,\nwhile Matthew et al. [24] contributed a multi-level typology of\nvisualization tasks. Deep into scatter charts, Sarikaya et al. [196]\ncollected model tasks from a variety of sources in data visualization\nliterature to formulate the seed for a scatterplot-speciﬁc analytic\ntask list. Recently, Shen et al. [211] summarized 18 classical\nanalytic tasks by a survey covering both academia and industry.\n4.2.2\nIntent inference\nAlthough considerable",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 33,
    "chunk_text": " tasks by a survey covering both academia and industry.\n4.2.2\nIntent inference\nAlthough considerable previous works focus on task modeling, few\nvisualization systems have attempted to infer the user’s analytic\ntask before the emergence of natural language interfaces. Gotz\nand Wen [70] monitored the user’s click interactions for implicit\nsignals of user intent. Steichen et al. [233] and Gingerich et al.\n[68] used the eye-gazing patterns of users while interacting with a\ngiven visualization to predict the user’s analytic task. Battle et al.\n[16] investigated the relationship between latency, task complexity,\nand user performance. However, these behavior-based systems\nare limited to few pre-deﬁned tasks; generalization for automatic\nvisualization systems does not exist yet.\nRather than inferring the analytic task through the user’s behav-\nior, systems supporting NL interaction depend on understanding\nthe NL utterances to analyze the user’s intent since they may\nhint at the user’s analysis",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 34,
    "chunk_text": "erstanding\nthe NL utterances to analyze the user’s intent since they may\nhint at the user’s analysis goals. Most systems infer the analytic\ntasks by comparing the query tokens to a predeﬁned list of task\nkeywords [63], [83], [174], [205], [239], [273]. For example,\nNL4DV [174] identiﬁes ﬁve low-level analytic tasks: Correlation,\nDistribution, Derived Value, Trend, and Filter. A task keyword list\nis integrated internally (e.g., Correlation task includes ‘correlate,’\n‘relationship,’ etc., Distribution task includes ‘range,’ ‘spread,’ etc.).\nNL4DV also leverages POS tags and query parsing results to model\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n6\nTABLE 5\nTen low-level analytic tasks proposed by Amar et al. [9]. They have been widely applied for automatic visualization creation.\nTask\nDescription\nRepresentative papers\nCharacterize Distribution Given a set of data cases and a quantitative attribute of interest, characterize\nthe distribution of that ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 35,
    "chunk_text": "a set of data cases and a quantitative attribute of interest, characterize\nthe distribution of that attribute’s values.\n[9], [61], [143], [174], [194], [196], [225], [254], [273]\nCluster\nGiven a dataset, ﬁnd clusters of similar attribute values.\n[9], [61], [194], [212], [225], [273]\nCompute Derived Value\nGiven a dataset, compute an aggregate numeric representation of the data.\n[9], [61], [86], [143], [194], [225], [254], [273]\nCorrelate\nGiven a dataset and two attributes, determine useful relationships between the\nvalues of those attributes.\n[9], [61], [86], [143], [174], [194], [196], [212], [225],\n[254], [273]\nDetermine Range\nGiven a dataset and a data attribute, ﬁnd the span of values within the set.\n[9], [61], [194], [225], [273]\nFilter\nFind data cases satisfying the given concrete conditions on attribute values.\n[9], [61], [174], [194], [225], [254], [273]\nFind Anomalies\nIdentify any anomalies within a given set of data cases with respect to a given\nrelationship or expectation, e.",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 36,
    "chunk_text": "y anomalies within a given set of data cases with respect to a given\nrelationship or expectation, e.g. statistical outliers.\n[9], [61], [85], [194], [196], [212], [225], [254], [273]\nFind Extremum\nFind data cases possessing an extreme value of an attribute over its range.\n[9], [61], [85], [113], [143], [194], [225], [254], [273]\nRetrieve Value\nGiven a set of speciﬁc cases, ﬁnd attributes of those cases.\n[9], [61], [85], [113], [174], [194], [225], [254], [273]\nSort\nGiven a dataset, rank them according to some ordinal metric.\n[9], [61], [194], [225], [254], [273]\nthe relationship between query phrases and populate task details.\nFor conversational analysis systems, Nicky [116] interprets task\ninformation in conversations based on a domain-speciﬁc ontology.\nEvizeon [83] and Orko [231] support follow-up queries through\nconversational centering [71], which is a model commonly used\nin linguistic conversational structure but only for the ﬁlter task.\nADE [38] supports the analytic process via ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 37,
    "chunk_text": "ic conversational structure but only for the ﬁlter task.\nADE [38] supports the analytic process via task recommendation\ninvoked by inferences about user interactions in mixed-initiative\nenvironments. The recommendation is also accompanied by a\nnatural language explanation. Instead of simply matching keywords,\nFu et al. [61] maintained a dataset of NL queries for visual data\nanalytics and trained a multi-label task classiﬁcation model based\non BERT [49], an advanced pre-trained NLP model.\nDiscussion: Although most V-NLI systems support task in-\nference, as shown in Table 3 (column Task Inference), the tasks\nintegrated in the system are limited (e.g., a subset of ten low-\nlevel tasks [9]). More tasks with hierarchical modeling can be\nconsidered to better cover the user’s intent, as well as tasks for\nanalyzing speciﬁc data types (e.g., tree [235], map [54], and graph\n[127]). Besides, rule-based approaches account for the majority;\nvarious advanced learning models provide a great opportuni",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 38,
    "chunk_text": "ased approaches account for the majority;\nvarious advanced learning models provide a great opportunity to\ninfer analytic tasks in an unbiased and rigorous manner.\n4.3\nData Attributes Inference\nA dataset contains numerous data attributes. However, the user\nmay only be interested in several certain data attributes in a query.\nThe systems should be able to extract data attributes that are\nmentioned both explicitly (e.g., directly refer to attribute names)\nand implicitly (e.g., refer to an attribute’s values or alias). Illustrated\nby an example, NL4DV [174] maintains an alias map and allows\ndevelopers to conﬁgure aliases (e.g., GDP for Gross domestic\nproduct and investment for production budget). It iterates through\nthe generated N-grams discussed in Section 4.1, checking for both\nsyntactic and semantic similarity between N-grams and a lexicon\ncomposed of data attributes, aliases, and values. For syntactic\nsimilarity, NL4DV adopts cosine similarity function; for semantic\nsimilarity, it com",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 39,
    "chunk_text": ". For syntactic\nsimilarity, NL4DV adopts cosine similarity function; for semantic\nsimilarity, it computes the Wu-Palmer similarity score based on\nWordNet [164]. If the similarity score reaches the threshold, the\ncorresponding data attributes will be extracted and further presented\non the visualization. Most V-NLI systems have taken a similar\napproach, and they just differ in the integrated rules. Besides,\nAnalyza [50] utilizes additional heuristics to derive information\nfrom data attributes and expands the lexicon with a proprietary\nknowledge graph. Recently, Liu et al. [143] proposed a deep\nlearning-based pipeline, ADVISor. It uses BERT [49] to generate\nembeddings of both NL queries and table headers, which are\nfurther used by a deep neural networks model to decide data\nattributes, ﬁlter conditions, and aggregation type. Similarly, Luo\net al. [150] presented an end-to-end solution using a transformer-\nbased [248] sequence-to-sequence (seq2seq) model [12]. In the\nﬂow of visual analytic",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 40,
    "chunk_text": " a transformer-\nbased [248] sequence-to-sequence (seq2seq) model [12]. In the\nﬂow of visual analytical conversations, data attributes can also be\nextracted through co-reference resolution (Section 9.2).\nDiscussion: During the survey and evaluation of state-of-the-\nart systems, we found that most V-NLIs are primarily geared to\nsupport speciﬁcation-oriented queries like examples in their manual\n(e.g., “Show me acceleration and horsepower across origins”),\nfrom which it is easy to extract data attributes. However, when it\ncomes to underspeciﬁed or vague queries (e.g., include synonym,\nabbreviation, and terminology in different ﬁelds), these systems\noften fail to generate appropriate visualizations. So improving the\nrobustness of V-NLIs should be one of the focuses in future work.\n4.4\nDefault for Underspeciﬁed Utterances\nNumerous systems support the ﬁlter task [63], [83], [174], [205],\n[273], aiming to select data attributes and ranges. An input query\nwill be underspeciﬁed if it lacks enou",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 41,
    "chunk_text": ", aiming to select data attributes and ranges. An input query\nwill be underspeciﬁed if it lacks enough information for the\nsystem to infer data attributes and perform ﬁltering. Presenting\ndefaults in the interface by inferring the underspeciﬁed utterances\ncan be an effective option to address this issue. However, the\narea has received relatively little attention, as shown in Table\n3 (column Default for Underspeciﬁed Utterances). Within the\nresearch, vague modiﬁers like “tall” and “cheap” are a prevalent\nkind of underspeciﬁcation in human language. Hearst et al. [78]\nmade the ﬁrst step toward design guidelines for dealing with vague\nmodiﬁers based on existing cognitive linguistics research and a\ncrowdsourcing study. Tableau’s Ask Data [2] internally leverages\nlightweight and intermediate language, Arklang [208], to infer\nunderspeciﬁed details. It emphasizes how the linguistic context of\nprevious utterances affects the interpretation of a new utterance.\nSentiﬁers [207] can determine whic",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 42,
    "chunk_text": "revious utterances affects the interpretation of a new utterance.\nSentiﬁers [207] can determine which data attributes and ﬁlter\nranges to associate the vague predicates using word co-occurrence\nand sentiment polarities. As shown in Figure 5, when analyzing\nthe earthquakes dataset, the user inputs “where is it unsafe” and\nSentiﬁers automatically associates “unsafe” with the magnitude\nattribute. A top N ﬁlter of magnitude six and higher is applied, and\nsimilar negative sentiment polarities are marked red on the map.\nDiscussion: Although the aforementioned approaches are\nuseful, some more complex interpretations are still not supported in\ncurrent V-NLIs, such as combinations of vague modiﬁers. So when\nencountering ambiguity, apart from formulating a sensible default,\nhuman interaction (e.g., ambiguity widgets) is another effective\nmethod (see Section 8.1). In addition, users’ explicit judgments\nfor the same utterance can differ signiﬁcantly. A more adaptative\nsolution may be to personaliz",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 43,
    "chunk_text": "ents\nfor the same utterance can differ signiﬁcantly. A more adaptative\nsolution may be to personalize the inferencing logic by identifying\nindividuals’ unique analytic tasks.\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n7\nFig. 5. Vague modiﬁer interpretation in Sentiﬁers [63]. The system\nassociates the vague modiﬁer “unsafe” with magnitude attribute.\n5\nDATA TRANSFORMATION\nAfter extracting data attributes, various data transformation oper-\nations can be made to transform raw data into focused data. This\nsection will introduce how V-NLI systems transform raw data for\ndata insights, with additional discussions on a closely related topic,\nNLI for databases.\n5.1\nTransformation for Data Insights\nA consensus is that the purpose of visualization is insights, not\npictures. A “boring” dataset may become “interesting” after data\ntransformations (e.g., aggregation, binning, and grouping). There-\nfore, identifying the data transformation information is another\n",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 44,
    "chunk_text": "on, binning, and grouping). There-\nfore, identifying the data transformation information is another\nimportant characteristic in V-NLI. To describe related works, we\ncategorize tabular data into four types: temporal, quantitative,\nnominal, and ordinal, which are widely adopted in the visualization\ncommunity [168]. For temporal data, systems can support binning\nbased on temporal units [62], [148], [165], [169], [174], [205],\n[273]. For instance, Setlur et al. [205] developed a temporal\nanalytics module based on temporal entities and expressions\ndeﬁned in TimeML [91]. To parse temporal expressions, this\nmodule incorporates the following temporal token types: Temporal\nUnits (e.g., years, months, days, hours and minutes, and their\nabbreviations.), Temporal Prepositions (e.g., ’in,’ ’during,’ ’near,’\nand ’around.’), and Temporal Connectives (e.g., ‘before’ and\n‘after’). For quantitative data, various aggregation functions (e.g.,\ncount, average, and sum) can be performed, binning and grouping",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 45,
    "chunk_text": "various aggregation functions (e.g.,\ncount, average, and sum) can be performed, binning and grouping\noperations are also commonly used [10], [38], [41], [50], [63],\n[83], [93], [121], [148], [159], [178], [205], [208], [225], [273],\n[284]. For example, DeepEye [148] includes various aggregation\nfunctions in the visualization language. Arklang [208], which is an\nintermediate language to resolve NL utterances to formal queries,\nintegrates a ﬁnite set of aggregation operators. Eviza [205] enables\naggregation using regular spatial bins. Unlike continuous data,\nnominal and ordinal data are usually used as grouping metrics. As\nan end-to-end system, ncNet [150] supports more complex data\ntransformation types (e.g., relational join, GroupBY, and OrderBY)\nby applying Neural Machine Translation (NMT) models. After\nextracting relevant information, the commonly used visualization\nspeciﬁcation languages (e.g., Vega [198], Vega-Lite [197], and\nVizQL [234]) all provide various data transformation pri",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 46,
    "chunk_text": "ges (e.g., Vega [198], Vega-Lite [197], and\nVizQL [234]) all provide various data transformation primitives\nto realize. Besides, to our knowledge, most V-NLIs are designed\nto deal with tabular data, but there are also several systems that\nfocus on speciﬁc data types (e.g., network [231] and map [204]).\nHowever, they mostly only display the raw data without data\ntransformations.\nDiscussion: The back-end engine can directly perform calcula-\ntions and visualize the results if the data transformation information\ncan be extracted from the user’s NL queries. Otherwise, despite\nadvances in technologies for data management and analysis [17],\n[67], it remains time-consuming to inspect a dataset and construct a\nvisualization that allows meaningful analysis to begin. So a practical\nsystem should be in a position to automatically recommend data\ninsights for users. Fortunately, various systems have been designed\nto generate data facts in the context of visual data exploration, such\nas DataSite [42]",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 47,
    "chunk_text": "een designed\nto generate data facts in the context of visual data exploration, such\nas DataSite [42], Voder [225], Wrangler [99], Quickinsights [53],\nForesight [46], and SpotLight [75]. They can serve as back-end\nengines for insight-driven recommendations.\n5.2\nNLI for Database\nNatural Language Interface for Database (NLI4DB), or Text-to-\nSQL, is a task to automatically translate a user’s query text in\nnatural language into an executable query for databases like SQL\n[6]. NLI4DB is strongly related to data transformation in V-NLI,\nand V-NLI can augment NLI4DB with effective visualizations of\nquery results. Besides, not all the queries about a dataset need a\nvisualization as a response. For instance, one might ask, “What\nwas the GDP of the USA last year?” or “Which country won\nthe most gold medals in the Tokyo Olympics?”. In such cases,\nNLI4DB can directly compute and present values in response to\nthe user’s questions instead of displaying through visualizations\nthat require the user to i",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 48,
    "chunk_text": "onse to\nthe user’s questions instead of displaying through visualizations\nthat require the user to interpret for answers. Generally, there are\nthree types of methods for NLI4DB: symbolic approach, statistical\napproach, and connectionist (neural network) approach [175]. The\nsymbolic approach utilizes explicit linguistic rules and knowledge\nto process the user’s query, which dominated the area in the early\nyears. DISCERN [221] is a representative example and integrates a\nmodel that extracts information from scripts, lexicon, and memory.\nThe statistical approach exploits statistical learning methods such\nas hidden Markov models [141] and probabilistic context-free\ngrammars [87]. From the view of the connectionist approach,\nthe Text-to-SQL task is a kind of special Machine Translation\n(MT) problem. However, it is hard to be directly handled with\nstandard end-to-end neural machine translation models due to the\nabsence of detailed speciﬁcation in the user’s query [73]. Another\nproblem is the",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 49,
    "chunk_text": " models due to the\nabsence of detailed speciﬁcation in the user’s query [73]. Another\nproblem is the out-of-domain items mentioned in the user’s query\ndue to the user’s unawareness of the ontology set. Aiming at the\ntwo problems, IRNet [73] proposes a tree-shaped intermediate\nrepresentation synthesized from the user’s query. It is later fed into\nan inference model to generate SQL statements with a domain\nknowledge base. Furthermore, to capture the special format of SQL\nstatements, SQLnet [268] adopts a universal sketch as a template\nand predicts values for each slot. Wang et al. [252] employed a\ntwo-stage pipeline to subsequently predict the semantic structure\nand generate SQL statements with structure-enhanced query text.\nRecently, TaPas [80] extends BERT’s architecture and trains from\nweak supervision to answer questions over tables. For more details,\nplease refer to the survey [6], [290] and related papers in Table 2.\nDiscussion: V-NLI can augment NLI4DB with well-designed\nvisualiza",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 50,
    "chunk_text": "90] and related papers in Table 2.\nDiscussion: V-NLI can augment NLI4DB with well-designed\nvisualizations, and NLI4DB can complement V-NLI with exact\nvalue answers. First, the two ﬁelds can draw on each other as\nthey share similar principles. Second, we can combine the two\ntechnologies to generate visualizations that “contain” answers.\nThird, there are many benchmarks in the NLI4DB community like\nWikiSQL [287] and Spider [274], which can be utilized to further\ndesign V-NLI benchmarks.\n6\nVISUAL MAPPING\nThis section will discuss how V-NLIs perform visual mapping in:\nspatial substrate, graphical elements, and graphical properties.\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n8\n6.1\nSpatial Substrate\nSpatial substrate is the space to create visualizations. It is important\nto determine the layout conﬁgurations to apply in the spatial\nsubstrate, such as which axes to use. Some V-NLI systems support\nexplicitly specifying layout information like inputting “",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 51,
    "chunk_text": "ch axes to use. Some V-NLI systems support\nexplicitly specifying layout information like inputting “show GDP\nseries at y axis and year at x axis grouped by Country Code”. If the\nmapping information is not clariﬁed in the query, the search space\nwill be very huge, in which some combinations of data attributes\nand visual encodings may not generate a valid visualization.\nFor instance, the encoding type “Y-axis” is inappropriate for\ncategorical attributes. Fortunately, there are many design rules\neither from traditional wisdom or users to help prune meaningless\nvisualizations. These design rules are typically given by experts.\nVoyager [259], [260], [261], DIVE [86], Show me [154], Polaris\n[234], Proﬁler [100], DeepEye [149], and Draco [168] all contribute\nto the enrichment of design rules with more data types. Besides,\nmany systems [168], [247], [261] also allow users to specify their\ninterested data attributes and assign them on certain axes, which\nis a more direct way to perform visual m",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 52,
    "chunk_text": "sted data attributes and assign them on certain axes, which\nis a more direct way to perform visual mapping. In Falx [250],\nusers can specify visualizations with examples of visual mapping,\nand the system automatically infers the visualization speciﬁcation\nand transforms data to match the design. With the development of\nmachine learning in recent years, advanced models can be applied\nfor more effective visual mapping. VizML [84] identiﬁes ﬁve key\ndesign choices while creating visualizations, including mark type,\nX or Y column encoding, and whether X or Y is the single column\nrepresented along that axis or not. For a new dataset, 841 dataset-\nlevel features extracted are fed into neural network models to\npredict the design choices. Luo et al. [150] proposed an end-to-end\napproach that applies the transformer-based seq2seq [12] model to\ndirectly map NL queries and data to chart templates. In addition to\nsingle chart, considering multiple visualizations, Evizeon [83] uses\na grid-based layo",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 53,
    "chunk_text": "n addition to\nsingle chart, considering multiple visualizations, Evizeon [83] uses\na grid-based layout algorithm to position new charts, while Voder\n[225] allows the user to choose a slide or dashboard layout.\nDiscussion: When performing visual mappings in the spatial\nsubstrate, the vast majority of systems are limited to 2-dimension\nspace (along x and y axes). However, considering more axes\ntypes, creating 3-dimensional (e.g., add z axes) and even hyper-\ndimensional (e.g., parallel coordinate) representations is possible.\n6.2\nGraphical Elements\nGraphical element (e.g., points, lines, surfaces, and volumes),\nwhich is usually named mark or chart type, is an important\npart of visualizations. Choosing an appropriate mark can convey\ninformation more efﬁciently. Similar to deciding the layout, some\nV-NLI systems allow users to specify marks like inputting “create\na scatterplot of mpg and cylinders”, which is a simple way to map\nmark information of visualizations. However, in most cases, the",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 54,
    "chunk_text": "nders”, which is a simple way to map\nmark information of visualizations. However, in most cases, the\nmark information is not accessible. So after parsing the NL queries,\nmost systems integrate predeﬁned rules or leverage Visualization\nRecommendation (VisRec) technologies to deal with the extracted\nelements. Voyager [260], [261] applies visualization design best\npractices drawn from prior research [153], [154], [192], [203]\nand recommends mark types based on the data types of the x\nand y channels. Srinivasan et al. [225] developed the heuristics\nand mappings between data attribute combinations, analytic tasks,\nand chart types in Table 6 through iterative informal feedback\nfrom fellow researchers and students. TaskVis [211] integrates 18\nclassical low-level analysis tasks with their appropriate chart types\nby a survey both in academia and industry. Instead of considering\ncommon visualization types, Wang et al. [253] developed rules\nTABLE 6\nMappings between data attribute combinations (N:",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 55,
    "chunk_text": "on types, Wang et al. [253] developed rules\nTABLE 6\nMappings between data attribute combinations (N: Numeric, C:\nCategorical), analytic tasks, and chart types in Voder [225].\nAttributes\nTask(s)\nVisualization\nN\nFind Extremum\nStrip plot\nCharacterize Distribution\nStrip plot\nBox plot\nHistogram\nFind Anomalies\nStrip plot\nBox plot\nC\nFind Extremum\nBar chart\nDomut chart\nCharacterize Distribution\nNxN\nCorelation\nScatterplot\nCharacterize Distribution\nCxN\nCharacterize Distribution(+Derived Value) Bar chart\nDomut chart\nFind Extremum(+Derived Value)\nFind Extremum\nStrip plot\nScatterplot\nCxC\nFind Extremum\nStacked bar chart\nScatterplot+Size\nCharacterize Distribution(+Find Extremum)\nNxNxN\nCharacterize Distribution\nScatterplot+Size\nCxNxN\nCorrelation\nScatterplot\nCorrelation(+Filter)\nScatterplot+Color\nCharacterize Distibution(+Filter)\nScatterplot+Color\nScatterplot+Size\nCxCxN\nFind Extremum\nStrip plot+Color\nScatterplot+Color\nFind Extremum(+Derived Value)\nStrip plot+Color\nScatterplot+Color\nScatterplot+Size\nfor",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 56,
    "chunk_text": "tterplot+Color\nFind Extremum(+Derived Value)\nStrip plot+Color\nScatterplot+Color\nScatterplot+Size\nfor automatic selection of line graphs or scatter plots to visualize\ntrends in time series, while Sarikaya et al. [196] deepened into\nscatter charts. To better beneﬁt from design guidance provided by\nempirical studies, Moritz et al. [168] proposed a formal framework\nthat models visualization design knowledge as a collection of\nanswer set programming constraints.\nDiscussion: V-NLIs mostly maintain an alias map for data\nattribute inference (see Section 4.3) but often ignore mark inference.\nTherefore, though they can explicitly interpret mark information\nfrom the user’s NL queries, they may fail when the information is\nimplicitly expressed (e.g., “point” instead of “scatterplot”). Besides,\nas shown in Table 3 (column Visualization Type), existing works\nfocus more on traditional chart types, more chart types, and even\ninteractive visualizations can be extended in future work.\n6.3\nGraphical Prop",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 57,
    "chunk_text": " chart types, and even\ninteractive visualizations can be extended in future work.\n6.3\nGraphical Properties\nJacques Bertin ﬁrst identiﬁed seven “retinal” graphical properties\n(visual encoding properties) in visualizations: position, size, value,\ntexture, color, orientation, and shape. Some other types are later\nexpanded in the community, such as “gestalt” properties (e.g.,\nconnectivity and grouping) and animation [153], [243]. For visual\nmapping in V-NLI, color, size, and shape are most commonly\napplied to graphical elements, making them more noticeable. The\nrule-based approach is also dominant here, and human perceptual\neffectiveness metrics are usually considered. For example, the\ncardinality of variables in the color/size/shape channel should not\nbe too high (e.g., 100). Otherwise, the chart would be too messy\nto distinguish. The aforementioned works also developed various\ndesign rules for graphical properties, especially for color, size,\nand shape channels in legend [86], [100], [14",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 58,
    "chunk_text": " for graphical properties, especially for color, size,\nand shape channels in legend [86], [100], [149], [154], [168],\n[211], [234], [260], [261]. Besides, Text-to-Viz [41] integrates a\ncolor module that aims to generate a set of color palettes for a\nspeciﬁc infographic. Similarly, InfoColorizer [275] recommends\ncolor palettes for infographics in an interactive and dynamic\nmanner. Wu et al. [264] proposed a learning-based method to\nautomate layout parameter conﬁgurations, including orientation,\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n9\nFig. 6. User interface of Orko [231] to explore a network of European\nsoccer players.\nbar bandwidth, max label length, etc. Liu et al. [146] explored\ndata-driven mark orientation for trend estimation in scatterplots.\nCAST [65] and Data Animator [240] enable interactive creation of\nchart animations.\nDiscussion: For graphical properties, color, size, and shape\naccount for the majority in current V-NLIs. Although so",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 59,
    "chunk_text": "graphical properties, color, size, and shape\naccount for the majority in current V-NLIs. Although some other\nforms have been investigated deeply, they are rarely used in V-NLI\n(e.g., orientation and texture). Besides, color is a tricky choice to\nuse. On one hand, it is unfriendly to those who are color-blind. On\nthe other hand, it may involve cultural issues in certain scenarios\n(e.g., red is a “festive” color in China, whereas it may mean\n“warning” in many Western societies). So color needs to be used\nwith the discretion of color vision deﬁciencies and cultural meaning.\n7\nVIEW TRANSFORMATION\nAfter visual mapping, the generated visualization speciﬁcations\ncan be rendered through a library (e.g., D3 [23]). View transfor-\nmations are also supported here. The three commonly used view\ntransformation types are location probes which reveal additional\ninformation through locations, viewpoint controls that scale or\ntranslate a view, and distortions that modify the visual structure\n[28]. Surpri",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 60,
    "chunk_text": "ntrols that scale or\ntranslate a view, and distortions that modify the visual structure\n[28]. Surprisingly, this stage is rarely used in the context of our\nsurvey. To our knowledge, there have been few works that focus on\nnatural language interaction for view transformation. It is usually\nrealized through multimodal interaction, as shown in Table 3\n(column View Transformation). For example, Orko [231] facilitates\nboth natural language and direct manipulation input to explore\nnetwork visualization. Figure 6 is the user interface of Orko, and\nit shows an example that explores a network of European soccer\nplayers. When the user says “Show connection between Ronaldo\nand Bale”, the system will zoom in to display the details of nodes.\nIt also allows users to perform view transformation (e.g., zooming\nand panning) through a pen or ﬁnger. Similarly, Valletto [103],\nInChorus [226], Data@Hand [115], DataBreeze [227], Power BI’s\nQ&A [5], and Tableau’s Ask data [2] all support standard view\ntransf",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 61,
    "chunk_text": "], DataBreeze [227], Power BI’s\nQ&A [5], and Tableau’s Ask data [2] all support standard view\ntransformations. Besides, the software visualization community\nalso presents several closely related works. Bieliauskas et al. [20]\nproposed an interaction approach with software visualizations\nbased on a conversational interface. It can automatically display\nbest ﬁtting views according to meta information from natural\nlanguage sentences. Seipel et al. [201] explored natural language\ninteraction for software visualization in Augmented Reality (AR)\nwith Microsoft HoloLens device, which can provide various view\ntransformation interactions through gesture, gaze, and speech.\nDiscussion: View transformation has attracted less attention in\nthe community. Only a few V-NLI systems support NL control over\nview transformations, and they are mainly limited to viewpoint\nFig. 7. Ambiguity widgets in Datatone [63]. The user can correct the\nimprecise system decisions caused by ambiguity.\nnavigation. Future i",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 62,
    "chunk_text": " [63]. The user can correct the\nimprecise system decisions caused by ambiguity.\nnavigation. Future improvements can focus on other aspects (e.g.,\nanimation [240], data-GIF [215], and visual distortions [176]).\n8\nHUMAN INTERACTION\nThis section will discuss how V-NLI allows users to provide\nfeedback to visualization with human interactions. The primary\npurpose is to help users better express their intents.\n8.1\nAmbiguity Widgets\nDue to the vague nature of natural language, the system may\nfail to recognize the user intent and extract data attributes [161].\nConsiderable research has been devoted to addressing the ambiguity\nand underspeciﬁcation of NL queries. Mainstream approaches can\nbe divided into two categories. One is to generate appropriate\ndefaults by inferencing underspeciﬁed natural language utterances,\ndiscussed in Section 4.4. The other is to return the decision right\nto users through ambiguity widgets. Ambiguity widget is a kind\nof interactive widget that allows users to input t",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 63,
    "chunk_text": "ugh ambiguity widgets. Ambiguity widget is a kind\nof interactive widget that allows users to input through a mouse.\nDatatone [63] ﬁrst integrates ambiguity widgets and presents a\nmixed-initiative approach to manage ambiguity in the user query.\nAs shown in Figure 7, for the query in (b), DataTone detects data\nambiguities about medal, hockey, and skating. Three corresponding\nambiguity widgets are presented for users to correct the system\nchoices in (c). Additional design decision widgets like color and\naggregation are also available in (e). Eviza [205], Evizeon [83],\nNL4DV [174], and AUDiaL [169] all borrow the idea of DataTone\nand expand the ambiguity widgets to richer forms (see Table\n3 (column Ambiguity Widgets)), such as maps and calendars.\nDIY [173] enables users to interactively assess the response from\nNLI4DB systems for correctness.\nDiscussion: Resolving underspeciﬁed queries through ambi-\nguity widgets mainly consists of two steps: detecting ambiguity\nand presenting widgets to t",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 64,
    "chunk_text": "gh ambi-\nguity widgets mainly consists of two steps: detecting ambiguity\nand presenting widgets to the user. For the ﬁrst step, most systems\nnow still leverage heuristics for algorithmic resolution of ambiguity.\nA general probabilistic framework may handle a broader range\nof ambiguities. For the second, richer interactive widgets can be\nadopted to enhance the user experience.\n8.2\nAutocompletion and Command Suggestion\nUsers may be unaware of what operations the system can perform\nand whether a speciﬁc language structure is preferred in the\nsystem. Although advanced text understanding models give users\nthe freedom to express their intents, system discoverability to\nhelp formulate analytical questions is still an indispensable part\nof V-NLI. Discoverability entails awareness: making users aware\nof what operations the system can perform, and understanding:\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n10\nFig. 8. Autocompletion in Sneak pique [206], The ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 65,
    "chunk_text": " AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n10\nFig. 8. Autocompletion in Sneak pique [206], The user is prompted with\nwidgets that provide appropriate previews of the underlying data.\neducating users about how to phrase queries that can be interpreted\ncorrectly by the system [224]. Generally, current V-NLI systems\npay relatively little attention to system discoverability. Major\nattempts include autocompletion and command suggestion. This\ncharacteristic offers interpretable hints or suggestions matching the\nvisualizations and datasets, which is considered a fruitful interaction\nparadigm for information sense-making. When using V-NLI of\nTableau [2] or Power BI [5], the autocompleted content will be\npresented as we are typing, especially when there is a parsing\nerror. They are mostly reminders of commonly used queries, such\nas data attributes and aggregation functions. Suggestions will not\nintrusively appear when the user has formulated a valid query.\nSimilarly, Bacci et al. [10] and ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 66,
    "chunk_text": "not\nintrusively appear when the user has formulated a valid query.\nSimilarly, Bacci et al. [10] and Yu et al. [273] both adopted\ntemplate-based approaches to support autocompletion in their V-\nNLIs. In addition to text prompts, data previews can be more useful\nacross all autocompletion variants. Deeper into this area, three\ncrowdsourcing studies conducted by Setlur et al. [206] indicated\nthat users prefer widgets for previewing numerical, geospatial,\nand temporal data while textual autocompletion for hierarchical\nand categorical data. On this basis, they built a design probe,\nSneak Pique. As shown in Figure 8, when analyzing a dataset of\ncoronavirus cases, the user is prompted with widgets that provide\nappropriate previews of the underlying data in various types. The\nsystem also supports toggling from a widget to a corresponding\ntext autocompletion dropdown. Most recently, Srinivasan et al.\n[229] proposed Snowy, a prototype system that generates and\nrecommends utterance recommendations",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 67,
    "chunk_text": "al.\n[229] proposed Snowy, a prototype system that generates and\nrecommends utterance recommendations for conversational visual\nanalysis by suggesting data features while implicitly making users\naware of which input the NLI supports.\nDiscussion: Current utterance realization approaches are all\ntemplate-based, which just works effectively for a small set of\ntasks and can hardly apply to large-scale systems. Going forward,\nanother important consideration that needs further research is\nwhich commands to show and when and how the suggestions will\nbe presented. In addition, existing technologies can only handle\nkeyboard input. There remains an open area for the discoverability\nof speech-based V-NLI. The tone of voice can also provide insights\ninto the user’s sentiments [147].\n8.3\nMultimodal Interaction\nVan Dam [244] envisioned post-WIMP user interfaces as “one con-\ntaining at least one interaction technique not dependent on classical\n2D widgets such as menus and icons.” With the advancements",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 68,
    "chunk_text": "tion technique not dependent on classical\n2D widgets such as menus and icons.” With the advancements in\nhardware and software technologies that can be used to support\nnovel interaction modalities, researchers are empowered to take a\nstep closer to post-WIMP user interfaces, enabling users to focus\nmore on their tasks [128]. A qualitative user study conducted by\nSaktheeswaran et al. [195] also found that participants strongly\nprefer multimodal input over unimodal input. In recent years,\nFig. 9. Multimodal interaction interface in InChorus [226], which allows\npen, touch, and speech input on tablet devices.\nmany works have examined how multiple input forms (e.g., mouse,\npen, touch, keyboard, and speech) can be combined to provide a\nmore natural and engaging user experience. For instance, Orko\n[231] is a prototype visualization system that combines both\nnatural language interface and direct manipulation to assist visual\nexploration and analysis of graph data. Valletto [103] allows users\nto",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 69,
    "chunk_text": "manipulation to assist visual\nexploration and analysis of graph data. Valletto [103] allows users\nto specify visualizations through a speech-based conversational\ninterface, multitouch gestures, and a conventional GUI interface.\nInChorus [226] is designed to maintain interaction consistency\nacross different visualizations, and it supports pen, touch, and\nspeech input on tablet devices. Its interface components are shown\nin Figure 9, including typical WIMP components (A, B, C), speech\ncommand display area (D), and content supporting pen and touch\n(E, F, G). DataBreeze [227] couplings pen, touch, and speech-\nbased multimodal interactions with ﬂexible unit visualizations,\nenabling a novel data exploration experience. RIA [284] is an\nintelligent multimodal conversation system to aid users in exploring\nlarge and complex datasets, which is powered by an optimization-\nbased approach to visual context management. Data@Hand [115]\nleverages the synergy of speech and touch input modalities for\nper",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 70,
    "chunk_text": "ntext management. Data@Hand [115]\nleverages the synergy of speech and touch input modalities for\npersonal data exploration on the mobile phone. Srinivasan et\nal. [224] proposed to leverage multimodal input to enhance\ndiscoverability and the human-computer interaction experience.\nDiscussion: V-NLI is an essential part in the context of\npost-WIMP interaction with visualization systems. Typing and\nspeech are both commonly used modalities to input NL queries.\nHowever, speech has unique challenges from a system design\naspect, such as triggering speech input, lack of assistive features like\nautocompletion, and transcription errors [128], which deserve more\nattention in the future. Besides, limited research has incorporated\nhuman gesture recognition and tracking technology [94] to facilitate\nvisualization creation, especially with large displays.\n9\nDIALOGUE MANAGEMENT\nThis section will discuss dialogue management in V-NLI. Given the\nconversational nature of NLIs, users may frequently pose que",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 71,
    "chunk_text": "dialogue management in V-NLI. Given the\nconversational nature of NLIs, users may frequently pose queries\ndepending on their prior queries. By iterating upon their questions,\nthe user can dive deep into their aspects of interest on a chart and\nreﬁne existing visualizations.\n9.1\nAnalytical Conversations\nConversional NLIs have been widely applied in many ﬁelds to\nprompt users to open-ended requests (e.g., recommender systems\n[102] and intelligent assistants [219]). With conversational agents,\nusers can also reinforce their understanding of how the system\nparses their queries. In the visualization community, Eviza [205]\nand Evizeon [83] are two representative visualization systems that\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n11\nFig. 10. Conversational V-NLI in Evizeon [83]. The user has a back-and-\nforth information exchange with the system.\nprovide V-NLI for visual analysis cycles. Figure 10 shows a back-\nand-forth information exchange process be",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 72,
    "chunk_text": " V-NLI for visual analysis cycles. Figure 10 shows a back-\nand-forth information exchange process between Evizeon [83] and\nthe user when analyzing the measles dataset. The system supports\nvarious forms of NL interactions with a dashboard applying\npragmatic principles. For example, when the user ﬁrst types\n“measles in the UK”, all the charts presented are ﬁltered to cases\nof measles in the UK (Figure 10(a)). Then, the user types “show\nme the orange spike”, and Evizeon will add details to the spike in\nthe orange line as it understands that this is a reference to visual\nproperties of the line chart (Figure 10(b)). Similarly, Articulate2\n[121] is intended to automatically transform the user queries into\nvisualizations using a full-ﬂedged conversational interface. Analyza\n[50] combines two-way conversation with a structured interface to\nenable effective data exploration. Besides, Fast et al. [58] proposed\na new conversational agent, Iris, which can generate visualizations\nfrom and combine t",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 73,
    "chunk_text": "[58] proposed\na new conversational agent, Iris, which can generate visualizations\nfrom and combine the previous commands, even non-visualization\nrelated. Ava [134] uses controlled NL queries to program data\nscience workﬂow.\nConversational transitions model, which describes how to\ntransition visualization states during the analytical conversation,\nis an essential part to enable the aforementioned interactive\nconversations. In the early years, Grosz et al. [71] explored how the\ncontext of a conversation adjusts over time to maintain coherence\nthrough transitional states (retain, shift, continue, and reset). On this\nbasis, Tory and Setlur [242] introduced a conversational transitions\nmodel (see Figure 11) that emerged from their analysis in the\ndesign of Tableau’s V-NLI feature, Ask data. After interpreting a\ngenerated visualization, a user may formulate a new NL query to\ncontinue the analytical conversation. The user’s transitional goal\nmeans how the user wishes to transform the existing",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 74,
    "chunk_text": "tical conversation. The user’s transitional goal\nmeans how the user wishes to transform the existing visualization\nto answer a new question. The model contains the following\ntransitional goals: elaborate, adjust/pivot, start new, retry, and undo,\nwhich in turn drive user actions. The visualization states (select\nattributes, transform, ﬁlter, and encode) are ﬁnally updated, and\nnew visualizations are presented to the user. An essential insight\nduring the analysis of Tory and Setlur was that applying transitions\nto ﬁlters alone (like Evizeon [83] and Orko [231]) is insufﬁcient.\nThe user’s intent around transitions may apply to any aspects of a\nvisualization state.\nDiscussion: Some V-NLI systems leverage a VisRec engine to\ngenerate visualizations. However, it is insufﬁcient for analytical\nconversation. A more intelligent system should infer the user’s\ntransitional goals based on their interactions and then respond to\neach visualization state accordingly. Besides, empirical studies [77]\nFi",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 75,
    "chunk_text": "actions and then respond to\neach visualization state accordingly. Besides, empirical studies [77]\nFig. 11. Conversational transitions model [242] that describes how to\ntransition a visualization state during an analytical conversation.\nTABLE 7\nExamples of co-reference types\nType\nExample\nResolution\nPronoun-\nNoun\nShow me the most popular movie in New York and\nits rating.\nits →movie’s\nNoun-\nNoun\nHow many Wal-Mart stores in Seattle and how’s\nthe annual proﬁts for the malls?\nmalls →Wal-Mart\nPronoun-\nPronoun\nThe manchurian tigers have been heavily hunted,\nwhich caused a dramatic drop of their existing\nnumber and they may ﬁnally get extinct.\nthey, their →\nmanchurian tigers\nPronoun-\nPronoun\nWe are all the Canadians while Lily was born in the\nU.S. and she immigrated to Canada two years ago.\nWe →Canadians,\nshe →Lily\nhave shown that users tend to prefer additional content beyond\nthe exact answers when asking questions in such conversational\ninterfaces, which had signiﬁcant implications for design",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 76,
    "chunk_text": "hen asking questions in such conversational\ninterfaces, which had signiﬁcant implications for designing insight-\ndriven conversation interfaces in the future.\n9.2\nCo-reference Resolution\nCo-reference Resolution (CR) aims to ﬁnd linguistic expressions\n(called mentions) in a given text that refers to the same entity\n[236]. CR is an important sub-task in dialogue management as\nthe user usually uses pronouns to refer to certain visual elements.\nA typical scenario is to identify the entity to which a pronoun in\nthe text refers. The task also involves recognizing co-reference\nrelations between multiple noun phrases [181]. Table 7 shows\nexamples of co-reference types. Before the booming development\nof deep learning-based language models, human-designed rules,\nknowledge, and features dominated CR tasks [280]. Nowadays,\nmost state-of-the-art CR models are neural networks and employ an\nend-to-end fashion, where the pipeline includes encoding context,\ngenerating representations for all potential ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 77,
    "chunk_text": "fashion, where the pipeline includes encoding context,\ngenerating representations for all potential mentions, and predicting\nco-reference relations [131] [96]. Beneﬁting from the powerful\nunderstanding and predicting ability of language models like BERT\n[49] and GPT-3 [25], reliance on annotated span mentions is\nbecoming weaker [117]. A CR task can even be treated as a token\nprediction task [118]. Several existing models exist to deal with\nthis situation. Besides, the usage of structured knowledge bases\n[279] [145] and higher-order information [132] has been proven to\nbe beneﬁcial to the overall performance of CR tasks.\nCo-reference resolution is also an essential task in the multi-\nmodal sense as users may even pose queries that are the follow-\nup to their direct manipulations on the interface. When the user\nfaces a graphical representation, each element in the representation\ncan become a referent. The system should have a declarative\nrepresentation for all these potential referents a",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 78,
    "chunk_text": " a referent. The system should have a declarative\nrepresentation for all these potential referents and ﬁnd the best\nmatch to make multimodal interaction more smooth. Articulate2\n[120] addresses this issue by leveraging Kinect to detect deictic\ngestures on the virtual touch screen in front of a large display.\nIf referring expressions are detected, and Kinect has detected a\ngesture, information about any objects pointed to by the user will be\nstored. The system then can ﬁnd the best match between properties\nof each relevant entity. The properties of visualizations and objects\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n12\nFig. 12. Visualizations with annotations generated by Vis-Annotator [122].\nkeeping track of include statistics and trends in the data, title, mark,\nand any more prominent objects within the visualization (e.g., hot-\nspots, street names, and bus stops). In multimodal systems [103],\n[115], [195], [224], [226], [227], [231], users can",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 79,
    "chunk_text": "es, and bus stops). In multimodal systems [103],\n[115], [195], [224], [226], [227], [231], users can focus on speciﬁc\nvisual elements in the visualization by selecting with mouse, pen,\nor ﬁnger. The follow-up queries can automatically link to related\nvisual elements or data.\nDiscussion: At the natural language level, existing V-NLIs\nmostly leverage NLP toolkits to perform co-reference resolution.\nAlthough useful, they lack detailed modeling of visualization\nelements, as discussed in Section 4.1. At the multimodal level,\nin addition to commonly used modalities (e.g., mouse, pen, and\nﬁnger), it may enable a better user experience by integrating more\nmodalities like eye-gaze and gesture.\n10\nPRESENTATION\nIn most cases, V-NLI accepts natural language as input and\noutputs well-designed visualizations. As shown in Table 3 (column\nVisualization Type), the output is not limited to traditional charts\nbut also involves other richer forms (e.g., maps, networks, and\ninfographics). In addition to vi",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 80,
    "chunk_text": "rts\nbut also involves other richer forms (e.g., maps, networks, and\ninfographics). In addition to visual presentation, an emerging theme\nis to complement visualizations with natural language. The previous\nsections have discussed natural language as an input modality. This\nsection will introduce using natural language as an output modality.\n10.1\nAnnotation\nAnnotation plays a vital role in explaining and emphasizing\nkey points in the dataset. The systems should generate valuable\nnatural language statements and map the text to a visualization\nappropriately. Annotation tools were ﬁrst applied to complement\nnews articles. Kandogan [101] introduced the concept of just-\nin-time descriptive analytics that helps users easily understand\nthe structure of data. Given a piece of news, Contextiﬁer [89]\nﬁrst computes clusters, outliers, and trends in line charts and\nthen automatically produces annotated stock visualizations based\non the information. Although it offers a promising example of\nhow human",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 81,
    "chunk_text": "d stock visualizations based\non the information. Although it offers a promising example of\nhow human-produced news visualizations can be complemented\nin a speciﬁc context, it is only suitable for stock sequence data.\nNewsViews [64] later extends Contextiﬁer to support more types\nof appropriate data such as time series and georeferenced data.\nIn addition, several works incorporate the visual elements in the\nchart into annotations, synchronizing with textual descriptions. As\nshown in Figure 12, Vis-Annotator [122] leverages the Mask R-\nCNN model to identify visual elements in the target visualization,\nalong with their visual properties. Textual descriptions of the chart\nare synchronously interpreted to generate visual search requests.\nBased on the identiﬁed information, each descriptive sentence is\ndisplayed beside the described focal areas as annotations. Similarly,\nClick2Annotate [33] and Touch2Annotate [34] are both semi-\nautomatic annotations generators. To be interactive, Calliope [",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 82,
    "chunk_text": "d Touch2Annotate [34] are both semi-\nautomatic annotations generators. To be interactive, Calliope [213]\nand ChartAccent [190] support creating stories via interactive\nFig. 13. Coupling from the text about NBA game report to stories [162].\nannotation generation and placement. Most recently, ADVISor\n[143] can generate visualizations with annotations to answer the\nuser’s NL questions on tabular data.\nDiscussion: Annotation can turn data into a story. However,\nthe scalability of current works is weak as most of them are\ntemplate-based. To improve the system usability, advanced NLG\nmodels [29] can be leveraged to assist generating appropriate\ntextual annotations. Extending existing systems to support richer\ndata types and annotation forms is also a meaningful direction.\nBesides, it would be helpful to devise an annotation speciﬁcation\nlanguage for additional ﬂexibility.\n10.2\nNarrative Storytelling\nNarrative storytelling gives data a voice and helps communicate\ninsights more effectively. A ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 83,
    "chunk_text": "elling\nNarrative storytelling gives data a voice and helps communicate\ninsights more effectively. A data story means a narrative to the\ndata that depicts actors and their interactions, along with times,\nlocations, and other entities [200]. Constructing various visual and\ngenerated textual (natural language) elements in visualizations is\nvital for narrative storytelling. To address this issue, the template\nis the most commonly applied method. For instance, Text-to-Viz\n[41] is designed to generate infographics from NL statements with\nproportion-related statistics. It builds 20 layout blueprints that\ndescribe the overall look of the resulting infographics. For each\nlayout blueprint, Text-to-Viz enumerates all extracted segments by\ntext analyzer and then generates all valid infographics. DataShot\n[254], TSIs [26], Chen et al. [36], and Retrieve-Then-Adapt [183]\nall use similar template-based approaches for narrative storytelling.\nWith the advances of deep learning, some works leverage gene",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 84,
    "chunk_text": " approaches for narrative storytelling.\nWith the advances of deep learning, some works leverage generative\nadversarial networks (GAN) [69] to synthesize layouts [139],\n[286]. Those learning-based methods do not rely on handcrafted\nfeatures. However, due to limited training data, they often fail\nto achieve comparable performance with rule-based methods.\nLikewise, storytelling has been extensively used to visualize\nnarrative text produced by online writers and journalism media.\nMetoyer et al. [162] proposed a novel approach to generate data-\nrich stories. It ﬁrst extracts narrative components (who, what, when,\nwhere) from the text and then generates narrative visualizations by\nintegrating the supporting data evidence with the text. As shown\nin Figure 13, given a highlighted NBA game report sentence,\nthe system generates an initialized story dashboard based on\nthe interpretation information. Similarly, Story Analyzer [165]\nextracts subjects, actions, and objects from the narrative text to",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 85,
    "chunk_text": ". Similarly, Story Analyzer [165]\nextracts subjects, actions, and objects from the narrative text to\nproduce interrelated and user-responsive story dashboards. RIA\n[284] dynamically derives a set of layout constraints (e.g., ensure\nvisual balance and avoid object occlusion) to tell spatial stories.\nDiscussion: Although the aforementioned systems are promis-\ning and work relatively well, they are restricted to one speciﬁc\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n13\nFig. 14. The automatic VQA pipeline [110] answers the three questions\ncorrectly (marked in green) and gives correct explanations.\ntype of information respectively. More types of information (e.g.,\ntimeline and map) can be combined to create more expressive\nstories. Besides, similar to annotation, template-based methods also\ndominate narrative storytelling. It will be interesting to explore\nlearning-based approaches to further improve the story quality as\nwell as formulating datasets s",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 86,
    "chunk_text": "ore\nlearning-based approaches to further improve the story quality as\nwell as formulating datasets specially for this topic.\n10.3\nNatural Language Generation for Visualization\nIn the past years, there has been a body of research on automatically\ngenerating descriptions or captions for charts. The majority of\nearly works are rule-based [39], [44], [57], [166], [167]. While\nmodern pipelines generally include a chart parsing module and a\nsubsequent caption generation module. Chart parsing module [8],\n[180], [199] deconstructs the original charts and extracts valuable\nelements such as text, axis, and lines using relevant techniques\nlike text localization, optical character recognition, and boundary\ntracing. Deshpande et al. [48] proposed a novel method for chart\nparsing which adopts a question answering approach to query\nkey elements in charts. The organized elements are subsequently\nfed into the caption generation module [144], [177] to output\ncaptions. A common shortcoming of the models ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 87,
    "chunk_text": "o the caption generation module [144], [177] to output\ncaptions. A common shortcoming of the models above is the\ndemand for manually designed procedures or features. For example,\nAl-Zaidy et al. [8] relied on pre-deﬁned templates to generate\nsentences, and Chart-to-Text [177] needs additional annotated\ntabular data to describe elements in a given chart. Beneﬁting from\nthe development of deep learning, several end-to-end models have\nbeen proposed recently [177], [184], [223]. FigJAM [184] employs\nResNet-50 [76] to encode a chart as a whole image and uses OCR to\nencode text to generate slot values. They are along with the image\nfeature vectors as initialization of a LSTM network to generate\ncaptions. Spreaﬁco et al. [223] exploited the encoder-decoder\nLSTM architecture to take time-series data as input and generate\ncorresponding captions. Obeid et al. [177] applied a transformer-\nbased model [248] for generating chart summaries. Most recently,\nKim et al. [111] explored how readers gather",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 88,
    "chunk_text": "el [248] for generating chart summaries. Most recently,\nKim et al. [111] explored how readers gather takeaways when\nconsidering charts and captions together. Results suggest that the\ntakeaways differ when the caption mentions visual features of\ndiffering prominence levels, which provides valuable guidance for\nfuture research.\nDiscussion: So far, most related works in NLG4Vis can only\napply to simple charts (e.g., bar charts, scatterplots, and line charts).\nFurther research can pay attention to improving the chart type\ncoverage, as well as the language quality and descriptive ability.\nBesides, a more fundamental direction is to develop large datasets\ncovering diverse domains and chart types.\n10.4\nVisual Question Answering\nVisual Question Answering (VQA) is a semantic understanding\ntask that aims to answer questions based on a given visualization\nand possibly along with the informative text. In this subsection, we\nonly focus on images with charts or infographics rather than images\nwith n",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 89,
    "chunk_text": "t. In this subsection, we\nonly focus on images with charts or infographics rather than images\nwith natural objects and scenes in the computer vision community.\nGenerally, visualizations and questions are respectively encoded\nand then fused to generate answers [158]. For visualizations, Kim\net al. [110] leveraged a semantic parsing model, Sempre [282], to\ndevelop an automatic chart question answering pipeline with visual\nexplanations describing how the answer was produced. Figure\n14 shows that the pipeline answers all three questions correctly\n(marked in green) and gives correct explanations of how it obtains\nthe answer. While for infographics, the OCR module additionally\nneeds to identify and tokenize the text, which serves as facts for\nanswer generation [30], [97], [98], [158], [222]. Later models\nemploy more sophisticated fusion modules, among which the\nattention mechanism has reached great success. LEAF-Net [30]\ncomprises chart parsing, question, and answer encoding according\nto cha",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 90,
    "chunk_text": "great success. LEAF-Net [30]\ncomprises chart parsing, question, and answer encoding according\nto chart elements followed by an attention network is proposed.\nSTL-CQA [222] applies a structural transformer-based learning\napproach that emphasizes exploiting the structural properties of\ncharts. Recently, with the development of multimodal transformer\n[140], [237], Mathew et al. [158] proposed a pipeline where the\nquestion and infographic are mapped into the same vector space and\nsimply added as the input to stacks of transformer layers. Besides,\nthey also delivered a summary of VQA datasets for reference.\nDiscussion: Some systems extend existing QA engines (e.g.,\nSempre) to answer questions about charts; as a result, the interpre-\ntation ability is limited as they are not designed for visualization.\nA more proper way is to develop speciﬁc models based on\nvisualization datasets. However, the range of questions that these\nlearning-based approaches can handle may be restricted to the\ndiversi",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 91,
    "chunk_text": " range of questions that these\nlearning-based approaches can handle may be restricted to the\ndiversity of datasets. So it may be promising to transfer existing\nmodels to other areas and iteratively extend the dataset.\n11\nRESEARCH OPPORTUNITY\nBy conducting a comprehensive survey under the guidance of an\ninformation visualization pipeline proposed by Card et al. [28],\nwe found that, as a rapidly growing ﬁeld, V-NLI faces many\nthorny challenges as well as open research opportunities. In this\nsection, we organize these from a macro-perspective into ﬁve\naspects: knowledge, model, interaction, presentation, and dataset,\nwith additional discussions on applications. Our target is to cover\nthe critical gaps and emerging topics that deserve more attention.\n11.1\nDomain Knowledge\nA major limitation of most existing V-NLI systems is the absence\nof domain knowledge. We have conducted an evaluation on four\nstate-of-the-art open-source V-NLIs, both academic [174], [273]\nand commercial [2], [5]. We fou",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 92,
    "chunk_text": "four\nstate-of-the-art open-source V-NLIs, both academic [174], [273]\nand commercial [2], [5]. We found that none of the systems can\nrecognize that nitrogen dioxide and NO2 have the same meaning,\nnor can they recognize the relationship between age and birth\nyear. Therefore, the support of domain knowledge is crucial,\nespecially for extracting data attributes in the NL query. CogNet\n[249], a knowledge base dedicated to integrating various existing\nknowledge bases (e.g., FrameNet, YAGO, Freebase, DBpedia,\nWikidata, and ConceptNet), may be useful for broadening the\nrepertoire of utterances supported. Speciﬁcally, these knowledge\nbases can be wholly or partly used to create word embeddings.\nWith the help of embedding results, the similarity between N-grams\nof the NL query and data attributes names can be computed at both\nthe syntactic and semantic levels. Naturally, it is conceivable that\ndata attributes with high similarity or whose similarity exceeds a\ncertain threshold will be extracted.",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 93,
    "chunk_text": "a attributes with high similarity or whose similarity exceeds a\ncertain threshold will be extracted.\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n14\nTABLE 8\nSummary of existing V-NLI datasets.\nName\nPublication\nNL queries\nData tables\nBenchmark\nOther Contributions\nWebsite\nKim et al. [110]\nCHI’20\n629\n52\n×\nVQA with explanations\nhttps://github.com/dhkim16/VisQA-release\nQuda [61]\narXiv’20\n14,035\n36\n×\nThree Quda applications\nhttps://freenli.github.io/quda/\nNLV [228]\nCHI’21\n893\n3\n✓\nCharacterization of utterances\nhttps://nlvcorpus.github.io/\nnvBench [151]\nSIGMOD’21\n25,750\n780\n✓\nNL2SQL-to-NL2VIS and SEQ2VIS\nhttps://github.com/TsinghuaDatabaseGroup/nvBench\n11.2\nNLP Model\n11.2.1\nApplication of more advanced NLP models\nThe performance of V-NLI depends to a great extent on NLP\nmodels. As shown in Table 3 (column NLP Toolkit or Technology\nand column Recommendation Algorithm), most of the existing\nV-NLI systems just apply hand-crafted grammar rules or typical\nNLP ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 94,
    "chunk_text": "Algorithm), most of the existing\nV-NLI systems just apply hand-crafted grammar rules or typical\nNLP toolkits for convenience. Recently, several state-of-the-art\nNLP models have reached human performance in speciﬁc tasks,\nsuch as ELMO [179], BERT [49], GPT-3 [25], and CPM-2 [283].\nSome works have leveraged the advances for data visualization\n[251], [262]. However, few works have applied them in V-NLI.\nDuring our survey, we also found that existing works provided\nlimited support for free-form NL input. To construct a more robust\nsystem, a promising direction is to apply more advanced NLP\nmodels to learn universal grammar patterns from a large corpus of\nconversations in visual analysis. In the process, we believe that the\nexisting high-quality text datasets will help train and evaluate robust\nNLP models for V-NLI. Besides, the community can pay more\nattention to the end-to-end approach due to its increased efﬁciency,\ncost cutting, ease of learning, and smarter visualization inference\nabil",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 95,
    "chunk_text": "to its increased efﬁciency,\ncost cutting, ease of learning, and smarter visualization inference\nability (e.g., chart type selection, data insight mining, and missing\ndata prediction). To this end, advanced neural machine translation\nmodels [43] can be applied with various optimization schemes. NLP\nmodels of multiple languages also deserve attention since all V-NLI\nsystems we found can only support English. In fact, numerous NLP\nmodels can deal effectively with other languages (e.g., Chinese,\nFrench, and Spanish) and can be leveraged to develop V-NLI to\nserve more people who speak different languages.\n11.2.2\nDeep interpretation of dataset semantics\nSemantic information plays an important role in V-NLI. The state-\nof-the-art systems have considered leveraging semantic parsing\ntoolkits like SEMPRE [282] to parse NL queries [110], [273].\nHowever, just considering the semantics of the NL query is\nlimited, and the semantics of the dataset should also be taken\ninto consideration. The existing",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 96,
    "chunk_text": "y is\nlimited, and the semantics of the dataset should also be taken\ninto consideration. The existing technologies for data attribute\nmatching only conﬁne to the letter-matching level but do not go\ndeep into the semantic-matching level, as described in Section 4.3.\nFor example, when the analyzed dataset is about movies, a full-\nﬂedged system should be able to recognize that the Name attribute\nin the dataset refers to the movie name and automatically associate\nit with other attributes appearing in the query. A promising way to\naugment semantic interpretation ability is to connect with recent\nsemantic data type detection models for visualization like Sherlock\n[90], Sato [278], ColNet [31], DCOM [156], EXACTA [267], and\nDoduo [238]. Incorporating such connections will help better infer\nattribute types upon initialization and may also reduce the need\nfor developers to manually conﬁgure attribute aliases. Besides,\nthe aforementioned models are limited to a ﬁxed set of semantic\ntypes. An addi",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 97,
    "chunk_text": "ute aliases. Besides,\nthe aforementioned models are limited to a ﬁxed set of semantic\ntypes. An additional essential direction is to extend the existing\nsemantic data type detection models to support more semantic\ntypes. Additionally, with the contextually deep interpretation of\ndataset semantics, supporting queries for multi-table schema would\nbe an interesting research hotspot.\n11.3\nInteraction\n11.3.1\nRicher prompt to improve system discoverability\nThe user may not be aware of what input is valid and what chart\ntypes are supported by the system with open-ended textboxes. As\ndescribed in Section 8.2, discoverability has received relatively\nlittle attention in current V-NLIs compared to other characteristics.\nThe existing practices have revealed that real-time prompts can\neffectively help the user better understand system features and\ncorrect input errors in time when typing queries [206], [224],\n[273] (see Table 3 (column Autocompletion)). The most common\nmethod is template-based text",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 98,
    "chunk_text": "], [224],\n[273] (see Table 3 (column Autocompletion)). The most common\nmethod is template-based text autocompletion [273]. However,\nthis method is relatively monotonous in practice and does not\nwork for spoken commands. Several works offer the user prompt\nwidgets of data previews to speed up the user’s input speed\n[206], but the supporting prompt widget types are limited. We\nbelieve that richer interactive widgets with prompts and multimodal\nsynergistic interaction can greatly improve the system’s usability.\nShowing provenance of prompt behavior can also enhance the\ninterpretability of visualization results. Concerning this issue, Setlur\net al. [206] conducted a crowdsourcing study regarding the efﬁcacy\nof autocompletion suggestions. The insights drawn from the studies\nare of great value to inspire the future design of V-NLI.\n11.3.2\nTake advantage of the user’s interaction history\nThe intent behind NL queries can be satisﬁed by various types\nof charts. These visualization results are t",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 99,
    "chunk_text": "tent behind NL queries can be satisﬁed by various types\nof charts. These visualization results are too broad that existing\nsystems can hardly account for varying user preferences. Although\nseveral conversational V-NLI systems have been proposed [83],\n[121], [205] to analyze NL queries in context, few systems\nhave taken the user’s interaction history into account. Recently,\nZehrung et al. [276] conducted a crowdsourcing study analyzing\ntrust in humans versus algorithmically generated visualization\nrecommendations. Based on the results, they suggested that\nthe recommendation system should be customized according\nto the speciﬁc user’s information search strategy. Personalized\ninformation derived from historical user interaction and context\ncan provide a richer model to satisfy the user’s analytic tasks.\nA large number of innovative models in the recommendation\nsystem [281] can also be applied for reference. Besides, Lee et\nal. [130] recently deconstructed categorization in visualization\nr",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 100,
    "chunk_text": "ed for reference. Besides, Lee et\nal. [130] recently deconstructed categorization in visualization\nrecommendation. Several works [79], [209] have studied the\nassociations and expectations about verbalization and visualization\nreported by users. Integrating the information for further modeling\nof the user’s interaction history is another interesting research topic.\n11.4\nPresentation\nWe hypothesize that we would not truly have succeeded in\ndemocratizing access to visual analysis through natural language\nuntil we design systems that use NL both as input and output\nmodality. For this vision, a promising direction is to combine all the\naforementioned characteristics (e.g., VQA, NLG, and annotation) to\ndesign a hybrid system. In addition, existing V-NLI systems mostly\nonly support 2D visualization. Nowadays, Immersive Analytics (IA)\nis a quickly evolving ﬁeld that leverages immersive technologies\nfor data analysis [56], [60]. In the visualization community, several\n\nIEEE TRANSACTIONS ON VISU",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 101,
    "chunk_text": "ies\nfor data analysis [56], [60]. In the visualization community, several\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n15\nworks have augmented static visualizations with virtual content\n[35], [124]. Reinders et al. [189] studied blind and low vision\npeople’s preferences when exploring interactive 3D printed models\n(I3Ms). However, there have been no systems that support NLI\nfor data visualization in an immersive way. A related work for\nreference is that Lee et al. [125] proposed the concept of data\nvisceralization and introduced a conceptual pipeline. It would be\ninteresting to enrich this pipeline with the integration of V-NLI.\nBesides, as described in Section 7, view transformations are rarely\ninvolved in existing systems. With immersive technologies, more\nview transformations can be explored and integrated into V-NLI to\nprovide an immersive interactive experience.\n11.5\nDataset\nThere is a widely recognized consensus that large-scale data\ncollecti",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 102,
    "chunk_text": "ctive experience.\n11.5\nDataset\nThere is a widely recognized consensus that large-scale data\ncollection is an effective way to facilitate community development\n(e.g., ImageNet [47] for image processing and GEM [66] for\nnatural language generation). Although there are various datasets\nfor general NLP tasks, they can hardly be directly applied to\nprovide training samples for V-NLI models. In the visualization\ncommunity, several works have begun to collect large datasets\nfor V-NLI, as shown in Table 8. To assist the deployment of\nlearning-based techniques for parsing human language, Fu et al.\n[61] proposed Quda, containing diverse user queries annotated\nwith analytic tasks. Srinivasan et al. [228] conducted an online\nstudy to collect NL utterances and characterized them based on\ntheir phrasing type. VizNet [85] is a step forward in addressing the\nneed for large-scale data and visualization repositories. Kim et al.\n[110] collected questions people posed about various bar charts\nand line cha",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 103,
    "chunk_text": "epositories. Kim et al.\n[110] collected questions people posed about various bar charts\nand line charts, along with their answers and explanations to the\nquestions. Fully considering the user’s characteristics, Kassel et al.\n[104] introduced a linguistically motivated two-dimensional answer\nspace that varies in the level of both support and information to\nmatch the human language in visual analysis. The major limitation\nof them is that the query types contained are not rich enough.\nBesides, unfortunately, a benchmark or the ground truth of a given\ndataset is usually unavailable. Although Srinivasan et al. [228]\nmade an initial attempt to present a collection of NL utterances\nwith the mapping visualizations, Luo et al. [151] synthesized\nNL-to-Vis benchmarks by piggybacking NL-to-SQL benchmarks\nand produced a NL-to-Vis benchmark, nvBench, the supporting\nvisualization types and tasks are limited. Therefore, collecting\nlarge-scale datasets and creating new benchmarks that support\nmore task",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 104,
    "chunk_text": "mited. Therefore, collecting\nlarge-scale datasets and creating new benchmarks that support\nmore tasks, domains, and interactions will be an indispensable\nresearch direction in the future.\n11.6\nApplication\nApart from facilitating data analysis, much attention has been paid\nto beneﬁt real-world applications by integrating visual analysis\nand natural language interface. Huang et al. [88] proposed a\nquery engine to convert, store and retrieve spatial uncertain mobile\ntrajectories via intuitive NL input. Leo John et al. [133] proposed a\nnovel V-NLI to promote medical imaging and biomedical research.\nWith an audio travel podcast as input, Crosscast [266] identiﬁes\ngeographic locations and descriptive keywords within the podcast\ntranscript through NLP and text mining techniques. The information\nis later used to select relevant photos from online repositories\nand synchronize their display to align with the audio narration.\nMeetingVis [214] leverages ASR and NLP techniques to promote\neffective ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 105,
    "chunk_text": "gn with the audio narration.\nMeetingVis [214] leverages ASR and NLP techniques to promote\neffective meeting summaries in team-based workplaces. PathViewer\n[255] leverages ideas from ﬂow diagrams and NLP to visualize the\nsequences of intermediate steps that students take. Since V-NLI\ncan be easily integrated as a module into the visualization system,\nmore applications can be explored in the future.\n12\nCONCLUSION\nThe past two decades have witnessed the rapid development of\nvisualization-oriented natural language interfaces, which act as\na complementary input modality for visual analytics. However,\nthe community lacks a comprehensive survey of related works\nto guide follow-up research. We ﬁll the gap, and the resulting\nsurvey gives a comprehensive overview of what characteristics are\ncurrently concerned and supported by V-NLI. We also propose\nseveral promising directions for future work. To our knowledge,\nthis paper is the ﬁrst step towards reviewing V-NLI in a novel and\nsystematic manner",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 106,
    "chunk_text": " our knowledge,\nthis paper is the ﬁrst step towards reviewing V-NLI in a novel and\nsystematic manner. We hope that this paper can better guide future\nresearch and encourage the community to think further about NLI\nfor data visualization.\nACKNOWLEDGMENTS\nThe authors would like to thank all the reviewers for their\nvaluable comments. They also thank all the authors of related\npapers, especially those who kindly provide images covered in this\nsurvey. The work was supported by the National Natural Science\nFoundation of China (No. 71690231) and Beijing Key Laboratory\nof Industrial Bigdata System and Application.\nREFERENCES\n[1]\nApache OpenNLP. http://opennlp.apache.org/.\n[2]\nAsk data. https://www.tableau.com/products/new-features/ask-data.\n[3]\nGoogle NLP. https://cloud.google.com/natural-language/.\n[4]\nIBM Watson Analytics. http://www.ibm. com/analytics/watson-analytics.\n[5]\nMicrosoft Power BI. https://docs.microsoft.com/en-us/power-bi/create-\nreports/power-bi-tutorial-q-and-a.\n[6]\nK. Affolte",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 107,
    "chunk_text": " https://docs.microsoft.com/en-us/power-bi/create-\nreports/power-bi-tutorial-q-and-a.\n[6]\nK. Affolter, K. Stockinger, and A. Bernstein. A comparative survey of\nrecent natural language interfaces for databases. VLDB J., 28(5), 2019.\n[7]\nA. Akbik, D. Blythe, and R. Vollgraf. Contextual String Embeddings for\nSequence Labeling. In Proc. COLING’19. ACM, 2018.\n[8]\nR. A. Al-Zaidy and C. L. Giles. Automatic Extraction of Data from Bar\nCharts. In Proc. ICKC’15. ACM, 2015.\n[9]\nR. Amar, J. Eagan, and J. Stasko. Low-level components of analytic\nactivity in information visualization. In Proc. INFOVIS’05. IEEE, 2005.\n[10]\nF. Bacci, F. M. Cau, and L. D. Spano. Inspecting Data Using Natural\nLanguage Queries. Lect. Notes Comput. Sci., 12254:771–782, 2020.\n[11]\nB. Bach, Z. Wang, M. Farinella, and et al. Design Patterns for Data\nComics. In Proc. CHI’18. ACM, 2018.\n[12]\nD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by\njointly learning to align and translate. In Proc. ICLR’15, 2015.\n[13]\nC",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 108,
    "chunk_text": "eural machine translation by\njointly learning to align and translate. In Proc. ICLR’15, 2015.\n[13]\nC. Baik, H. V. Jagadish, and Y. Li. Bridging the semantic gap with SQL\nquery logs in natural language interfaces to databases. In Proc. ICDE’19.\nIEEE, 2019.\n[14]\nF. Basik, B. H¨attasch, A. Ilkhechi, and et al. DBPal: A learned NL-\ninterface for databases. In Proc. SIGMOD’18. ACM, 2018.\n[15]\nH. Bast and E. Haussmann. More Accurate Question Answering on\nFreebase. In Proc. CIKM’15. ACM, 2015.\n[16]\nL. Battle, R. J. Crouser, A. Nakeshimana, and et al. The Role of Latency\nand Task Complexity in Predicting Visual Search Behavior. IEEE Trans.\nVis. Comput. Graph., 26(1):1246–1255, 2020.\n[17]\nL. Battle and C. Scheidegger. A Structured Review of Data Management\nTechnology for Interactive Visualization and Analysis. IEEE Trans. Vis.\nComput. Graph., 27(2):1128–1138, 2021.\n[18]\nY. Belinkov and J. Glass.\nAnalysis Methods in Neural Language\nProcessing: A Survey. Trans. Assoc. Comput. Linguist., 7:49–72, ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 109,
    "chunk_text": "Analysis Methods in Neural Language\nProcessing: A Survey. Trans. Assoc. Comput. Linguist., 7:49–72, 2019.\n[19]\nS. Bergamaschi, F. Guerra, M. Interlandi, and et al. Combining user\nand database perspective for solving keyword queries over relational\ndatabases. Inf. Syst., 55:1–19, 2016.\n[20]\nS. Bieliauskas and A. Schreiber. A Conversational User Interface for\nSoftware Visualization. In Proc. VISSOFT’17. IEEE, 2017.\n[21]\nS. Bird. NLTK: the natural language toolkit. In Proc. COLING-ACL’06.\nACL, 2006.\n[22]\nL. Blunschi, C. Jossen, D. Kossmann, and et al. SODA: Generating SQL\nfor business users. Proc. VLDB Endow., 5(10):932–943, 2012.\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n16\n[23]\nM. Bostock, V. Ogievetsky, and J. Heer. D3: Data-Driven Documents.\nIEEE Trans. Vis. Comput. Graph., 17(12):2301–2309, 2011.\n[24]\nM. Brehmer and T. Munzner. A Multi-Level Typology of Abstract\nVisualization Tasks. IEEE Trans. Vis. Comput. Graph., 19(12), 2013.\n[25]\nT. Brown,",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 110,
    "chunk_text": "ology of Abstract\nVisualization Tasks. IEEE Trans. Vis. Comput. Graph., 19(12), 2013.\n[25]\nT. Brown, B. Mann, N. Ryder, and et al. Language Models are Few-Shot\nLearners. In Proc. NeurIPS’20. MIT Press, 2020.\n[26]\nC. Bryan, K. L. Ma, and J. Woodring. Temporal Summary Images:\nAn Approach to Narrative Visualization via Interactive Annotation\nGeneration and Placement. IEEE Trans. Vis. Comput. Graph., 2017.\n[27]\nZ. Bylinskii, S. Alsheikh, S. Madan, and et al.\nUnderstanding\nInfographics through Textual and Visual Tag Prediction. arXiv, 2017.\n[28]\nS. Card, J. Mackinlay, and B. Shneiderman. Readings in Information\nVisualization: Using Vision to Think. Morgan Kaufmann, 1999.\n[29]\nA. Celikyilmaz, E. Clark, and J. Gao. Evaluation of Text Generation: A\nSurvey. arxiv, pages 1–75, 2021.\n[30]\nR. Chaudhry, S. Shekhar, U. Gupta, and et al. LEAF-QA: Locate, encode\nattend for ﬁgure question answering. In Proc. WACV’20, 2020.\n[31]\nJ. Chen, E. Jim´enez-Ruiz, I. Horrocks, and C. Sutton.\nColNet:\nEmbedding th",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 111,
    "chunk_text": "roc. WACV’20, 2020.\n[31]\nJ. Chen, E. Jim´enez-Ruiz, I. Horrocks, and C. Sutton.\nColNet:\nEmbedding the Semantics of Web Tables for Column Type Prediction.\nIn Proc. AAAI’19. AAAI, 2019.\n[32]\nS. Chen, J. Li, G. Andrienko, and et al. Supporting Story Synthesis:\nBridging the Gap between Visual Analytics and Storytelling. IEEE Trans.\nVis. Comput. Graph., 26(7):2499–2516, 2020.\n[33]\nY. Chen, S. Barlowe, and J. Yang. Click2Annotate: Automated Insight\nExternalization with rich semantics. In Proc. VAST’10. IEEE, 2010.\n[34]\nY. Chen, J. Yang, S. Barlowe, and D. H. Jeong.\nTouch2Annotate:\nGenerating better annotations with less human effort on multi-touch\ninterfaces. In Proc. CHI’10. ACM, 2010.\n[35]\nZ. Chen, W. Tong, Q. Wang, and et al. Augmenting Static Visualizations\nwith PapARVis Designer. In Proc. CHI’20. ACM, 2020.\n[36]\nZ. Chen, Y. Wang, Q. Wang, and et al. Towards automated infographic\ndesign: Deep learning-based auto-extraction of extensible timeline. IEEE\nTrans. Vis. Comput. Graph., 26(1):91",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 112,
    "chunk_text": "eep learning-based auto-extraction of extensible timeline. IEEE\nTrans. Vis. Comput. Graph., 26(1):917–926, 2020.\n[37]\nJ. Choo, C. Lee, H. Kim, and et al.\nVisIRR: Visual analytics for\ninformation retrieval and recommendation with large-scale document\ndata. In Proc. VAST’14. IEEE, 2014.\n[38]\nK. Cook, N. Cramer, D. Israel, and et al. Mixed-initiative visual analytics\nusing task-driven recommendations. In Proc. VAST’15. IEEE, 2015.\n[39]\nM. Corio and G. Lapalme. Generation of texts for information graphics.\nIn Proc. EWNLG’99. ACL, 1999.\n[40]\nK. Cox, R. E. Grinter, S. L. Hibino, and et al. A multi-modal natural\nlanguage interface to an information visualization environment. Int. J.\nSpeech Technol., 4(3):297–314, 2001.\n[41]\nW. Cui, X. Zhang, Y. Wang, and et al. Text-to-Viz: Automatic Generation\nof Infographics from Proportion-Related Natural Language Statements.\nIEEE Trans. Vis. Comput. Graph., 26(1):906–916, 2020.\n[42]\nZ. Cui, S. K. Badam, M. A. Yalc¸in, and N. Elmqvist.\nDataSite:\nProactive ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 113,
    "chunk_text": " 26(1):906–916, 2020.\n[42]\nZ. Cui, S. K. Badam, M. A. Yalc¸in, and N. Elmqvist.\nDataSite:\nProactive visual data exploration with computation of insight-based\nrecommendations. Inf. Vis., 18(2):251–267, 2019.\n[43]\nR. Dabre, C. Chu, and A. Kunchukuttan. A Survey of Multilingual\nNeural Machine Translation. ACM Comput. Surv., 53(5), 2020.\n[44]\nS. Demir, S. Carberry, and K. F. McCoy. Generating textual summaries\nof bar charts. In Proc. INLG’08. ACL, 2008.\n[45]\nS. Demir, S. Carberry, and K. F. McCoy. Summarizing Information\nGraphics Textually. Comput. Linguist., 38(3):527–574, 2012.\n[46]\nC¸ . Demiralp, P. J. Haas, S. Parthasarathy, and T. Pedapati. Foresight:\nRecommending visual insights. VLDB Endow., 10(12):1937–1940, 2017.\n[47]\nJ. Deng, W. Dong, R. Socher, and et al.\nImageNet: A large-scale\nhierarchical image database. In Proc. CVPR’09. IEEE, 2009.\n[48]\nA. P. Deshpande and C. N. Mahender. Summarization of Graph Using\nQuestion Answer Approach. In Adv. Intell. Syst. Comput., pages 205–216.\nSp",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 114,
    "chunk_text": "marization of Graph Using\nQuestion Answer Approach. In Adv. Intell. Syst. Comput., pages 205–216.\nSpringer, 2020.\n[49]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding. In\nProc. NAACL’19. ACL, 2019.\n[50]\nK. Dhamdhere, K. S. McCurley, R. Nahmias, and et al.\nAnalyza:\nExploring data with conversation. In Proc. IUI’17. ACM, 2017.\n[51]\nV. Dibia and C. Demiralp. Data2Vis: Automatic Generation of Data\nVisualizations Using Sequence-to-Sequence Recurrent Neural Networks.\nIEEE Comput. Graph. Appl., 39(5):33–46, 2019.\n[52]\nE. Dimara and C. Perin. What is Interaction for Data Visualization?\nIEEE Trans. Vis. Comput. Graph., 26(1):119–129, 2020.\n[53]\nR. Ding, S. Han, Y. Xu, and et al. Quickinsights: Quick and automatic\ndiscovery of insights from multi-dimensional data. In Proc. SIGMOD’19.\nACM, 2019.\n[54]\nI. K. Duncan, S. Tingsheng, S. T. Perrault, and M. T. Gastner. Task-\nBased Effectiveness of Interactive Contiguous A",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 115,
    "chunk_text": " Tingsheng, S. T. Perrault, and M. T. Gastner. Task-\nBased Effectiveness of Interactive Contiguous Area Cartograms. IEEE\nTrans. Vis. Comput. Graph., 27(3):2136–2152, 2021.\n[55]\nJ. Eisenschlos, S. Krichene, and T. M¨uller. Understanding tables with\nintermediate pre-training. In Find. Assoc. Comput. Linguist. EMNLP\n2020. ACL, 2020.\n[56]\nB. Ens, B. Bach, M. Cordeil, and et al. Grand Challenges in Immersive\nAnalytics. In Proc. CHI’21. ACM, 2021.\n[57]\nM. Fasciano and G. Lapalme. PostGraphe: a system for the generation of\nstatistical graphics and text Important factors in the generation process.\nIn Proc. INLG’96. ACL, 1996.\n[58]\nE. Fast, B. Chen, J. Mendelsohn, and et al. Iris: A conversational agent\nfor complex tasks. In Proc. CHI’18. ACM, 2018.\n[59]\nL. Ferres, G. Lindgaard, and L. Sumegi. Evaluating a tool for improving\naccessibility to charts and graphs. In Proc. ASSETS’10. ACM, 2010.\n[60]\nA. Fonnet and Y. Prie. Survey of Immersive Analytics. IEEE Trans. Vis.\nComput. Graph., 27(3):2101–21",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 116,
    "chunk_text": "A. Fonnet and Y. Prie. Survey of Immersive Analytics. IEEE Trans. Vis.\nComput. Graph., 27(3):2101–2122, 2019.\n[61]\nS. Fu, K. Xiong, X. Ge, and et al. Quda: Natural Language Queries for\nVisual Data Analytics. arXiv.\n[62]\nJ. Fulda, M. Brehmel, and T. Munzner. TimeLineCurator: Interactive\nAuthoring of Visual Timelines from Unstructured Text. IEEE Trans. Vis.\nComput. Graph., 22(1):300–309, 2016.\n[63]\nT. Gao, M. Dontcheva, E. Adar, and et al. Datatone: Managing ambiguity\nin natural language interfaces for data visualization. In Proc. UIST’15.\nACM, 2015.\n[64]\nT. Gao, J. Hullman, E. Adar, and et al. NewsViews: An automated\npipeline for creating custom geovisualizations for news. In Proc. CHI’14.\nACM, 2014.\n[65]\nT. Ge, B. Lee, and Y. Wang. CAST: Authoring Data-Driven Chart\nAnimations. In Proc. CHI’21. ACM, 2021.\n[66]\nS. Gehrmann, T. Adewumi, K. Aggarwal, and et al.\nThe GEM\nBenchmark: Natural Language Generation, its Evaluation and Metrics.\nIn Proc. GEM’21, pages 96–120. ACL, 2021.\n[67]\nA. Ghos",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 117,
    "chunk_text": "guage Generation, its Evaluation and Metrics.\nIn Proc. GEM’21, pages 96–120. ACL, 2021.\n[67]\nA. Ghosh, M. Nashaat, J. Miller, and et al. A comprehensive review\nof tools for exploratory analysis of tabular industrial datasets.\nVis.\nInformatics, 2(4):235–253, 2018.\n[68]\nM. Gingerich and C. Conati.\nConstructing models of user and\ntask characteristics from eye gaze data for user-adaptive information\nhighlighting. In Proc. AAAI’15. AAAI, 2015.\n[69]\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, and et al.\nGenerative\nAdversarial Nets. In Proc. NIPS’14. MIT Press, 2014.\n[70]\nD. Gotz and Z. Wen. Behavior-driven visualization recommendation. In\nProc. IUI’09. ACM, 2009.\n[71]\nB. J. Grosz and C. L. Sidner. Attention, intentions, and the structure of\ndiscourse. Comput. Linguist., 12(3):175–204, 1986.\n[72]\nJ. Gu, Z. Lu, H. Li, and V. O. Li. Incorporating Copying Mechanism in\nSequence-to-Sequence Learning. In Proc. ACL’16. ACL, 2016.\n[73]\nJ. Guo, Z. Zhan, Y. Gao, and et al. Towards Complex Text-to-SQL\nin",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 118,
    "chunk_text": " In Proc. ACL’16. ACL, 2016.\n[73]\nJ. Guo, Z. Zhan, Y. Gao, and et al. Towards Complex Text-to-SQL\nin Cross-Domain Database with Intermediate Representation. In Proc.\nACL’19. ACL, 2019.\n[74]\nI. Gur, S. Yavuz, Y. Su, and X. Yan. DialSQL: Dialogue Based Structured\nQuery Generation. In Proc. ACL’18. ACL, 2018.\n[75]\nC. Harris, R. A. Rossi, S. Malik, and et al. Insight-centric Visualization\nRecommendation. arXiv, 2021.\n[76]\nK. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image\nRecognition. In IEEE, editor, Proc. CVPR’16. IEEE, 2016.\n[77]\nM. Hearst and M. Tory.\nWould You Like A Chart With That?\nIncorporating Visualizations into Conversational Interfaces. In Proc.\nVIS’19. IEEE, 2019.\n[78]\nM. Hearst, M. Tory, and V. Setlur. Toward Interface Defaults for Vague\nModiﬁers in Natural Language Interfaces for Visual Analysis. In Proc.\nVIS’19. IEEE, 2019.\n[79]\nR. Henkin and C. Turkay. Words of Estimative Correlation: Studying\nVerbalizations of Scatterplots. IEEE Trans. Vis. Comput. Grap",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 119,
    "chunk_text": "ds of Estimative Correlation: Studying\nVerbalizations of Scatterplots. IEEE Trans. Vis. Comput. Graph., 2020.\n[80]\nJ. Herzig, P. K. Nowak, T. M¨uller, and et al. TaPas: Weakly Supervised\nTable Parsing via Pre-training. In Proc. ACL’20. ACL, 2020.\n[81]\nM. Honnibal and I. Montani. spacy 2: Natural language understanding\nwith Bloom embeddings.\nConvolutional Neural Networks Increm.\nParsing, 2017.\n[82]\nA. K. Hopkins, M. Correll, and A. Satyanarayan. VisuaLint: Sketchy In\nSitu Annotations of Chart Construction Errors. Comput. Graph. Forum,\n39(3):219–228, 2020.\n[83]\nE. Hoque, V. Setlur, M. Tory, and I. Dykeman. Applying Pragmatics\nPrinciples for Interaction with Visual Analytics. IEEE Trans. Vis. Comput.\nGraph., 24(1):309–318, 2018.\n[84]\nK. Hu, M. A. Bakker, S. Li, and et al. VizML: A machine learning\napproach to visualization recommendation. In Proc. CHI’19. ACM.\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n17\n[85]\nK. Hu, S. S. Gaikwad, M. Hulsebos, and ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 120,
    "chunk_text": "ATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n17\n[85]\nK. Hu, S. S. Gaikwad, M. Hulsebos, and et al. VizNet: Towards a\nlarge-scale visualization learning and benchmarking repository. In Proc.\nCHI’19. ACM, 2019.\n[86]\nK. Hu, D. Orghian, and C. Hidalgo. DIVE: A mixed-initiative system\nsupporting integrated data exploration workﬂows. In Proc. HILDA’2018.\nACM, 2018.\n[87]\nB. Huang, G. Zhang, and P. C.-Y. Sheu. A Natural Language Database\nInterface Based on a Probabilistic Context Free Grammar. In Proc.\nWSCS’08. IEEE, 2008.\n[88]\nZ. Huang, Y. Zhao, W. Chen, and et al. A Natural-language-based Visual\nQuery Approach of Uncertain Human Trajectories. IEEE Trans. Vis.\nComput. Graph., 26(1):1–11, 2019.\n[89]\nJ. Hullman, N. Diakopoulos, and E. Adar. Contextiﬁer: Automatic\ngeneration of annotated stock visualizations. In Proc. CHI’13. ACM.\n[90]\nM. Hulsebos, A. Satyanarayan, and et al. Sherlock: A deep learning\napproach to semantic data type detection. In Proc. KDD’19. ACM, 2019.\n[91]\nR. Ingria, R",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 121,
    "chunk_text": "eep learning\napproach to semantic data type detection. In Proc. KDD’19. ACM, 2019.\n[91]\nR. Ingria, R. Sauri, J. Pustejovsky, and et al.\nTimeML: Robust\nSpeciﬁcation of Event and Temporal Expressions in Text. New Dir.\nQuest. answering, 3:28–34, 2003.\n[92]\nS. Iyer, I. Konstas, A. Cheung, and et al. Learning a Neural Semantic\nParser from User Feedback. In Proc. ACL’17. ACL, 2017.\n[93]\nQ. Jiang, G. Sun, Y. Dong, and R. Liang. DT2VIS: A Focus+Context\nAnswer Generation System to Facilitate Visual Exploration of Tabular\nData. IEEE Comput. Graph. Appl., 41(5):45–56, 2021.\n[94]\nS. Jiang, P. Kang, X. Song, and et al. Emerging Wearable Interfaces and\nAlgorithms for Hand Gesture Recognition: A Survey. IEEE Rev. Biomed.\nEng., pages 1–11, 2021.\n[95]\nM. Joshi, D. Chen, Y. Liu, and et al. SpanBERT: Improving Pre-training\nby Representing and Predicting Spans. Trans. Assoc. Comput. Linguist.,\n8:64–77, 2020.\n[96]\nM. Joshi, O. Levy, L. Zettlemoyer, and D. Weld. BERT for Coreference\nResolution: Baselines an",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 122,
    "chunk_text": ".\n[96]\nM. Joshi, O. Levy, L. Zettlemoyer, and D. Weld. BERT for Coreference\nResolution: Baselines and Analysis. In Proc. EMNLP’19. ACL, 2019.\n[97]\nK. Kaﬂe, B. Price, S. Cohen, and C. Kanan. DVQA: Understanding Data\nVisualizations via Question Answering. In Proc. CVPR’18. IEEE, 2018.\n[98]\nS. E. Kahou, V. Michalski, A. Atkinson, and et al. FigureQA: An\nAnnotated Figure Dataset for Visual Reasoning. In Proc. ICLR’18, 2018.\n[99]\nS. Kandel, A. Paepcke, J. Hellerstein, and J. Heer. Wrangler: Interactive\nvisual speciﬁcation of data transformation scripts. In Proc. CHI’11.\nACM, 2011.\n[100] S. Kandel, R. Parikh, A. Paepcke, and et al. Proﬁler: Integrated statistical\nanalysis and visualization for data quality assessment. In Proc. AVI’12.\nACM, 2012.\n[101] E. Kandogan. Just-in-time annotation of clusters, outliers, and trends in\npoint-based data visualizations. In Proc. VAST’12. IEEE, 2012.\n[102] J. Kang, K. Condiff, S. Chang, and et al. Understanding How People\nUse Natural Language to Ask for Re",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 123,
    "chunk_text": ". Kang, K. Condiff, S. Chang, and et al. Understanding How People\nUse Natural Language to Ask for Recommendations. In Proc. RecSys’17.\nACM, 2017.\n[103] J. F. Kassel and M. Rohs. Valletto: A multimodal interface for ubiquitous\nvisual analytics. In Proc. CHI EA’18. ACM, 2018.\n[104] J. F. Kassel and M. Rohs. Talk to me intelligibly: Investigating an answer\nspace to match the user’s language in visual analysis. In Proc. DIS’19.\nACM, 2019.\n[105] T. Kato, M. Matsushita, and E. Maeda. Answering it with charts: dialogue\nin natural language and charts. In Proc. COLING’02. ACM, 2002.\n[106] P. Kaur, M. Owonibi, and B. Koenig-Ries.\nTowards visualization\nrecommendation-a semi-automated domain-speciﬁc learning approach.\nCEUR Workshop Proc., 1366:30–35, 2015.\n[107] S. Kerpedjiev, G. Carenini, S. F. Roth, and J. D. Moore. AutoBrief:\na multimedia presentation system for assisting data analysis. Comput.\nStand. Interfaces, 18(6-7):583–593, 1997.\n[108] N. Kerracher and J. Kennedy. Constructing and Evaluat",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 124,
    "chunk_text": "tand. Interfaces, 18(6-7):583–593, 1997.\n[108] N. Kerracher and J. Kennedy. Constructing and Evaluating Visualisation\nTask Classiﬁcations: Process and Considerations. Comput. Graph. Forum,\n36(3):47–59, 2017.\n[109] A. Key, B. Howe, D. Perry, and C. Aragon. VizDeck: Self-organizing\ndashboards for visual analytics. In Proc. SIGMOD’12. ACM, 2012.\n[110] D. H. Kim, E. Hoque, and M. Agrawala. Answering Questions about\nCharts and Generating Visual Explanations. In Proc. CHI’20. ACM.\n[111] D. H. Kim, V. Setlur, and M. Agrawala. Towards Understanding How\nReaders Integrate Charts and Captions: A Case Study with Line Charts.\nIn Proc. CHI’21. ACM, 2021.\n[112] H. Kim, J. Oh, Y. Han, and et al. Thumbnails for Data Stories: A Survey\nof Current Practices. In Proc. VIS’19. IEEE, 2019.\n[113] Y. Kim and J. Heer. Assessing Effects of Task and Data Distribution\non the Effectiveness of Visual Encodings.\nComput. Graph. Forum,\n37(3):157–167, 2018.\n[114] Y. Kim and J. Heer. Gemini: A Grammar and Recommender Sys",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 125,
    "chunk_text": ". Graph. Forum,\n37(3):157–167, 2018.\n[114] Y. Kim and J. Heer. Gemini: A Grammar and Recommender System for\nAnimated Transitions in Statistical Graphics. IEEE Trans. Vis. Comput.\nGraph., 27(2):485–494, 2021.\n[115] Y.-H. Kim, B. Lee, A. Srinivasan, and E. K. Choe. Data@Hand: Fostering\nVisual Exploration of Personal Data on Smartphones Leveraging Speech\nand Touch Interaction. In Proc. CHI’21. ACM, 2021.\n[116] R. Kincaid and G. Pollock. Nicky: Toward a Virtual Assistant for Test\nand Measurement Instrument Recommendations. In Proc. ICSC’17.\nIEEE, 2017.\n[117] Y. Kirstain, O. Ram, and O. Levy. Coreference Resolution without Span\nRepresentations. arXiv, 2021.\n[118] V. Kocijan, A.-M. Cretu, O.-M. Camburu, and et al. A Surprisingly\nRobust Trick for the Winograd Schema Challenge. Proc. 57th Annu.\nMeet. Assoc. Comput. Linguist. ACL’19, pages 4837–4842, 2019.\n[119] N. Kong and M. Agrawala. Graphical Overlays: Using Layered Elements\nto Aid Chart Reading. IEEE Trans. Vis. Comput. Graph., 18(12), 201",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 126,
    "chunk_text": " Overlays: Using Layered Elements\nto Aid Chart Reading. IEEE Trans. Vis. Comput. Graph., 18(12), 2012.\n[120] A. Kumar, J. Aurisano, B. Di Eugenio, and et al. Multimodal Coreference\nResolution for Exploratory Data Visualization Dialogue: Context-Based\nAnnotation and Gesture Identiﬁcation. In Proc. SEMDIAL’17. ISCA.\n[121] A. Kumar, J. Aurisano, and et al. Towards a dialogue system that\nsupports rich visualizations of data. In Proc. SIGDIAL’16. ACL, 2016.\n[122] C. Lai, Z. Lin, R. Jiang, and et al. Automatic Annotation Synchronizing\nwith Textual Description for Visualization. In Proc. CHI’20. ACM, 2020.\n[123] S. Lalle, D. Toker, and C. Conati. Gaze-Driven Adaptive Interventions\nfor Magazine-Style Narrative Visualizations. IEEE Trans. Vis. Comput.\nGraph., 27(6):2941–2952, 2019.\n[124] R. Langner, M. Satkowski, W. B¨uschel, and R. Dachselt. MARVIS:\nCombining Mobile Devices and Augmented Reality for Visual Data\nAnalysis. In Proc. CHI’21. ACM, 2021.\n[125] B. Lee, D. Brown, B. Lee, and et al. Da",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 127,
    "chunk_text": " for Visual Data\nAnalysis. In Proc. CHI’21. ACM, 2021.\n[125] B. Lee, D. Brown, B. Lee, and et al. Data Visceralization: Enabling\nDeeper Understanding of Data Using Virtual Reality. IEEE Trans. Vis.\nComput. Graph., 27(2):1095–1105, 2021.\n[126] B. Lee, P. Isenberg, N. H. Riche, and S. Carpendale. Beyond Mouse\nand Keyboard: Expanding Design Considerations for Information\nVisualization Interactions. IEEE Trans. Vis. Comput. Graph., 2012.\n[127] B. Lee, C. Plaisant, C. S. Parr, and et al. Task taxonomy for graph\nvisualization. In Proc. BELIV’06. ACM, 2006.\n[128] B. Lee, A. Srinivasan, P. Isenberg, and J. Stasko. Post-wimp interaction\nfor information visualization. Found. Trends Human-Computer Interact.,\n14(1):1–95, 2021.\n[129] D. J. L. Lee, A. Quamar, E. Kandogan, and F. ¨Ozcan. Boomerang:\nProactive Insight-Based Recommendations for Guiding Conversational\nData Analysis. In Proc. SIGMOD’21 demo, 2021.\n[130] D. J.-L. Lee, V. Setlur, M. Tory, and et al. Deconstructing Categorization\nin Visualiz",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 128,
    "chunk_text": ", 2021.\n[130] D. J.-L. Lee, V. Setlur, M. Tory, and et al. Deconstructing Categorization\nin Visualization Recommendation: A Taxonomy and Comparative Study.\nIEEE Trans. Vis. Comput. Graph., 2626:1–14, 2021.\n[131] K. Lee, L. He, M. Lewis, and L. Zettlemoyer.\nEnd-to-end Neural\nCoreference Resolution. In Proc. EMNLP’17. ACL, 2017.\n[132] K. Lee, L. He, and L. Zettlemoyer. Higher-Order Coreference Resolution\nwith Coarse-to-Fine Inference. In Proc. NAACL’18. ACL, 2018.\n[133] R. J. Leo John, J. M. Patel, A. L. Alexander, and et al. A Natural\nLanguage Interface for Dissemination of Reproducible Biomedical Data\nScience. Lect. Notes Comput. Sci., 11073:197–205, 2018.\n[134] R. J. Leo John, N. Potti, and J. M. Patel. Ava: From data to insights\nthrough conversation. In Proc. CIDR’17, 2017.\n[135] F. Li and H. V. Jagadish.\nNaLIR: An interactive natural language\ninterface for querying relational databases. In Proc. SIGMOD’14. ACM.\n[136] F. Li and H. V. Jagadish. Constructing an interactive natural lang",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 129,
    "chunk_text": "s. In Proc. SIGMOD’14. ACM.\n[136] F. Li and H. V. Jagadish. Constructing an interactive natural language\ninterface for relational databases. Proc. VLDB Endow., 8(1):73–84, 2014.\n[137] F. Li and H. V. Jagadish. Understanding Natural Language Queries over\nRelational Databases. ACM SIGMOD Rec., 45(1):6–13, 2016.\n[138] H. Li, Y. Wang, S. Zhang, and et al. KG4Vis: A Knowledge Graph-Based\nApproach for Visualization Recommendation. IEEE Trans. Vis. Comput.\nGraph., pages 1–11, 2021.\n[139] J. Li, J. Yang, A. Hertzmann, and et al. LayoutGAN: Generating Graphic\nLayouts with Wireframe Discriminators. arXiv, 2019.\n[140] L. H. Li, M. Yatskar, D. Yin, and et al. VisualBERT: A Simple and\nPerformant Baseline for Vision and Language. arXiv, 2019.\n[141] P. Li, L. Liu, J. Xu, and et al. Application of Hidden Markov Model in\nSQL Injection Detection. In Proc. COMPSAC’17. IEEE, 2017.\n[142] H. Lin, D. Moritz, and J. Heer. Dziban: Balancing Agency & Automation\nin Visualization Design via Anchored Recommendatio",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 130,
    "chunk_text": "nd J. Heer. Dziban: Balancing Agency & Automation\nin Visualization Design via Anchored Recommendations.\nIn Proc.\nCHI’20. ACM, 2020.\n[143] C. Liu, Y. Han, R. Jiang, and X. Yuan.\nADVISor: Automatic\nVisualization Answer for Natural-Language Question on Tabular Data.\nIn Proc. PaciﬁcVis’21. IEEE, 2021.\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n18\n[144] C. Liu, L. Xie, Y. Han, and et al. AutoCaption: An Approach to Generate\nNatural Language Description from Visualization Automatically. In Proc.\nPaciﬁcVis’20. IEEE, 2020.\n[145] Q. Liu, H. Jiang, Z.-H. Ling, and et al. Commonsense Knowledge\nEnhanced Embeddings for Solving Pronoun Disambiguation Problems\nin Winograd Schema Challenge. arXiv, 2016.\n[146] T. Liu, X. Li, C. Bao, and et al. Data-Driven Mark Orientation for Trend\nEstimation in Scatterplots. In Proc. CHI’21. ACM, 2021.\n[147] G. L´opez, L. Quesada, and L. A. Guerrero. Alexa vs. Siri vs. Cortana\nvs. Google Assistant: A Comparison of Speech-Based N",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 131,
    "chunk_text": " and L. A. Guerrero. Alexa vs. Siri vs. Cortana\nvs. Google Assistant: A Comparison of Speech-Based Natural User\nInterfaces. In Proc. AHFE’17. Springer, 2017.\n[148] Y. Luo, X. Qin, N. Tang, and et al. DeepEye: Creating good data\nvisualizations by keyword search. In Proc. SIGMOD’18. ACM, 2018.\n[149] Y. Luo, X. Qin, N. Tang, and G. Li. Deepeye: towards automatic data\nvisualization. In Proc. ICDE’18. IEEE, 2018.\n[150] Y. Luo, N. Tang, G. Li, and et al. Natural Language to Visualization by\nNeural Machine Translation. In Proc. VIS’21. IEEE, 2021.\n[151] Y. Luo, N. Tang, G. Li, and et al. Synthesizing Natural Language to\nVisualization (NL2VIS) Benchmarks from NL2SQL Benchmarks. In\nProc. SIGMOD’21. ACM, 2021.\n[152] Y. Ma, A. K. H. Tung, W. Wang, and et al. ScatterNet: A Deep Subjective\nSimilarity Model for Visual Analysis of Scatterplots. IEEE Trans. Vis.\nComput. Graph., 26(3):1562–1576, 2020.\n[153] J. Mackinlay.\nAutomating the design of graphical presentations of\nrelational information. ACM Tr",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 132,
    "chunk_text": "53] J. Mackinlay.\nAutomating the design of graphical presentations of\nrelational information. ACM Trans. Graph., 5(2):110–141, 1986.\n[154] J. Mackinlay, P. Hanrahan, and C. Stolte.\nShow Me: Automatic\nPresentation for Visual Analysis. IEEE Trans. Vis. Comput. Graph.,\n13(6):1137–1144, 2007.\n[155] S. Madan, Z. Bylinskii, M. Tancik, and et al. Synthetically Trained Icon\nProposals for Parsing and Summarizing Infographics. arXiv, 2018.\n[156] S. Maji, S. S. Rout, and S. Choudhary. DCoM: A Deep Column Mapper\nfor Semantic Data Type Detection. arXiv, 2021.\n[157] C. Manning, M. Surdeanu, J. Bauer, and et al. The Stanford CoreNLP\nNatural Language Processing Toolkit. In Proc. ACL’14. ACL, 2014.\n[158] M. Mathew, V. Bagal, and et al. InfographicVQA. arXiv, 2021.\n[159] M. Matsushita, E. Maeda, and T. Kato. An interactive visualization\nmethod of numerical data based on natural language requirements. Int. J.\nHum. Comput. Stud., 60(4):469–488, 2004.\n[160] L. McNabb and R. S. Laramee. Survey of Surveys (S",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 133,
    "chunk_text": " J.\nHum. Comput. Stud., 60(4):469–488, 2004.\n[160] L. McNabb and R. S. Laramee. Survey of Surveys (SoS) - Mapping\nThe Landscape of Survey Papers in Information Visualization. Comput.\nGraph. Forum, 36(3):589–617, 2017.\n[161] R. Metoyer, B. Lee, N. Henry Riche, and M. Czerwinski. Understanding\nthe verbal language and structure of end-user descriptions of data\nvisualizations. In Proc. CHI’12. ACM, 2012.\n[162] R. Metoyer, Q. Zhi, B. Janczuk, and W. Scheirer.\nCoupling story\nto visualization: Using textual analysis as a bridge between data and\ninterpretation. In Proc. IUI’18. ACM, 2018.\n[163] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient estimation of\nword representations in vector space. In Proc. ICLR’13, 2013.\n[164] G. A. Miller. WordNet: A Lexical Database for English. Commun. ACM,\n38(11):39–41, 1995.\n[165] M. Mitri.\nStory Analysis Using Natural Language Processing and\nInteractive Dashboards. J. Comput. Inf. Syst., pages 1–11, 2020.\n[166] V. O. Mittal, J. D. Moore, G. Carenini, a",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 134,
    "chunk_text": "Dashboards. J. Comput. Inf. Syst., pages 1–11, 2020.\n[166] V. O. Mittal, J. D. Moore, G. Carenini, and S. Roth. Describing Complex\nCharts in Natural Language: A Caption Generation System. Comput.\nLinguist., 24(3):431–467, 1998.\n[167] P. Moraes, G. Sina, K. McCoy, and S. Carberry. Generating Summaries\nof Line Graphs. In Proc. INLG’14. ACL, 2014.\n[168] D. Moritz, C. Wang, G. L. Nelson, and et al. Formalizing Visualization\nDesign Knowledge as Constraints: Actionable and Extensible Models in\nDraco. IEEE Trans. Vis. Comput. Graph., 25(1):438–448, 2019.\n[169] T. Murillo-Morales and K. Miesenberger. AUDiaL: A Natural Language\nInterface to Make Statistical Charts Accessible to Blind Persons. In Lect.\nNotes Comput. Sci., pages 373–384. Springer, 2020.\n[170] B. Mutlu, E. Veas, and C. Trattner. VizRec: Recommending personalized\nvisualizations. ACM Trans. Interact. Intell. Syst., 6(4):1–39, 2016.\n[171] M. Nafari and C. Weaver.\nAugmenting Visualization with Natural\nLanguage Translation of Interacti",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 135,
    "chunk_text": "71] M. Nafari and C. Weaver.\nAugmenting Visualization with Natural\nLanguage Translation of Interaction: A Usability Study. Comput. Graph.\nForum, 32(3):391–400, 2013.\n[172] M. Nafari and C. Weaver. Query2Question: Translating visualization\ninteraction into natural language. IEEE Trans. Vis. Comput. Graph.,\n21(6):756–769, 2015.\n[173] A. Narechania, A. Fourney, and et al. DIY: Assessing the Correctness of\nNatural Language to SQL Systems. In Proc. IUI’21. ACM, 2021.\n[174] A. Narechania, A. Srinivasan, and J. Stasko. NL4DV: A Toolkit for\nGenerating Analytic Speciﬁcations for Data Visualization from Natural\nLanguage Queries. IEEE Trans. Vis. Comput. Graph., 27(2), 2021.\n[175] N. Nihalani, S. Silakari, and M. Motwani. Natural language Interface\nfor Database: A Brief review. Int. J. Comput. Sci., 8(2):600–608, 2011.\n[176] L. G. Nonato and M. Aupetit. Multidimensional Projection for Visual\nAnalytics: Linking Techniques with Distortions, Tasks, and Layout\nEnrichment. IEEE Trans. Vis. Comput. Gra",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 136,
    "chunk_text": "ics: Linking Techniques with Distortions, Tasks, and Layout\nEnrichment. IEEE Trans. Vis. Comput. Graph., 25(8):2650–2673, 2019.\n[177] J. Obeid and E. Hoque. Chart-to-Text: Generating Natural Language\nDescriptions for Charts by Adapting the Transformer Model. In Proc.\nINLG’20. ACL, 2020.\n[178] M. Oppermann, R. Kincaid, and T. Munzner. VizCommender: Com-\nputing Text-Based Similarity in Visualization Repositories for Content-\nBased Recommendations. IEEE Trans. Vis. Comput. Graph., 2021.\n[179] M. Peters, M. Neumann, M. Iyyer, and et al. Deep Contextualized Word\nRepresentations. In Proc. NAACL’18. ACL, 2018.\n[180] J. Poco and J. Heer. Reverse-Engineering Visualizations: Recovering\nVisual Encodings from Chart Images. Comput. Graph. Forum, 2017.\n[181] S. Pradhan, A. Moschitti, N. Xue, and et al. CoNLL-2012 Shared Task:\nModeling Multilingual Unrestricted Coreference in OntoNotes. In Proc.\nEMNLP’12. ACL, 2012.\n[182] P. Qi, Y. Zhang, Y. Zhang, and et al. Stanza: A Python Natural Language\nProcess",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 137,
    "chunk_text": "12. ACL, 2012.\n[182] P. Qi, Y. Zhang, Y. Zhang, and et al. Stanza: A Python Natural Language\nProcessing Toolkit for Many Human Languages. In Proc. ACL’20. ACL.\n[183] C. Qian, S. Sun, W. Cui, and et al. Retrieve-Then-Adapt: Example-based\nAutomatic Generation for Proportion-related Infographics. IEEE Trans.\nVis. Comput. Graph., 27(2):443–452, 2021.\n[184] X. Qian, E. Koh, F. Du, and et al. Generating Accurate Caption Units\nfor Figure Captioning. In Proc. WWW’21. ACM, 2021.\n[185] X. Qian, R. A. Rossi, F. Du, and et al.\nLearning to Recommend\nVisualizations from Data. In Proc. KDD’21. ACM, 2021.\n[186] X. Qian, R. A. Rossi, F. Du, and et al. Personalized Visualization\nRecommendation. arXiv, 2021.\n[187] X. Qin, Y. Luo, N. Tang, and G. Li. Making data visualization more\nefﬁcient and effective: a survey. VLDB J., 29(1):93–117, 2020.\n[188] A. Quamar, C. Lei, D. Miller, and et al. An Ontology-Based Conversation\nSystem for Knowledge Bases. In Proc. SIGMOD’20. ACM, 2020.\n[189] S. Reinders, M. Butler",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 138,
    "chunk_text": "Conversation\nSystem for Knowledge Bases. In Proc. SIGMOD’20. ACM, 2020.\n[189] S. Reinders, M. Butler, and K. Marriott. ”Hey Model!” - Natural User\nInteractions and Agency in Accessible Interactive 3D Models. In Proc.\nCHI’20. ACM, 2020.\n[190] D. Ren, M. Brehmer, Bongshin Lee, and et al. ChartAccent: Annotation\nfor data-driven storytelling. In Proc. PaciﬁcVis’17. IEEE, 2017.\n[191] A. Rind, W. Aigner, M. Wagner, and et al.\nTask Cube: A three-\ndimensional conceptual space of user tasks in visualization design and\nevaluation. Inf. Vis., 15(4):288–300, 2016.\n[192] S. F. Roth, J. Kolojejchick, J. Mattis, and J. Goldstein. Interactive graphic\ndesign using automatic presentation knowledge. In Proc. CHI’94. ACM.\n[193] D. Saha, A. Floratou, K. Sankaranarayanan, and et al. ATHENA: An\nontologydriven system for natural language querying over relational data\nstores. Proc. VLDB Endow., 9(12):1209–1220, 2016.\n[194] B. Saket, A. Endert, and C. Demiralp. Task-Based Effectiveness of Basic\nVisualizations. ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 139,
    "chunk_text": "2016.\n[194] B. Saket, A. Endert, and C. Demiralp. Task-Based Effectiveness of Basic\nVisualizations. IEEE Trans. Vis. Comput. Graph., 25(7), 2019.\n[195] A. Saktheeswaran, A. Srinivasan, and J. Stasko. Touch? Speech? or\nTouch and Speech? Investigating Multimodal Interaction for Visual\nNetwork Exploration and Analysis. IEEE Trans. Vis. Comput. Graph.,\n26(6):2168–2179, 2020.\n[196] A. Sarikaya and M. Gleicher. Scatterplots: Tasks, Data, and Designs.\nIEEE Trans. Vis. Comput. Graph., 24(1):402–412, 2018.\n[197] A. Satyanarayan, D. Moritz, K. Wongsuphasawat, and J. Heer. Vega-Lite:\nA Grammar of Interactive Graphics. IEEE Trans. Vis. Comput. Graph.,\n23(1):341–350, 2017.\n[198] A. Satyanarayan, R. Russell, J. Hoffswell, and J. Heer.\nReactive\nVega: A Streaming Dataﬂow Architecture for Declarative Interactive\nVisualization. IEEE Trans. Vis. Comput. Graph., 22(1):659–668, 2016.\n[199] M. Savva, N. Kong, A. Chhajta, and et al.\nReVision: Automated\nclassiﬁcation, analysis and redesign of chart images. In",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 140,
    "chunk_text": " A. Chhajta, and et al.\nReVision: Automated\nclassiﬁcation, analysis and redesign of chart images. In Proc. UIST’11.\nACM, 2011.\n[200] E. Segel and J. Heer. Narrative visualization: Telling stories with data.\nIEEE Trans. Vis. Comput. Graph., 16(6):1139–1148, 2010.\n[201] P. Seipel, A. Stock, S. Santhanam, and et al. Speak to your Software\nVisualization—Exploring Component-Based Software Architectures\nin Augmented Reality with a Conversational Interface.\nIn Proc.\nVISSOFT’19. IEEE, 2019.\n[202] J. Sen, F. Ozcan, and et al. Natural Language Querying of Complex\nBusiness Intelligence Queries. In Proc. SIGMOD’19. ACM, 2019.\n[203] J. Seo and B. Shneiderman.\nA Rank-by-Feature Framework for\nInteractive Exploration of Multidimensional Data. Inf. Vis., 2005.\n[204] V. Setlur, S. Battersby, and T. Wong.\nGeoSneakPique : Visual\nAutocompletion for Geospatial Queries. In Proc. VIS’21. IEEE, 2021.\n[205] V. Setlur, S. E. Battersby, M. Tory, and et al. Eviza: A natural language\ninterface for visual analysis. ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 141,
    "chunk_text": "tlur, S. E. Battersby, M. Tory, and et al. Eviza: A natural language\ninterface for visual analysis. In Proc. UIST’16. ACM, 2016.\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n19\n[206] V. Setlur, E. Hoque, D. H. Kim, and A. X. Chang.\nSneak pique:\nExploring autocompletion as a data discovery scaffold for supporting\nvisual analysis. In Proc. UIST’20. ACM, 2020.\n[207] V. Setlur and A. Kumar. Sentiﬁers: Interpreting Vague Intent Modiﬁers\nin Visual Analysis using Word Co-occurrence and Sentiment Analysis.\nIn Proc. VIS’20. IEEE, 2020.\n[208] V. Setlur, M. Tory, and A. Djalali. Inferencing underspeciﬁed natural\nlanguage utterances in visual analysis. In Proc. IUI’19. ACM, 2019.\n[209] R. Sevastjanova, F. Beck, B. Ell, and et al. Going beyond Visualization:\nVerbalization as Complementary Medium to Explain Machine Learning\nModels. In Proc. VISxAI’18. IEEE, 2018.\n[210] S. Shekarpour, E. Marx, A.-C. Ngonga Ngomo, and S. Auer. SINA:\nSemantic interpretation of user",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 142,
    "chunk_text": "[210] S. Shekarpour, E. Marx, A.-C. Ngonga Ngomo, and S. Auer. SINA:\nSemantic interpretation of user queries for question answering on\ninterlinked data. J. Web Semant., 30:39–51, 2015.\n[211] L. Shen, E. Shen, Z. Tai, and et al. TaskVis : Task-oriented Visualization\nRecommendation. In Proc. EuroVis’21. Eurographics, 2021.\n[212] D. Shi, Y. Shi, X. Xu, and et al. Task-Oriented Optimal Sequencing of\nVisualization Charts. In Proc. VDS’19. IEEE, 2019.\n[213] D. Shi, X. Xu, F. Sun, and et al. Calliope: Automatic Visual Data\nStory Generation from a Spreadsheet. IEEE Trans. Vis. Comput. Graph.,\n27(2):453–463, 2021.\n[214] Y. Shi, C. Bryan, S. Bhamidipati, and et al. MeetingVis: Visual Narratives\nto Assist in Recalling Meeting Context and Content. IEEE Trans. Vis.\nComput. Graph., 24(6):1918–1929, 2018.\n[215] X. Shu, A. Wu, J. Tang, and et al. What Makes a Data-GIF Understand-\nable? IEEE Trans. Vis. Comput. Graph., 27(2):1492–1502, 2021.\n[216] N. Siddiqui and E. Hoque. ConVisQA: A Natural Language ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 143,
    "chunk_text": "Comput. Graph., 27(2):1492–1502, 2021.\n[216] N. Siddiqui and E. Hoque. ConVisQA: A Natural Language Interface for\nVisually Exploring Online Conversations. In Proc. IV’20. IEEE, 2020.\n[217] T. Siddiqui, P. Luh, Z. Wang, and et al.\nShapeSearch: Flexible\npatternbased querying of trend line visualizations. Proc. VLDB Endow.,\n11(12):1962–1965, 2018.\n[218] T. Siddiqui, P. Luh, Z. Wang, and et al. From Sketching to Natural Lan-\nguage: Expressive Visual Querying for Accelerating Insight. SIGMOD\nRec., 50(1):51–58, 2021.\n[219] S. Sigtia, E. Marchi, S. Kajarekar, and et al. Multi-Task Learning for\nSpeaker Veriﬁcation and Voice Trigger Detection. In Proc. ICASSP’20.\nIEEE, 2020.\n[220] A. Simitsis, G. Koutrika, and Y. Ioannidis. Pr´ecis: from unstructured\nkeywords as queries to structured databases as answers.\nVLDB J.,\n17(1):117–149, 2007.\n[221] S. Sinclair, R. Miikkulainen, and S. Sinclair.\nSubsymbolic Natural\nLanguage Processing: An Integrated Model of Scripts, Lexicon, and\nMemory, volume 73. MIT ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 144,
    "chunk_text": "ic Natural\nLanguage Processing: An Integrated Model of Scripts, Lexicon, and\nMemory, volume 73. MIT Press, 1997.\n[222] H. Singh and S. Shekhar. STL-CQA: Structure-based Transformers\nwith Localization and Encoding for Chart Question Answering. In Proc.\nEMNLP’20. ACL, 2020.\n[223] A. Spreaﬁco and G. Carenini. Neural Data-Driven Captioning of Time-\nSeries Line Charts. In Proc. AVI’20. ACM, 2020.\n[224] A. Srinivasan, M. Dontcheva, E. Adar, and S. Walker. Discovering\nnatural language commands in multimodal interfaces. In Proc. IUI’19.\nACM, 2019.\n[225] A. Srinivasan, S. M. Drucker, A. Endert, and J. Stasko. Augmenting\nvisualizations with interactive data facts to facilitate interpretation and\ncommunication. IEEE Trans. Vis. Comput. Graph., 25(1):672–681, 2019.\n[226] A. Srinivasan, B. Lee, N. Henry Riche, and et al. InChorus: Designing\nConsistent Multimodal Interactions for Data Visualization on Tablet\nDevices. In Proc. CHI’20. ACM, 2020.\n[227] A. Srinivasan, B. Lee, and J. T. Stasko.\nInterwea",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 145,
    "chunk_text": " Tablet\nDevices. In Proc. CHI’20. ACM, 2020.\n[227] A. Srinivasan, B. Lee, and J. T. Stasko.\nInterweaving Multimodal\nInteraction with Flexible Unit Visualizations for Data Exploration. IEEE\nTrans. Vis. Comput. Graph., 14(8):1–15, 2020.\n[228] A. Srinivasan, N. Nyapathy, B. Lee, and et al. Collecting and Character-\nizing Natural Language Utterances for Specifying Data Visualizations.\nIn Proc. CHI’21, 2021.\n[229] A. Srinivasan and V. Setlur.\nSnowy:Recommending Utterances for\nConversational Visual Analysis. In Proc. UIST’21. ACM, 2021.\n[230] A. Srinivasan and J. Stasko. Natural Language Interfaces for Data\nAnalysis with Visualization: Considering What Has and Could Be Asked.\nIn Proc. EuroVis’17. Eurographics, 2017.\n[231] A. Srinivasan and J. Stasko. Orko: Facilitating Multimodal Interaction\nfor Visual Exploration and Analysis of Networks. IEEE Trans. Vis.\nComput. Graph., 24(1):511–521, 2018.\n[232] A. Srinivasan and J. Stasko. How to ask what to say?: Strategies for\nevaluating natural langua",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 146,
    "chunk_text": "[232] A. Srinivasan and J. Stasko. How to ask what to say?: Strategies for\nevaluating natural language interfaces for data visualization.\nIEEE\nComput. Graph. Appl., 40(4):96–103, 2020.\n[233] B. Steichen, G. Carenini, and C. Conati. User-adaptive information\nvisualization - Using eye gaze data to infer visualization tasks and user\ncognitive abilities. In Proc. IUI’13. ACM, 2013.\n[234] C. Stolte, D. Tang, and P. Hanrahan. Polaris: a system for query, analysis,\nand visualization of multidimensional relational databases. IEEE Trans.\nVis. Comput. Graph., 8(1):52–65, 2002.\n[235] D. Streeb, Y. Metz, U. Schlegel, and et al. Task-based Visual Interactive\nModeling: Decision Trees and Rule-based Classiﬁers. IEEE Trans. Vis.\nComput. Graph., XXX(XXX):1–18, 2021.\n[236] N. Stylianou and I. Vlahavas. A neural Entity Coreference Resolution\nreview. Expert Syst. Appl., 168(114466):1–20, 2021.\n[237] W. Su, X. Zhu, Y. Cao, and et al. VL-BERT: Pre-training of Generic\nVisual-Linguistic Representations. arXiv",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 147,
    "chunk_text": "X. Zhu, Y. Cao, and et al. VL-BERT: Pre-training of Generic\nVisual-Linguistic Representations. arXiv, 2019.\n[238] Y. Suhara, J. Li, Y. Li, and et al. Annotating Columns with Pre-trained\nLanguage Models. arXiv, 2021.\n[239] Y. Sun, J. Leigh, A. Johnson, and S. Lee. Articulate: A Semi-automated\nModel for Translating Natural Language Queries into Meaningful\nVisualizations. In Lect. Notes Comput. Sci. Springer, 2010.\n[240] J. R. Thompson, Z. Liu, and J. Stasko. Data Animator: Authoring\nExpressive Animated Data Graphics. In Proc. CHI’21. ACM, 2021.\n[241] C. Tong, R. Roberts, R. Borgo, and et al. Storytelling and visualization:\nAn extended survey. Inf., 9(3):1–42, 2018.\n[242] M. Tory and V. Setlur. Do What I Mean, Not What I Say! Design Con-\nsiderations for Supporting Intent and Context in Analytical Conversation.\nIn Proc. VAST’19. IEEE, 2019.\n[243] B. Tversky, J. B. Morrison, and M. Betrancourt. Animation: Can it\nfacilitate? Int. J. Hum. Comput. Stud., 57(4):247–262, 2002.\n[244] A. Van Dam. ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 148,
    "chunk_text": "t. Animation: Can it\nfacilitate? Int. J. Hum. Comput. Stud., 57(4):247–262, 2002.\n[244] A. Van Dam. Post-WIMP User Interfaces. Commun. ACM, 40(2), 1997.\n[245] S. Van Den Elzen and J. J. Van Wijk. Small multiples, large singles: A\nnew approach for visual data exploration. Comput. Graph. Forum, 32(3\nPART2):191–200, 2013.\n[246] M. Vartak, S. Madden, A. Parameswaran, and N. Polyzotis. SEEDB:\nAutomatically generating query visualizations. Proc. VLDB Endow.,\n7(13):1581–1584, 2014.\n[247] M. Vartak, S. Rahman, S. Madden, and et al. SEEDB: Efﬁcient data-\ndriven visualization recommendations to support visual analytics. Proc.\nVLDB Endow., 8(13):2182–2193, 2015.\n[248] A. Vaswani, N. Shazeer, N. Parmar, and et al. Attention Is All You Need.\nIn Proc. NIPS’17. NIPS, 2017.\n[249] C. Wang, Y. Chen, Z. Xue, and et al. CogNet: Bridging Linguistic\nKnowledge, World Knowledge andCommonsense Knowledge. In Proc.\nAAAI’21. AAAI, 2020.\n[250] C. Wang, Y. Feng, R. Bodik, and et al.\nFalx: Synthesis-Powered\nVisualiz",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 149,
    "chunk_text": ".\nAAAI’21. AAAI, 2020.\n[250] C. Wang, Y. Feng, R. Bodik, and et al.\nFalx: Synthesis-Powered\nVisualization Authoring. In Proc. CHI’21. ACM, 2021.\n[251] Q. Wang, Z. Chen, Y. Wang, and H. Qu. A Survey on ML4VIS: Applying\nMachine Learning Advances to Data Visualization. arXiv, 2021.\n[252] W. Wang, Y. Tian, H. Wang, and W.-S. Ku. A Natural Language Interface\nfor Database: Achieving Transfer-learnability Using Adversarial Method\nfor Question Understanding. In Proc. ICDE’20. IEEE, 2020.\n[253] Y. Wang, F. Han, L. Zhu, and et al. Line Graph or Scatter Plot? Automatic\nSelection of Methods for Visualizing Trends in Time Series. IEEE Trans.\nVis. Comput. Graph., 24(2):1141–1154, 2018.\n[254] Y. Wang, Z. Sun, H. Zhang, and et al. DataShot: Automatic Generation\nof Fact Sheets from Tabular Data. IEEE Trans. Vis. Comput. Graph.,\n26(1):895–905, 2020.\n[255] Y. Wang, W. M. White, and E. Andersen. PathViewer: Visualizing\nPathways through Student Data. In Proc. CHI’17. ACM, 2017.\n[256] Y. Wang, H. Zhang, H. ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 150,
    "chunk_text": " Visualizing\nPathways through Student Data. In Proc. CHI’17. ACM, 2017.\n[256] Y. Wang, H. Zhang, H. Huang, and et al. InfoNice: Easy Creation of\nInformation Graphics. In Proc. CHI’18. ACM, 2018.\n[257] Z. Wang, L. Sundin, D. Murray-Rust, and B. Bach. Cheat Sheets for\nData Visualization Techniques. In Proc. CHI’20. ACM, 2020.\n[258] G. Wohlgenannt, D. Mouromtsev, D. Pavlov, and et al. A Comparative\nEvaluation of Visual and Natural Language Question Answering over\nLinked Data. In Proc.K’19. SCITEPRESS, 2019.\n[259] K. Wongsuphasawat, D. Moritz, A. Anand, and et al. Towards a general-\npurpose query language for visualization recommendation. In Proc.\nHILDA’16. ACM, 2016.\n[260] K. Wongsuphasawat, D. Moritz, A. Anand, and et al.\nVoyager:\nExploratory Analysis via Faceted Browsing of Visualization Recom-\nmendations. IEEE Trans. Vis. Comput. Graph., 22(1):649–658, 2016.\n[261] K. Wongsuphasawat, Z. Qu, D. Moritz, and et al. Voyager 2: Augmenting\nvisual analysis with partial view speciﬁcations. In P",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 151,
    "chunk_text": "u, D. Moritz, and et al. Voyager 2: Augmenting\nvisual analysis with partial view speciﬁcations. In Proc. CHI’17. ACM.\n[262] A. Wu, Y. Wang, X. Shu, and et al. AI4VIS: Survey on Artiﬁcial\nIntelligence Approaches for Data Visualization. IEEE Trans. Vis. Comput.\nGraph., pages 1–20, 2021.\n[263] A. Wu, Y. Wang, M. Zhou, and et al. MultiVision: Designing Analytical\nDashboards with Deep Learning Based Recommendation. IEEE Trans.\nVis. Comput. Graph., pages 1–11, 2021.\n\nIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. X, XX 2022\n20\n[264] A. Wu, L. Xie, B. Lee, and et al.\nLearning to Automate Chart\nLayout Conﬁgurations Using Crowdsourced Paired Comparison. In\nProc. CHI’21. ACM, 2021.\n[265] H. Xia. Crosspower: Bridging graphics and linguistics. In Proc. UIST’20.\nACM, 2020.\n[266] H. Xia, J. Jacobs, and M. Agrawala. Crosscast: Adding Visuals to Audio\nTravel Podcasts. In Proc. UIST’20. ACM, 2020.\n[267] Y. Xian, H. Zhao, T. Y. Lee, and et al. EXACTA: Explainable Column\nAnnotatio",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 152,
    "chunk_text": "ST’20. ACM, 2020.\n[267] Y. Xian, H. Zhao, T. Y. Lee, and et al. EXACTA: Explainable Column\nAnnotation. In Proc. KDD’21. ACM, 2021.\n[268] X. Xu, C. Liu, and D. Song. SQLNet: Generating Structured Queries\nFrom Natural Language Without Reinforcement Learning. arXiv, 2017.\n[269] S. Yagcioglu, A. Erdem, E. Erdem, and N. Ikizler-Cinbis. RecipeQA: A\nChallenge Dataset for Multimodal Comprehension of Cooking Recipes.\nIn Proc. EMNLP’18. ACL, 2018.\n[270] N. Yaghmazadeh, Y. Wang, I. Dillig, and T. Dillig. SQLizer: query\nsynthesis from natural language. Proc. ACM Program. Lang., 2017.\n[271] T. Young, D. Hazarika, S. Poria, and E. Cambria. Recent Trends in Deep\nLearning Based Natural Language Processing. IEEE Comput. Intell.\nMag., 13(3):55–75, 2018.\n[272] B. Yu and C. T. Silva. VisFlow - Web-based Visualization Framework\nfor Tabular Data with a Subset Flow Model. IEEE Trans. Vis. Comput.\nGraph., 23(1):251–260, 2017.\n[273] B. Yu and C. T. Silva. FlowSense: A Natural Language Interface for\nVisual Data",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 153,
    "chunk_text": ":251–260, 2017.\n[273] B. Yu and C. T. Silva. FlowSense: A Natural Language Interface for\nVisual Data Exploration within a Dataﬂow System. IEEE Trans. Vis.\nComput. Graph., 26(1):1–11, 2020.\n[274] T. Yu, R. Zhang, K. Yang, and et al. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic parsing and text-to-SQL\ntask. In Proc. EMNLP’18. ACL, 2018.\n[275] L. Yuan, Z. Zhou, J. Zhao, and et al.\nInfoColorizer: Interactive\nRecommendation of Color Palettes for Infographics. IEEE Trans. Vis.\nComput. Graph., 2626:1–15, 2021.\n[276] R. Zehrung, A. Singhal, M. Correll, and L. Battle. Vis Ex Machina:\nAn Analysis of Trust in Human versus Algorithmically Generated\nVisualization Recommendations. In Proc. CHI’21. ACM, 2021.\n[277] Z. Zeng, P. Moh, F. Du, and et al. An Evaluation-Focused Framework for\nVisualization Recommendation Algorithms. IEEE Trans. Vis. Comput.\nGraph., pages 1–11, 2021.\n[278] D. Zhang, Y. Suhara, J. Li, and et al. Sato: Contextual semantic type\ndetection in t",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 154,
    "chunk_text": "11, 2021.\n[278] D. Zhang, Y. Suhara, J. Li, and et al. Sato: Contextual semantic type\ndetection in tables. Proc. VLDB Endow., 13(11):1835–1848, 2020.\n[279] H. Zhang, Y. Song, Y. Song, and D. Yu. Knowledge-aware Pronoun\nCoreference Resolution. In Proc. ACL’19. ACL, 2019.\n[280] H. Zhang, X. Zhao, and Y. Song. A Brief Survey and Comparative Study\nof Recent Development of Pronoun Coreference Resolution. arXiv, 2020.\n[281] S. Zhang, L. Yao, A. Sun, and Y. Tay.\nDeep Learning Based\nRecommender System. ACM Comput. Surv., 52(1):1–38, 2019.\n[282] Y. Zhang, P. Pasupat, and P. Liang. Macro Grammars and Holistic\nTriggering for Efﬁcient Semantic Parsing. In Proc. EMNLP’17. ACL.\n[283] Z. Zhang, Y. Gu, X. Han, and et al. CPM-2: Large-scale Cost-effective\nPre-trained Language Models. arXiv, 2021.\n[284] Zhen Wen, M. Zhou, and V. Aggarwal. An optimization-based approach\nto dynamic visual context management. In Proc. InfoVis’05. IEEE, 2005.\n[285] W. Zheng, H. Cheng, L. Zou, and et al.\nNatural Language Que",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 155,
    "chunk_text": ". In Proc. InfoVis’05. IEEE, 2005.\n[285] W. Zheng, H. Cheng, L. Zou, and et al.\nNatural Language Ques-\ntion/Answering. In Proc. CIKM’17. ACM, 2017.\n[286] X. Zheng, X. Qiao, Y. Cao, and R. W. H. Lau. Content-aware generative\nmodeling of graphic design layouts. ACM Trans. Graph., 2019.\n[287] V. Zhong and et al. Seq2SQL: Generating Structured Queries from\nNatural Language using Reinforcement Learning. arXiv, 2017.\n[288] M. Zhou, Q. Li, X. He, and et al. Table2Charts: Recommending Charts\nby Learning Shared Table Representations. In Proc. KDD’21. ACM.\n[289] S. Zhu, G. Sun, Q. Jiang, and et al. A survey on automatic infographics\nand visualization recommendations. Vis. Informatics, 4(3):24–40, 2020.\n[290] F. ˝Ozcan, A. Quamar, J. Sen, and et al. State of the Art and Open\nChallenges in Natural Language Interfaces to Data. In Proc. SIGMOD’20.\nACM, 2020.\nLeixian Shen received his bachelor’s degree in\nSoftware Engineering from the Nanjing University\nof Posts and Telecommunications in 2020. He\nis ",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 156,
    "chunk_text": " in\nSoftware Engineering from the Nanjing University\nof Posts and Telecommunications in 2020. He\nis currently a master student in the School of\nSoftware, Tsinghua University, Beijing, China. His\nresearch interests include data visualization and\nhuman computer interaction.\nEnya Shen received the BSc degree from Nan-\njing University of Aeronautics and Astronautics,\nin 2008, and MSc and PhD degree from National\nUniversity of Defense Technology, in 2010 and\n2014, respectively. He works as a research assis-\ntant in Tsinghua University. His research interests\ninclude data visualization, human computer inter-\naction, augmented reality, and geometry model-\ning.\nYuyu Luo received his bachelor’s degree in\nSoftware Engineering from the University of Elec-\ntronic Science and Technology of China in 2018.\nHe is currently a PhD student in the Department\nof Computer Science, Tsinghua University, Bei-\njing, China. His research interests include data\nvisualization and data cleaning.\nXiaocong Yang is a f",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 157,
    "chunk_text": "ng, China. His research interests include data\nvisualization and data cleaning.\nXiaocong Yang is a fourth-year undergraduate\nworking towards a BS degree at Tsinghua Uni-\nversity, and plan to apply for a PhD position in\nComputer Science. His research interests include\nnatural language processing and machine learn-\ning theory.\nXuming Hu received the BE degree in Computer\nScience and Technology, Dalian University of\nTechnology. He is working toward the PhD degree\nat Tsinghua University. His research interests\ninclude natural language processing and infor-\nmation extraction.\nXiongshuai Zhang received Bachelor’s Degree\nfrom South China University of Techinology,\nChina, in 2019. He is now a master student\nwith the School of Software, Tsinghua Univer-\nsity, China. His research interests include data\nvisualization and computer graphics.\nZhiwei Tai received his BS degree from the\nSchool of Software, Tsinghua University in 2020.\nHe is currently working toward the Master’s\ndegree in Software Engi",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  },
  {
    "chunk_id": 158,
    "chunk_text": "re, Tsinghua University in 2020.\nHe is currently working toward the Master’s\ndegree in Software Engineering at Tsinghua\nUniversity. His research interests include data\nvisualization, digital twin, and augmented reality.\nJianmin Wang received his bachelor’s degree\nfrom Peking University, China, in 1990, and the\nME and PhD degrees in computer software from\nTsinghua University, China, in 1992 and 1995,\nrespectively. He is a full professor with the School\nof Software, Tsinghua University. His research\ninterests include big data management and large-\nscale data analytics. He is leading to develop\na big data management system in the National\nEngineering Lab for Big Data Software.\n",
    "source_pdf": "LiteratureReview.pdf",
    "chunking_method": "simple_sliding_window",
    "source_extraction_method": "pymupdf_extraction_text"
  }
]